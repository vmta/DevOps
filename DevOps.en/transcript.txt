Transcript

Course Overview

Hi everyone, my name is Don Jones, and I'd like to introduce you to PowerShell and DevOps Global Summit 2016. Summit is a unique annual event for the global PowerShell and DevOps community and for 2016 we were fortunate to have Pluralsight help us capture some of the amazing presentations. You'll find more than 30 breakout sessions and general presentations representing some of the best to what summit has to offer, you will hear from PowerShell MVP's, product team members and community enthusiasts and more importantly, you'll explore incredibly deep topics that simply don't exist anywhere else. Want to peak under the hood of how Desired State Configuration works? Want to learn about advanced debugging techniques in PowerShell 5? Want to learn what DevOps really looks like in a real organization? Want to see the latest about domain-specific languages in PowerShell? All of those and more were covered at PowerShell and DevOps Global Summit 2016 and thanks to Pluralsight, you can watch them at your leisure.

Active Directory DSC

Introduction

Okay. Active Directory Desired Stake Configuration. How many of you have Active Directory in your environment? Oh, nice. Alright, you came to the same place, alright? How many of you have an Active Directory test lab in your environment. Nice, alright. That's better than average. You guys are above average here. How many have Lamas in your data center? Two, alright I'm not going to talk about Lamas in a data center. Alright, let's talk about Active Directory in DSC. That's my stuff up there, GoateePFE. If you search anywhere online you'll find me. Alright, let's dive. And yes, thank you, applause to all the guys who made this possible for the last 10 years. Okay, here's the problem. I'm a Microsoft Premier field engineer, and I visit premier customers and help them with their Active Directory, that's kind of what I've done for years, and most of those customers do not have an Active Directory test lab. So what I'm going to show you today is how to spin up your own Active Directory test lab in 30 minutes. Alright, that's mostly just wait time. It takes you just a minute to run the script. It's going to be about 30 minutes and you're going to have your own domain controller spun up in Azure that you can just destroy, do whatever you want to against it, and then at the end of the day, just delete and you're done. So you can have an on-demand domain controller to beat up anytime, just run the script. That's what we're going to do today. Does that sound like fun? Yep. Alright. So that was my goal then. I wanted to be able to deploy an Active Directory test lab to Azure including, because you know this DSC stuff, usually somebody just spins up a server, but I want to actually put OUs, and in those OUs, I wanted users and groups, and I wanted to enable the AD recycle bin, here do some AD configuration, and then I want it to be as simple as running a script and I did not want to have to open Visual Studio because I'm not a big Visual Studio person, right. I'm an infrastructure guy. I don't do---I'm an ops. I'm not a dev on that side of that equation.

DC Deployment Using ARM Template and Custom DSC Configuration

Here's what we're going to do. I'm going to go ahead and we are going to do this live right here today. So I've got some code. I've done my Login to AzureRM. I'm going to come back and show you how this works. I'm going to create a couple of variables here that are going to define the name of my resource group and then I'm going to just do a quick check to make sure that DNS name is available. There's no conflict. There shouldn't be, there aren't that many Goatee instances out on Azure. And then I'm going to spin up my Azure resource group. That one takes just a second to run. That's good. And just like that, I created a New-AzureRmResourceGroup. Now I'm just going to define a couple parameters, which are mostly variables already done here, I want to go ahead and run those and now the big line right here, New-AzureRmResourceGroupDeployment. I'm just passing in the splat of parameters and let's go ahead and run that line as well. This is the part, alright, it's going to ask me for my password, make sure I remember this, powershell5 thing. Alright, password, got it. What? Man oh man. Provided value east us for the template parameter. Guys, I have done this a million times and the only reason it's failing is because I'm in front of all of you right now. Alright, what's going on here? (Audience question) Yeah, it is, but it's all lowercase. Well, that one particular piece, yeah, you know what I've run this a million times just like it is. Oh, okay, yep, it is because I did change one last thing last night after I ran my demo, east, East US. You're right. I did change that little spot after my demo practice last night. That was a bad thing on my part. Now let's try it again. There we go. Alright. Come on, baby. Got the -Verbose in there, so it gives me the play by play. Alright. So what's happening now is I'm actually spinning up. I've got a new Azure resource group that I created. Now I'm actually deploying to that Azure resource group a single domain controller, all the storage and network and everything that goes with it and it's going to have a DSC configuration that I designed that was custom for Active Directory and for my domain controller, the data that I wanted to go in there and that's what I'm going to show you now is how that works. There we go. Get my prompt here. I want to make sure this gets off and running because---okay, now we're good. So we're creating a template deployment now for AlpineSkiHouseForest. I'm tired of using Contoso.com. Anybody else? Alpine Ski House is one of the other legal approved Microsoft fictitious company names. So Alpine Ski House sounds like a lot more fun than Contoso. So let's get back to the presentation. We'll come back and see this in a minute. So you came to talk about Active Directory in DSC. This is the xActiveDirectory resource module that you can get at PowerShell gallery or you can go look at it on GitHub. And in that module, there are a number of resources. Now how many have used this in DSC already? Anybody? Just a couple hands, okay. So the main thing to understand here is the difference between xADDomain and xADDomainController. XADDomain is going to be the first DC in the environment. Every other DC after that in the same domain is going to be a xADDomainController. Alright. And then xWaitForADDomain says once I've done the install, I need to make sure that it's come up and it's responding, so that's the wait. And then once that wait is successful, I can contact the domain, then I can go ahead and do my other resources here like DomainTrust, organizational units, users, groups, and so forth. So those are the resources we have today in that module. It's bare minimum at this point. It's been out there for a year and a half or so. We've got a little bit of traction out there, but there are some other things in process. The last I checked on GitHub, there's a default domain policy out there that's getting ready to be released soon. Also, we're actually looking for people to help build this out. So if you do some AD PowerShell and you'd like to help us add some resources here, we'd like to do a resource for just a generic Active Directory object or setting permissions on something in Active Directory, tons of things we can add to this, you know, sites and subnets, all that stuff. So feel free to go out there to GitHub, take a look at it, and see where you can jump in and help. So there's one person I would really like to thank and that's Iain Brighton. This is the stats off of GitHub. Iain has done the most work on this module in the community. So a big shout out to Iain for all of his work helping us move forward with the xActiveDirectory resource module. So what we're doing today is deploying a domain controller using what's called Azure Resource Manager templates. How many have heard of that before? Okay. Now basically, instead of going and clicking through the Azure portal to do your resource deployment, your---you've got to spin up, compute, and storage, and unicorns, and fairies, and all that in Azure to make everything work right, and you can do that with code obviously because we're all PowerShell people in here. But it's really easy now with Azure templates. You just say give me one of these and it just gives you all that stuff. It just does automatically all the storage, and the compute, and the network, and all that, just builds it all up for you and that's what I've been waiting on. See, now we're there, so I don't have to worry about knowing all the 1s and 0s of everything I have to set up. So we've got that we're using and I used---I cheated like everybody else. Hey, you know, when you write a script, you go look on the internet, find somebody else that did it, and then you start with that, right? So I just went out to GitHub. There's an AzureRM section out on GitHub where we've published these resource templates, and sure enough, there's one there for a domain controller, so I started with that. And then I snagged the latest Active Directory module here, xActiveDirectory from the PowerShell gallery, and then I went out and watched Trevor Sullivan's video on doing Azure Template deployments with PowerShell, some presentation he did over in Europe. Everybody know Trevor Sullivan? MVP? Yep, alright. So he did a really good job on that video. And then I did the phone-a-friend thing and all that and talked to some of my buddies and kind of put this, hacked this thing together.

Refining DSC Configuration from Template

So here are the challenges I ran into deploying a domain controller using this technique. Well, number one, the template that was out there on the Azure resource templates was kind of dated. It used an old version, an old, stale version of the xActiveDirectory resource module. It didn't have all the new bells and whistles. It still was using the xDisk DSC resource module, which has been deprecated now and it's now in xStorage. And then there's the xNetworking was out-of-date, so I basically, I found this template that I could use, but it was way out of date. I needed to fix it up. The other thing I wanted to do is I want to put my own domain controller DSC configuration in there. I didn't want to take the one that was in the template. I wanted to write my own, put that up in there as well. And then what I also learned was that when I was trying to pass parameters in, with the Azure template, it uses a JSON and when I was trying to pass an array, I couldn't figure out how to get the array to go through in a JSON parameter. Maybe some of you guys know that, but I couldn't figure that out. So but it was better, it was cleaner, instead, if I could just put my configuration script in one file and my psd configuration data file for DSC in another separate file, it has all of my AD configuration information in it. So those two files, plus the Azure JSON stuff. So here's the process. I installed that xActiveDirectory resource module on a VM to start testing with. I wrote and tested my AD DSC configuration, how I wanted it to build. I just tested that on a local VM just getting my configuration tuned the way I wanted. And then I basically took the arm template that I found out on GitHub and I just mashed those two together and this was a lot of mashing to make that stuff work right. But I basically took my configuration, mashed it up together with that template, and there are just a couple files there. So there's something on azuredeploy.json, which is what the Azure Resource Manager cmdlet uses to go find the information, where to point things, and what values you're going to pass in, things like that. Then there's a zip file. How many have done the Azure DSC extension? Have you worked with that? And when you publish a DSC configuration to Azure, there's this zip file that it builds. All that is a PS1 script file holding your DSC configuration script and then zipped up in there are the resource module folders that need to be installed on that target, that machine. And then there's a configuration data file. How many have done DSC with a configuration data file? Alright, or you used the -ConfigurationData when you build a DSC, right, you pass that in. So this is just that, that's a hash table file. It's got all my parameters in there. So those are the files that are hosted out on GitHub. By the end of this session, you'll be able to go out to GitHub and take the exact same files that I published out there, download it, one file, out of that GitHub repository to your machine and run it and build a domain controller 10 times a day if you wanted in your own instance of Azure. You can also use that code as a reference template for just how do I do a domain controller with DSC and that's what we're going to get into in just a second here when we start looking at the code. So then I deployed this template to Azure and did it work the first time? No. So it's fail, tweak, fail, tweak, fail, tweak, and bashing head on keyboard until finally it works, then success. Yay, alright. So that's, right, that's how we script. Right, we just keep trying until it works and then we, you know, fist bump, right. So yeah. So let's take a look then at the actual files behind this. Let's get this, let's get this going. So there's xActiveDirectory Resource Module. You can do Install-Module xActiveDirectory. I've already got that done, so I'm not going to worry about that right now. You can just slide this down out of the way. And so I can look at my resources in that module and so forth, but I don't want to go into that. I want to spend more time on focusing on the DSC configuration script. So here we go. I've got a configuration called CreateADDomainWithData, which is kind of the difference here. There are a lot of templates out there for deploying a domain controller with DSC, but this one is actually going to show you a fully populated OU user and group data structure. So what we're going to do is, this is just a standard DSC configuration and this is the one that came with the Azure resource template that I snagged off of GitHub. Let me show you where that is, by the way. You need to see this place. So out on GitHub, there's a github/Azure/azure-quickstart-templates. There are, I haven't counted, I'm guessing dozens, hundreds of these, any kind of ready-made recipe you just want to go spin up some environment with multiple servers all talking to one another and happy. In Azure, without having to do that manually, there you go. There's all kinds of those quick templates right there. Well one of these is a domain, so that's what I started with. And so this configuration came out of that and then I had to tweak it up. I had to import the PSDesiredStateConfiguration default module because v5 complains now if you don't do that. Importing the xActiveDirectory module, the xStorage module, and we're taking a parameter for the domain credentials because when you build a DC, you've got to give it credentials in a couple places. So now let's get into the actual DSC configuration piece. This is what you came for. So LocalConfigurationManager. This is the v4 syntax for the LCM and DSC and that's okay, it still works in v5. The reason we put it here is because in Azure when I spin this up, what happens is it's going to run this whole configuration, it's going to spin out localhost.mof and localhost.mega.mof and it's going to configure the LCM and then it's going to actually apply the configuration to the local box and Azure just makes all that happen magically with the DSC extension. So what I did there is I said hey continue after reboot because when you do a domain controller promotion here, it's going to have to take a reboot and you want to make sure that the configuration continues to apply after that reboot. I am only going to apply it, though, I'm not going to keep checking it out later and then RebootNodeIfNeeded and AllowModuleOverWrite. So when you're spinning up a domain controller in Azure, does anybody know about disk caching with domain controllers. What do they prefer? You don't want write caching, right? You don't want read/write caching on the disk for your database, logs, sysvol, that kind of thing. So you have to---when you deploy a DC in Azure under IAZ, you have to actually create a second disk, a data disk, assigned to that VM so that you can turn off that caching. So that's what this is for. It's waiting for that second disk, that data disk to come up in the Azure VM and that's all handled by the template. I didn't have to do any of that hard work. All I had to do was just edit this configuration. So Disk2 and there's a RetryInterval there that's already defined up here in our parameters for the configuration. So I wait for the disk and then I use the xDisk resource to create an F drive on DiskNumber 2, that's going to be our ADDataDisk in Azure. And you could just---it doesn't matter if you're doing this in Azure or not. This is the same thing you would do on your local data center if you're doing this locally as well. So when we're deploying domains with DSC, we have to install the AD-Domain-Services, right? So we installed AD-Domain-Services and then we run what used to be known as DC Promo. Now this is what we call DSC promo and it's going to call all the cmdlets in the back-end to make that happen. So I've installed the AD-Domain role here, also because I like to cheat and use the GUI tools from time to time, I want to go ahead and put the RSAT in there for ADDS in there as well. So that's going to give me my good old trusty ADUC and the AD GUI tools, AD administrative center, all that good stuff. So now we're down to actually building the domain controller. And when I did this presentation last year, this is essentially the same kind of stuff, but this was also part of that downloaded template from the AzureRM template gallery. So xADDomain is going to be my first domain controller. Here's the DomainName coming in from the parameters to the whole thing, the DomainAdministratorCredential and a safemode credential. Because this is just going to be a quick dev lab, I don't really care that they use the same credentials. No big deal. Now notice I can specify here the Database, Log, and Sysvol Path. That's kind of important. You want to put that on that F drive that has no caching on it. And then it depends on the feature being installed and the disk being available. If those aren't ready, then I don't want to proceed with building the DC. So that's the dependency there. Make sure that all happens in the right order and then we're going to wait for the domain. So what this says is domain controller is promoting here with this resource and that's going to take a while and actually, it's going to force a reboot. So what happens the next time that machine reboots, I've got the LCM set to continue configuring, it's going to run down through, it says oh this is already done, and then it's going to check and make sure that domain, alpineskihouse.com, is responsive, and once I know that the domain is up and online, then I can proceed with configuring the domain. So there's an xADRecycleBin resource I wrote about 14 months ago, so now I can enable the ADRecycleBin and it's going to be for this same domain and obviously I have to wait for the domain before I can enable the RecycleBin there, so that's a dependency. So now I've got the base domain stood up with DSC. Now the next part of the configuration gets kind of interesting. So what I want to do is I want to build out an OU structure and put users and groups in there. So here's how I chose to do that. In DSC, we're used to the simple quick demo configurations, but did you realize that you can actually put scripting and looping in the body of your DSC configuration script? So now we can iterate through data. So what I did was I have this psd1 data file here and this is your hash table with your AllNodes, here's the Nodename, localhost, which you have to have in Azure, it has to be localhost, and then this property called PSDscAllowDomainUser. Notice there's not allow, what is the unsecure password thing is not there, because azure will encrypt a password automatically for me. But this little thing will snag you. On v5, one of the last few iterations of v5 we introduced this safety, so we prefer that you not use domain credentials for the credentials in your resources, and so there's this warning. If you put Domain/Username, it's going to flag you that you're trying to use a domain credential in your configuration. So you have to put this in there, PSDscAllowDomainUser = true to allow it to use a domain user account, which you have to use in this case. Now this NonNodeData is just a section, of blob of data that I'm going to use to build out the lab and here I've got a CSV script block or script, I'm sorry, a string, a CSV multiline string that has comma-separated values for username, department, manager, and a plane text password there that I really don't care about because this is my lab, alright. I'm not telling you to do this in production. This is just for a lab. So I've got eight users that I'm going to spin up here with the username, their department, and their title, and then I've got my OUs, RootOUs, ChildOUs, and then a TestObjCount. So here are my RootOUs, Accounting, IT, Marketing, and Operations. Underneath each one of those four RootOUs, I'm going to create ChildOUs, Users, Computers, and Groups. That's going to be one technique that I demonstrate in how to populate the data inside the domain. The other technique I'm going to use is called TestObjCount. So here I could put 5, I could put 500, I could put 5,000. Whatever this number is, that's how many test user accounts and test groups it's going to build and just throw like User1 into Group1, User2 into Group2, it's just populating just random data for testing in AD. So this is the data then that's going in to my configuration script. So now on my configuration script for each RootOU that's in that configuration data block in the RootOUs, for each one of those, I'm going to create a xADOrganizationalUnit giving it a dynamic resource name there and it's going to use that RootOU name, it's going to be in the DomainRoot, which I calculated right here. Domain name is alpineskihouse.com. So now it becomes DC=Alpine, DC=COM. So then my OU path, that's going to be the parent path for the DomainRoot, I'll give it a description, use the credential that we did already, ensure that it's present, and that's going to have to wait until the RecyleBin is done before it kicks in. So now that I've got the RootOUs built, it's going to loop through and build each one of those RootOUs. After that, it's going to create child OUs under each one of those. So here's my xADOrganizationalUnit and I'm just going to give it a dynamically calculated name inside the configuration here, giving it a ChildOU, and then the parent is going to be that RootOU in the DomainRoot, and in this case, I decided I guess I did in the previously as well ProtectedFromAccidentalDeletion and then we'll give it a credential, Ensure that it's Present, and that's dependent on the parent RootOU being established, right? So I've got to have the RootOU and then I can create all the ChildOUs under it. And then finally, as I'm looping through here, I'm building a master dependency array of all the OrganizationalUnits. Actually, this is going to have all the child organizational units because before I can create a user in the user's ITOU, that OU has to exist, so I'm stitching together an array of all the configuration items that are dependency before I start building users and groups inside those OUs. So that array right there comes down next to my users section. As the dependency, it depends on that array of OU configuration items that have to exist first. So now I take my UserData out of that CSV. I love doing this in my scripts. I'll just take a big string block and pipe it to ConvertFrom-CSV and then instantly I've got a rich PowerShell object with all these properties now for first name/last name, and a department, and all that stuff. So now I can create the NewADUser here with the User resource and I give it the DomainName, I give it the User.UserName because now I've got the CSV data. There's the username, there's the title. Now their Department is actually the RootOU. The RootOU as we just looked at, Accounting, IT, Marketing, Operations, that's the department that they're in, so it's going to drop them automatically into the right department. The account is going to be enabled. Now have you ever tried to enable an account in Active Directory that did not have a password? It fails, right. You have to have a password and one that meets the complexity requirements. So in this case, I have to have a password defined here, which is using the old ConvertTo-SecureString from that User Password that was, again, in plaintext. It's in a lab, I don't care. I never do this in production. So I'm creating credential property there and the dependency and now I'm creating a new dependency for the users. So before I can put those users into groups, the users have to exist. So later, in my configuration when I get to the group piece, I'll list all these user accounts as the dependencies before I start spinning up the groups. Now the other technique I'm going to use to spin up user accounts and you could use either one of these is the TestObjectCount. So 1..5 would be what the equivalent here is just the range operator there 1.. and whatever number comes out of that configuration file, it could be a 1000 users. I'll just go blow them all in there and build them all out. And so I'm going to create NewADUser1, 2, 3, 4, 5, and give the test username 1, 2, 3, 4, 5. And then this one, in this case, I don't care if it's enabled it's just going to be Enabled = false. There is no password specified. I don't care for that. And again, that depends just on the RecycleBin being enabled. They're not going into any OU. They're just going to drop into the default user's container. Then next up are the groups. So I've got that list of RootOUs. Those RootOUs are the same thing as the department names. So now I'm going to create a G_Department Name global group that's going to be Global, Security group, give it a description, and then the members in that group are going to be that user's CSV data, and I'm just using a simple PowerShell filter here where their RootOU was the department name for that user. So if we flip over here real quick, we're basically taking this list of department, right here, for each one of these user accounts and whichever username matches that RootOU, that's the OU we're going to drop them into from that CSV data. And that depends on the user being created before we can actually create the group in that path and populate the users into it. And then the users are dependent on the OUs, so there's this nested dependency structure that makes everything happen in the correct order. Then down here is my 1 to infinity, however many I specify, however many I specify, how many new groups I want, NewADGroup1, NewADGroup2, 3, 4, and then inside there, the member is going to be TestUser1, TestUser2, TestUser3. So I just use ForEach loops and a ConfigurationData.psd1 file. So you can take this, take that CSV block, put whatever you want in there for users and parameters and just define and tweak this configuration, put whatever kind of OUs and users and groups you want in your domain that way. And this whole thing is just the script, so it starts out with configuration CreateADDomainData and then when I jump to the very end, that's it. There's no calling code, it's just the configuration block here. And then when the Azure DSC resource, the Azure DSC extension we're going to pass in over here in the JSON file. Over here we're going to look for, we called that CreateADDomainWithData, createaddomainwithdata. So over here, what we have in the resource, the JSON file, is it says call this script file, CreateADDomainData.ps1, call that configuration name, CreateADDomainWithData. So in other words, Azure builds this VM, it copies the configuration files inside of it, and then it calls this configuration function name essentially inside that file with that name. And then it finds the resources that they need in this zip file. The zip file is going to have the Active Directory, xActiveDirectory module, and that type of thing in it. So that's the---there was some other bits that I had to hack up in the JSON file and I'm intentionally kind of glossing over that part because you're going to learn this piece as well if you decide to play with the templates. It's a lot of fun banging your head against the keyboard for two days, but you'll get there. I promise.

Calling Script

So then in my calling script over here, what I did was I took these files and I copied them from there and I just put them out in my own GoateePFE AzureRM GitHub. This is public and it's out there for you guys to use. So all these files that I'm showing you right now are right here on the screen. So if you go out to github.com/GoateePFE/AzureRM, you'll find these files and you can use them and tweak them up for your own purposes. You can create a copy in your own GitHub and modify the configuration however you want and tweak a few things and you can start spinning up your own domain controllers all day long, or if you're kind of lazy like me, just run this one and just change the domain name. It doesn't have to be Alpine Ski House. You can call it whatever you want and you can go in and edit things however you like and just make your own. So you've got here the data file and then inside that zip, you're going to have the actual script file, the ps1 configuration file and the resources in there. Alright so, basically all I'm doing then is I'm taking that data, I threw it up on GitHub. Now really and truly, you should put it in an Azure storage account where it's really secure and you're the only person that can see it if you're going to use this in any kind of production capacity. But all I did was I had to say Install-Module using the PowerShell get to pull down the Azure resource module, install that, install AzureRM, and it goes out and it installs like 28 modules for you in AzureRM and this all documented out there already. Install-Module Azure, Import-Modules are optional commands and then I just log in, so really this is where it starts. I log into my AzureRM account. So this is where---how many of you have an Azure account. Alright, you got a MSDN, your enterprise agreement from Microsoft has Azure benefits, you can go get a trial. So as long as you've got some kind of account, you just log in here, that's it. You put in your name and password. In the past, you had to do all the subscription IDs and this jazz to try to run PowerShell commands against Azure. Now this is all it takes. Log into my account, it knows all my stuff. I don't have to mess with all those extra parameters. And so now, what I do is I give it the URL to my GitHub account and that azuredeploy.json file that I snagged and edited out of that template library. And then I just---you can change these up. I called mine GoateePSH and then RG for Resource Group, SA for Storage Account, and then AD for the DNS name. So you can just tweak up those names right there, whatever you want to call those. They do have to be lowercase and then you test their availability and make sure there's no DNS conflict there in Azure and then you just create the resource group. So really what we're talking about to get this all done is three commands, log in, create a resource group, and then you've got your parameters here pretty well already populated, and then you just do the resource group deployment. So three commands and in less than 30 minutes later, you've got a full-blown domain controller ready to bang on, so those three commands. Now let's see, is it done? Let's take a look here. It is. So let's take a look at the verbose output here. So here's the play by play on building it out. We started at 3:05 PM and we scroll down. We saw it's deployed all these pieces. It ends at 3:33 PM. I've noticed it takes 27-28 minutes on average for me and then it shows me at the very end, here's my Azure resource group name and here are all the parameters that we passed in, here's the VNETs, all that stuff, so it's done. It's baked. I've got a standard D1 Azure VM out there called adDC. Now how do I get to it? Well, what I'm going to do is I'm going to use this GetAzureRM public IP address cmdlet and I just give it the resource group name, grab the first VM that comes back from that here, and so I'm going to grab the IP and the FQDN of that domain controller that I just spun up in Azure. Now, the other Azure cmdlets have this command called Get-AzureRemoteDesktopFile. I haven't found the equivalent of that for the RM set of cmdlets, so I'll I did was just do a Start-Process on the RDP client and pass it in either the FQDN or the IP, either one. It doesn't care that I pulled out of that other information there. So now, I'm going to log in here as alpineskihouse/adadministrator. In Azure, you can't use administrator for your AD name for your administrator account, so in this case, the template said let's call it AD administrator and I'm fine with that. And I'll put in my super-secret password here, make sure I type that exactly right. Now this might take a minute connecting in for the first time. It's a brand new virgin VM out there in Azure. It might just take a minute before it knocks the cobwebs out of its head and it gets ready to roll. Here we go. It's spinning up. So what you're watching on the screen right now is a brand new pristine Azure domain controller spinning up for the very first time with an RDP session to it. And here it goes. Let's take a look here. AlpineSkiHouse logging in and now I've been talking this whole time for 37 minutes, but really when we started this whole thing, I just ran three commands. And now, less than 30 minutes later, I've got a fully populated Active Directory domain controller ready for me to script and do whatever I want to do to test, so I'm not doing those things in production, right, so I can test now out here on a clean VM. So it's spinning up and as usual, it's taking a minute just the first time for the server to wake up. So what are the things we're looking for now? We want to go see, first off, is it a domain controller, did it really work, did it promote my domain controller, is the data there, do I have the OUs, the users, and groups, is the recycle bin enabled? Alright, I want to see if this thing is actually configured the way we defined it in the configuration. So I'm going to go over here to tools. First thing you'll notice right here, remember the Active Directory RSAT we installed in the configuration. The RSATs there. So now I'm going to go over here to Active Directory, let's see, Administrative Center. There we go. And the Administrative Center comes up, I click on my Tree view over here because that's my favorite view, I click on the domain and over here on the right, first thing you'll notice is it's grayed out where it says Enable AD Recycle Bin, it's already done because the configuration told it to enable the recycle bin. Now right here in the front list, you can see there's an Accounting OU, there's an IT OU, a marketing OU, an Operations OU. Let's drill into one of those and there we go, Computers, Groups, and Users exist. Now my Users OU, there's Debbie and Harriet, and if I look in the groups there's G_Operations, which matches the department name in the OU there. I open that group and I look at the members in that group and there's Debbie and Harriet. So I've got an OU that has a group that has users, users in the group all with DSC out-of-the-box. Alright, now let's go down and take a look at the users default container. Down here, I have TestUser1, TestUser2, TestUser3, 4, 5, TestGroup1, 2, 3, 4, 5, and if I look in TestGroup1, there's a member TestUser1. What do you think? (Applause) Does that look handy for you? Yep. Alright. So we've now in three PowerShell commands, you run those commands before you go to your morning status meeting, you come back from your status meeting, you've got a domain controller ready to bang on. Alright, just like that. And then when we're all done at the end of the day, we've got a couple things here that we can do. Number one, if you've been in Azure very long, you've already learned this lesson the hard way. Shut down your VMs at the end of every day, right. So you want to run this command, Get-AzureRmVM. This makes it so easy using the new resource manager cmdlets. Give me all the VMs in my resource group and stop them. Done. They're stopped for the day or I can literally just run this one command, Remove-AzureRmResourceGroup, I want to run that. Bada bing, bada boom. And if I come over here to my Azure view, all my resources, you can see I've got two of these out there. I've got the PSH one and the AD one. I built one just as a fall back last night. But if I keep refreshing this view, you'll watch these resources start to drop off as it deletes them when I'm removing the resource group, and so it's performing that target right then. It's going to remove it and then we'll be all done.

Closing and Q&A

So here's your call to action, help us grow the xAD resource in DSC. If you've got a little bit of AD scripting skills, even if you don't have any AD scripting skills, just go watch my videos out on Microsoft Virtual Academy. Anybody watch those? There's a full eight sessions, full day of teaching you how to do Active Directory with PowerShell and then go start hacking away on these resources out there and help us grow the Active Directory resources. Now try it yourself. There's the URL, github.com/goateepffe/azurerm. You just go out there. You can download that one calling script file, you can tweak the parameters, the names of things, whatever, and then just run it and you're done. It'll spin you up an environment. Just like we did right here, you can do it yourself. And then, once you've got those files, just start editing and playing with them and make it work for yourself. Question? How do you set the holes cache on the second disk to _____? I didn't have to because over here in my azuredeploy.json, it does all that for me. Let's see if it actually, it doesn't show that. Let's see if it's got adDataDisk and the adDiskSize, somewhere in this long JSON template, it's set all that for me and that's the thing I like about these JSON templates. I don't have to know all that Azure business, alright. I just deploy the template and it builds it all for me. At the end of the day, I blow it all away and I don't care. So it's that easy. Yeah. And let's see, we're still deleting that group. And so now, one last favor I have to ask. Did anybody have to beg your boss to come to this event? Alright, so like, yeah I know it's like, you know, funding travel, budget, conference, all that stuff. Training budgets are really hard to come by. Well I kind of have to do that with my bosses too, so if you could send me a tweet @GoateePFE and just let me know was this helpful. Is this something that you're going to use? How can you see some impact in your environment? That'll help me sell my boss on coming back next year. Alright, appreciate it. Alright, yes. Would you like a copy of the emails? Oh yeah, I'll take a copy of the emails. Thanks. When they come in, I'll send them. Alright. So we've got about two minutes left here for questions. First off, was this helpful? Yes. Alright, good. Alright, so now you've seen what DSC looks like for Active Directory, and on top of that, now you can spin up a domain controller to bang on. Now this is just the scratching the surface here. You can deploy multiple machines with this. You can deploy 10 domain controllers and you know build it all out with different roles and all that and you could even, you know, kick off some other scripts to configure your environment. We don't have resources available yet to set FSMO roles and things like that, so we need help building those, so help us with that. Yes, question. Is there a similar infrastructure to the AzureRM if you're hosting locally? Like on Hyper-V? Is there a similar equivalent set of cmdlets like… Right, like if you've got a DSC template that you want to spin off of a VM in Hyper-V. For Hyper-V, not necessarily. We have some Hyper-V resources that you might be able to, but you couldn't do it exactly this way. Now you might check, keep an eye on Azure Stack when that comes out for onsite, you'd have your own private cloud stuff, see if that might work with the AzureRM cmdlets. I don't know. I can't make any official statements. I don't know what's happening there. Other questions. Yes. Do you know anybody using the Active Directory DSC stuff for like production systems like for users and stuff like that? I haven't heard of anybody using this much in production systems yet. It would be kind of one of those things you'd only want to set it once with just the install, right, so apply only for your LCM, right? That's what I just offhand I'd heard somebody. I don't know if it was at the summit last year or somewhere else and someone had said and referenced to to Active Directory DSC for like user provisioning and some OU provisioning and stuff like that where it was apply only. They weren't using it to build like refresher _____. Yeah, you could do that, but I just don't know if that's practical. I mean, you can just run one line of script to create a new user OU group anyway. Why would you do that with DSC? And somebody was asking me this last night and I thought, well, you know for me, I support Active Directory, so all day long I'm standing up AD test environments just to beat on and script against, so for me, this is really handy. I could run this all the time now. And for you guys out there where you need a test environment, you don't have to go through all your VM requests, you know, act of congress, you could just spin one of these up and then destroy it at the end of the day, so. Alright, we're out of time. Thank you all for coming and please fill out our email. Thank you.

Advanced Parameter Completion

Introduction, Using ValidateSet, and Dynamic Parameters

Alright, so hey everybody. Welcome to the PowerShell Summit 2016. My name's Rohn Edwards. Today, I'm going to talk to you about advanced parameter completion, so how to get IntelliSense to help work for you for your commands, even for other people's commands, you know, commands that you didn't design, how you can add your own completion in. So I actually don't have any slides. I took the last one out this morning because I found they took too much time and they were pretty boring and I cover the same stuff in the demo code. So we're going to jump straight into it. But actually what I'm talking, obviously, I'm sure everybody knows, but you know, I have just a little demo command where Show-DateTimeCompleter and to be able to get that completion results to show up like that and to be able to dynamically change into really, really help add a little bit of polish to your commands. We're going to start, we're going to very, very briefly cover using ValidateSet to do this. I'm sure everybody's familiar with that. In the old days, years ago, PowerShell V3 came along and one of the nice things if a parameter had a ValidateSet attribute, you got tab completion in IntelliSense for free, kind of. So it wasn't really what ValidateSet was, it's not its main purpose to provide argument completion like that. It's really for data validation, but as a side effect of that, you got IntelliSense. So all of this code is going to be available after we're done, so you're going to notice there's a few regions and code that we just skip over, like I'm not even going to really cover doing a static ValidateSet. I'll show you that, but I'm sure everybody's familiar, that's in this room, with creating a static ValidateSet around a parameter. But if in the past, the problem with static ValidateSets are they are static. You have to know the values you're going to put into them at compile time, at your function or command definition time and that a lot of times doesn't work so well. A lot of times you need to know that information at runtime to provide dynamic ValidateSets to your users. So the way to do that, of course, is with dynamic parameters. And by the way, can everybody see that? Is that a good size? Alright. So we have just a real simple, this is what using dynamic parameters to create dynamic ValidateSets looks like. It looks like a lot of code. It looks kind of scary, but if you've done anything with dynamic parameters, you're going to recognize what's going on. You start out, so what we're going to do is create a function that has a single parameter. It's going to be called FolderName and it's just going to get the, it's going to list the folders at the root of the C drive. Right, so in the DynamicParam block, we do a directory listing of the C drive and we pull off just the folder names. And then a DynamicParam block in a function, its job is to return a dictionary of parameters that have been set. Nor a lot of times you depending on user provided values at the command-line, you may create a parameter that only makes sense in that, you know, for like Get-ChildItem, you can directory and file or dynamic switches that are added if you're in the file system. You get a code signing switch if you're in the certificate provider, stuff like that. So in this case, we're just going to create a parameter that is created at runtime every single time. Here's where we create the parameter dictionary that's going to be returned and here's where we actually return the dictionary at the very bottom and then in the middle is where we define that parameter. So before we define the parameter, we're going to have to tell it the attributes that we're going to add, so in this case, we are, we create a collection, and then we add to that collection. This is a little bit more verbose than it has to be, but I feel like this is a little more readable to understand what's going on, so we create a parameter attribute. This is the equivalent of having a bracket parameter, open and close parentheses, just an empty parameter attribute. And then we're going to create a ValidateSet, so this is the equivalent of, you know, if you had a static ValidateSet having the ValidateSet inside the brackets, and then in parenthesis, spelling out all the values. And here's where we create the parameter and add it to the dictionary. So and the only thing this function is going to do is just return the parameters that were passed to it. So if we come in here, oops, and don't overwrite the function, but instead Show-DynamicValidateSetExample-FolderName. So you can see this is the C drive is a little dirty. Actually, I have some random folders I created when I was going to try to cover a bug that used to be in ValidateSet, but you have to take my word for it. This was dynamically generated. If I went in and deleted a folder off the root of the C drive, this would change with it as well. And if you add version 3 or 4 or early, early versions of version 5, then this part right here might not have worked exactly as you anticipate. It wouldn't put quotes around anything in a ValidateSet with a space. I do have a work-a-round. I can't really demo it here because it is fixed in the current version of PowerShell, but what we're going to cover next uses the same type work around. So anyway, that's what it looks like to create a dynamic ValidateSet.

Abusing ValidateSet

So the next thing I want to show you is, it's the last thing we're going to do on ValidateSet, and it's really a way to, it's really, in my opinion, abusing ValidateSet, taking it completely past what its intention or what its use is for, but for the longest time before I knew about argument completers, what we're going to talk about next, I always wanted a way to use a ValidateSet the way that argument completers are used the way like when you're working in with normal PowerShell commands and they give you that IntelliSense, but you're not forced to pick what's there. I used to always look for a way to do that and obviously argument completers are the way to do that, but there is a way you can do it with ValidateSet, again, using dynamic parameters. I'm not saying this is a good idea, but it doesn't use reflection or anything like that. Essentially what we're going to do, if we look, so this function is going to do the same thing, except it's going to allow, let me show you what I'm talking about like I want Show-DynamicValidateSetExample. Maybe I want to be able to put in wild cards right there and have that actually bring back those two values. That's not going to work when you're using ValidateSet the way that we just did because pro* is not a valid folder on there. ValidateSet is doing its job and it's not allowing me to input that. But if you want to do that, it would look something like this. So here we're doing a dynamic parameter and this part right here is exactly the same as the previous one. We get the list of folder names, we create the dictionary, we start to create an attribute collection. This part right here is exactly the same where we create the parameter and we add the attributes to it and add it to the dictionary. The difference is this part right here. So a little bit of background, when you interactively type a command in PowerShell, the completion engine in the background, to do its job, it needs to know as much information about that command as possible. So internally, it's essentially calling like Get-Command to get the metadata for the command. So it needs, because it needs to know hey maybe they've provided like for Get-ChildItem, maybe they provided a certificate path or certificate store path, so I need to know that code signing is a switch parameter that's valid here so that it can actually complete those parameter names. So when you're interactively typing on the command-line, when it's trying to get the metadata for this command, it's going to internally call the DynamicParam block multiple times potentially. I mean you don't really get to control how often it calls that. So when you do that, the MyInvocation automatic variable, it has a property on it called CommandOrigin. It has two possible values, it's either internal or runspace. So when PowerShell's calling that on your behalf to try to Get-Command metadata information, it sees it as, it was called internally. PowerShell itself called this to try to get some information. So what we're going to do here is we're going to lie to the engine. When it's doing it internally to try to get parameter completion information, we're going to tell it that you need this ValidateSet. And then when the user actually, when you press enter and you submit your command, that's when the command gets run for the last time. It runs the DynamicParam block. It runs the begin, and your process, and your end blocks. And then in that situation, we're not going to add the ValidateSet attribute. Now again, I'm not saying this is the best idea. There may be reasons that I haven't even thought of that it's a bad idea, but it does seem to work. So if we Show-DynamicValidateSetSuggestion and we come in---oh one other thing, notice on the other one to show that end block was much simpler, I just told it to show the PSBoundParameters. Well in this case, because we're not letting ValidateSet do its job, we're actually on the hook for doing the validation to make sure that the user puts something in that is valid. So of course, if you try to use suggestions, it's going to be the same thing with argument completers. If you allow the user to type anything and you just provide IntelliSense to help them out, you're on the hook for validating that they actually put, that they gave you valid input. So anyway, let's come back down. So it comes up with the IntelliSense. Now can I type in that? I can't. So that's a way to, again it's abusing ValidateSet. It's not using ValidateSet for its intended purpose and the way, if you have PowerShell V5, you really shouldn't try to use this for any reason. PowerShell V5 offers argument completers an easy way for everybody to use them out-of-the-box without doing anything special. So we're going to--- I keep talking about these argument completers, I guess let's jump into them and actually talk. Before I do that, any questions on using ValidateSet to get IntelliSense or… No. Alright.

Argument Completers

So argument completers. So what is an argument completer? An argument completer is essentially, it's a ScriptBlock. PowerShell is going to invoke it when it goes to look to see, when you're typing your command, PowerShell behind the scenes is doing all kinds of completion events, so when you start to type Get-Command, you know, as you're typing behind the scenes, there's a global function called TabExpansion2 that's getting invoked that ends up passing everything you just typed along with the cursor position and all that stuff into a helper function and that helper function is pretty complicated, but it ends up figuring out whether or not the cursors on what's probably a command, or parameter name, or parameter value, and behind the scenes, these completion events are occurring. And so when you go to start typing, well that should have popped up something, there we go. So at this stage, you know we could have registered what's called an argument completer and PowerShell would have known that we've given this ScriptBlock the responsibility of showing the user what valid parameters are available. And the completers themselves, they're actually just, they are just ScriptBlocks, so we're going to make a very, very simple one real quick. And you register them, you tell PowerShell about them with this Register-ArgumentCompleter command. So real quick, this presentation is mostly geared towards version 5, but almost everything I talk about here does apply to version 3 and 4, but you do have to do something special. That Tab expansion2 function, or yeah, the function that I mentioned, in version 3, it doesn't provide an easy entry point into having these custom argument completers. There's a module, the easiest way to get this available, there's a module called TabExpansionPlusPlus. It's created by a PowerShell team member. I believe it's the same guy that made PSReadLine. And essentially, so when I call Register-ArgumentCompleter here in version 5, this a native cmdlet, but if you have that module, it's going to essentially do the same things behind the scenes. It sets up, there are dictionaries of argument completers that all this stuff goes into. So what we're going to do, we're going to register an argument completer for a command called DummyCommand, which isn't a command yet, its definition is right here, and all we're going to do is output to strings. Alright, a script, an argument completer ScriptBlock, it's supposed to return completion result instances and we'll talk about what those are in just a minute, but it turns out PowerShell lets you do a lot of stuff, sometimes it will get you in trouble and it turns out that a regular string can just be coerced into a completion result. I'll show you why you probably don't want to do that in a minute. But this is an argument completer that's just going to tell it that there are two strings that are available and I just ran Register-ArgumentCompleter. That command has not been defined yet in my session and that's okay because all Register-ArgumentCompleter did was put this completer into a dictionary and it was associated with the DummyCommand, CommandName, and the Parameter, ParameterName. So let's read that in and type out DummyCommand. When I type in parameter, I hit space, nothing happened. If I hit Ctrl+Space to actually tell the completion engine to bring up the IntelliSense window, you notice, I don't know if you've noticed this or not, but I'm calling it out so you should notice it in just a second. There's no little icon right here, that's blank, and that's because we just let those strings get coerced into completion results. The completion results are what are used for CommandNames, for ParameterNames, for variable expansion, all that stuff and you're supposed to tell it what type of completion result it is. And so in this case, IntelliSense saw that this wasn't a parameter value completion, so that's why it didn't automatically show it. But let's go ahead and pick one of these. Now if you notice, I just picked that one. There's no quotes around that, so it's kind of like what I was saying earlier there was like a bug in ValidateSet. Well this actually is, in version 3 or 4 there was a bug in ValidateSet that would do this. This isn't actually a bug though. You want to be able to, and you can and I'll show you in just a second how, you want to be able to control exactly what is returned to the command-line as that parameter value. So it is on you if it's something that needs to be quoted, it's on you to get it quoted. So another problem with this, I can type in gibberish, and I can press Tab, and it goes through. So what's happening there, that ArgumentCompleter ScriptBlock, it's doing exactly what it was told. Every time it's invoked, whatever it returns, that's what the IntelliSense is going to show. So we told it to output those two strings. So it doesn't care what I have. I can start this with a G and that should come back with the goodnight string, but when I hit tab, it goes Hello World in that order. So every single time that ScriptBlock is invoked on behalf of PowerShell, it's going to output everything in that order. So if we come in and let's first fix the quoting problem, right. So here's that same completer. This time we're going to fancy it up some. We're going to take each one of the strings that we wanted to output and for each one, we're going to do a really bad check for how to whether or not it needs to be quoted. All this is doing is taking spaces into account. There are other reasons you wouldn't want to quote a string like parentheses and any other number of them, but for this instance, we're going to treat, we save that string off and then we determine whether or not we need to quote it, and if so, we add quotes, and then we're going to end up calling, creating a new completion result. And so, CompletionResult new, so it has two constructors. We saw earlier that it will obviously allow just a regular string and when you give it just a single string, it ends up for these three values that take strings, it just uses the same one for all of them. But here's something really neat about them, they allow you to have different values, this first one, this CompletionText, that's what's actually going to be put on, that's the value that will be put with the command, whether or not that's up in the Script pane or down in the command-line. The next one is what the IntelliSense menu is actually going to show you. Next, we have the type of completion result and if you're using Register-ArgumentCompleter to do an argument completer, you're probably always, I don't want to say you're always going to, but I can't think of a time you wouldn't tell it it's a parameter value completion type. And then you get to provide tooltip texts. So in this case, we're just going to put the word Tooltip in parenthesis, so you can see that it is doing something. So we do the same thing, Parameter, and now you notice Hello World and Goodnight Moon, they don't have quotes around them, but when I press Enter to select one, they do have the quotes. We still have that problem though where it's not taking into account whatever we typed. So in order to do that, that ArgumentCompleter when it's being invoked, it has a lot of arguments that are passed to it to give it information about the current command that's running. So instead of just telling you what they are, I'm going to demo it real quick. What we'll do, we're going to run, just monitor a little log file here and we will create an ArgumentCompleter for that same command that's just going to output stuff to this log file that we're watching. And so, we can come in now and again, try not to override that, DummyCommand -Parameter. And so, if you look up here, five arguments were passed into it. The first argument is a string and its value is DummyCommand, so it turns out that first argument is the CommandName. The completer is made aware of what command it is acting on behalf of. The next one is a string and its name is Parameter, so that matches up with the parameter name. The second argument is the current parameter name. The next one, this is blank right now, so if you come over here and type in gibberish and then invoke that again, you'll see that the value for the third argument is gibberish, so that's the word to complete, that's what the user has typed in. The fourth one is a CommandAst object, so you're given the entire abstract syntax tree of the command. So if you want to do something super crazy and advanced, you can have it. It gives you all the tools you need to do make completers that depend on other parameters that the user has typed in at the command-line or you can do lots of stuff with it. The last one is a hash table and that's, you're going to see that referred to in a little bit as the fake bound parameters. So you can see where it's got Parameter1 there. If I come in and give it, you know, we'll do OutVariable, just one of the common parameters and call that again. You can see down at the bottom where it has those, those are available. So I could do a check inside the completer to see, maybe I need, maybe I care what's in OutVariable, so I can check and see that. Now it's called the fake bound parameters. It's not truly, it's not doing everything that the real parameter binder would do once you submit a command. If you were to try something like this, just put a command inside parentheses and call it, even though echo var is going to do essentially the same thing as, or effectively the same thing as just putting a string out, it's not going to bind that. It's not considered a safe value. You could have anything in there. You maybe call a new item or something like that and you don't want to actually do that until the user presses Enter, so it's not going to necessarily bind everything, but if you look in the AST, you would be able to see everything that was typed, so if you had to, you could still go in and do the hard work of looking through that. So knowing that, let's now go back and modify our simple completer. It doesn't look quite as simple anymore, but this is exactly the same as the last time, except we have this ParamBlock now so that we can use the commands easily by name instead of having to do arg0, 1, 2, 3, 4. And the only other difference is right here, we do a little filter because remember the ScriptBlock is going to be invoked and whatever this ScriptBlock sends out, in this case we're going to construct completion results, but whatever it sends out, that's what's going to be returned in that order. So if we put a filter here to say these original strings, they need to be like whatever the user has said, and we have a wildcard on there, so if the word to complete is empty, then it's just saying if it's like any of those. So let's run that DummyCommand -Parameter and now let's see what happens. Let's hit Escape. The test right here is, so it went straight to Goodnight Moon. It didn't go to Hello World, gibberish. So I promise I'm pressing Tab, Ctrl+Space, nothing is happening because that doesn't match any of the completion results and this is what I would expect to happen with normal commands that have argument completion.

Improving Get-Command

So let's take that kind of to the next level, alright. So now what we're going to do, I want to show that you can actually add argument completers to commands that you don't even own, to native commands if you want to. I mean you want to be careful with that, but so I picked Get-Command to kind of pick on. There's you know I---ignore the title, Improve_Get-Command. It actually---Get-Command as what it does is amazing. Alright, there's nothing, I have no faults with that, but it'd seem like it would be a command to kind of pick on to show how you could have completers that depend on other parameters that are being passed in while you're typing on the command-line. So Get-Command, you know, it obviously it has some completion that happens natively, right. So if I type in module and bring this up, this takes a little while, it was able to complete that. It knew the available modules. Now if I type in -Name and hit Space, now this one's kind of funny. I had to hit Ctrl+Space to bring it up and if you look at that icon, that's not the parameter value icon. But still, it provided completion for this and it is filtered on just that module. If I didn't put any modules there, then it should have just had all the commands available. I can do the same thing with -Noun. This time around it looks like its, the default completion isn't doing parameter values, so again you have the blank there and I had to hit Ctrl+Space, but this is still filtered on module. But you can't do something like -ParameterName and try to bring up completions for that. That's not going to work. It doesn't provide ParameterName or ParameterType completions, so we're going to try to fix that. Now we're going to skip over the one that's labeled First try. You guys can look at that later and it's just going to be this exact same completer here, but just with not quite as much. But what we're going to do, I want to, actually first, before I embarrass myself, we have some helper functions, this wouldn't have worked earlier or later. We have one called ReregisterCompleter. What this is going to do every time we modify that completer ScriptBlock, this things job is to just go in and for every single parameter or not every single parameter, but all of these parameters, it's going to register this completer to the Get-Command command and to each one of these variables, so we'll be able to just, instead of having this foreach, we'll just call Reregister, oops, command. NewCompletionResult, this is just going to be, this is the helper function and you're going to want to do this probably. Remember how that simple completer earlier turned into something that's not quite so simple, it's got a whole bunch of lines. This makes it so that you can just take a string and send it to New-CompletionResult. It'll figure out if it needs to be quoted. In this case, this is mostly from the TabExpansionPlusPlus module. I did that one thing. This isn't the best practice here. It saved a couple lives for the demo, but this is actually going to take the word to complete into account from its parent scope, you know where it's being called. Just be aware of that if you try to take this and use it somewhere else since you probably don't want to do that in your real helper function, but we'll read that in. So now we're going to come down and here's kind of the neat thing. Since the completer will know the command and the parameter it's working against, you can do something like this where you use the exact same ArgumentCompleter code for something, for one command you can just have the same completer code, and in this case, you know, we have a check in the beginning, this isn't strictly necessary, but if this completer accidentally got registered to another command, we do a little check to see whether or not we should just return without putting any results out. If this accidentally got sent to another command, you don't want to confuse the output with completion results that don't make sense. The next thing we're going to do, we're actually going to call Get-Command on itself and we're just going to splat the fake bound parameters into it. So one thing we're going to do though, whatever the current parameter that's being operated on is, we're going to add a wildcard to that because otherwise, as we're typing a command, if we type -Module and we type three letters of a module in, that's going to mess your completion results up later when this gets splatted because that would actually splat those first three letters to Get-Command. And then we're going to, depending on what the parameter is, we're going to do something, depending on what the ParameterName is, we do something a little bit different. So if the current parameter, remember Get-Command has a ParameterName parameter, if its ParameterName, then we're going to figure out whether or not the ParameterType has been specified and if it has, we'll filter on, I mean literally whatever Get-Command returned earlier is stored in this ValidCommands, so we check the parameters dictionary for each one of those and then all the values, we filter them, we sort them alphabetically and get them a unique value and then send it down the NewCompletionResult. ParameterType is pretty similar. We're going to, just in that case, instead of getting the keys off those parameter dictionaries, we're going to get the values and then take their types. And then if it's any other parameter like verb or noun or name or module, we're going to take whatever those ValidCommands results are, whatever the results from a call and Get-Command are, and we're just going to select that property off of the commands. So what should happen… Question. Sure. Do you have a recommendation or idea when you might want to do multiple completers inside of one completer like you're doing versus first string multiple ArgumentCompleters? Yeah, so the question was do I have any recommendations on when to do separate completers. In this case, we're taking the exact same one and registering. And it just depends. You look at it by case by case basis. So in this case, if I were to make a separate one for each one, there would be so much, all the code would be duplicated, except for this switch statement. So in that case, it makes sense, they're all doing essentially the same thing. They're going to call Get-Command with whatever else has been passed and it's just if you look at it and it looks like you're reusing the exact same code over and over again, I would put it into one completer. I don't think it would hurt anything to have a separate completer for each one. I mean, in the grand scheme of things, this is putting everything in a global dictionary that it's not, it's the same amount of work for the PowerShell engine when it goes to look up hey, alright, I've got this command with this parameter, is there a completer available? So it wouldn't have hurt to do them separately, but in this case because the amount of code reuse would be incredibly high, I just chose to throw them into one. Sure. Another question? Is that okay? Yeah, absolutely. So what is that with Microsoft.PowerShell.Core\Get-Command? So that's the fully-qualified command name. I mean and to be honest, it's probably, so Get-Command is something, so there's an order of operations or a precedence for different commands, so if I were to make a function called Get-Command, that would actually kind of shadow the real Get-Command because functions are looked up first and I think aliases are looked up before functions. So in this case, this is just a way to say go to the real Get-Command cmdlet. That Microsoft.PowerShell.Core, that's a way that you can access all the core _____? Well so let's do Get-Command Get-Command and let's see somewhere in there, I guess we have to… (Audience talking) Yeah, I see that. In the middle there. It may have been. (Audience talking) I wonder if it would be, hold on. Nope. So it's available from Get-Command. In a little bit right after here I'll show you how you can actually grab that information. I don't want to embarrass myself anymore up here. So we'll take that completer ScriptBlock and we're going to run the ReregisterCompleter, which took that ScriptBlock and said for Get-Command register this to the Name parameter, to the Module parameter, all that stuff. So now we should be able to come in and we can limit this to a module and we can do name. Now remember earlier the name, this was coming back with command completions, that icon was a little bit different, so this time the completion just popped right up so that shows that it is using our ArgumentCompleter. Let's actually do like -Verb and Add. Now we can come over to ParameterName. And so, if this is doing its job, any verb in the PowerShell access control module or any command that has the Add verb in the PowerShell access control module, these are the parameters limited just to that. And you can look, there's not that many of them. If we take this and change it and just do ParameterName, you'll see that it took a little bit longer and there are a ton more that come up. Same thing for ParameterType, so we can actually take ParameterName and let's say our Name -ParameterType and you'll see that out of all the commands in PowerShell, again if this things doing its job, then if you're looking at commands with the ParameterName name, then this says all the commands, there is a SwitchParameter, at least one SwitchParameter that has the ParameterName, at least one scale or string parameter and at least one Array String parameter. And actually let's get the counts. So I don't want to cover too much about what's this is just a modified version we use group object under the ParameterName and ParameterType conditions in the switch block, but for the sake of time, I'll just kind of demo what it does a little bit more and you can take a look at it in a little while. But we're at, we're going to add counts to that. So Get-Command -ParameterName, and so now it has counts in there and you know of course I didn't show this, but when you press Enter, it didn't have the count information. That was we could tell the list to show something different than what was completed. And when you do look at this, it seems like it'd be kind of useful to be able to sort it maybe on the count and you can do that. You can put whatever kind of logic you want in that completer. So if I put logic in that said take a look at that word to complete. Now I ended up doing an exclamation point here because if you do just the less sign, then the completer won't work. But if I put that in, I told it to change the way you sort it, so now it's sorting by the count, the greatest number of parameters versus the least number. So you can see that all these common parameters obviously show up the most. And these counts should change as we add different things to filter on, -ParameterName and let's this time do it from an ascending order, but you can see all the different parameters. So instead of the 292 or whatever, it looks like we're down to like 24 that have that. And that's just an example of a way to show completers that can take other parameters that are being typed into account in real time.

Native Completers

Alright, so next I want to talk very, very briefly about a different type of completer. Alright, so those were I'm going to call them normal completers or just completers. There's also something called a native completer. Native completers are geared more towards external light executable applications. So it's things that aren't PowerShell commands, but they still have arguments like the net.exe utility. You can use it to start and stop services and to access shared drives and stuff like that. So the big difference from the completion standpoint is it has different arguments that it receives. So when you take a look, we'll do that thing again where we have a little debug log up, and this time we're actually going to, so this is going to output to that debug log, but it's also going to output some completion results. This is geared towards, this is for ipconfig. Its arguments are really, really simple, so I chose it. But the way you tell it that it's a native completer, you call Register-ArgumentCompleter, you give it the CommandName, and in this case, and then you give it a native switch. So with normal completers, the ParameterName is what's required and the CommandName is optional, so I did kind of skip over that. With normal completers, you can actually say anytime you come across a username parameter, I want you to run this completer, and as long as there's not a completer registered for username with a specific command, then that one will win out. It will end up, you can have it to where a certain parameter is always, has a certain completer that gets executed with it. Native completers, though, they don't, it doesn't take parameter names into account, you register it to a command, and then you provide this native switch. Let's take a look at, so now if we call ipconfig and I have to do Tab or Ctrl+Space, but you can see over here, this time we got three arguments. So the first one is the word to complete, which is useful, the second one is the CommandAst, and then the third one is the cursor position, in this case, 17. So you don't get the CommandName included there. You can grab it from the CommandAst. You don't get the ParameterName, again you can determine that yourself, and you also don't get the fake bound parameters. The CommandAst, like I said, if you want to take the time to look into that, you could get all that information out of it. So anyway, in this case, what I did was looking at the CommandAst and counting the number of elements that were here and I don't want to spend too much time on this. Honestly and truly, if you're interested in these native completers, the TabExpansionPlusPlus module, even if you have version 5, if you go take a look at it, they have a lot of native ArgumentCompletion because it gets a little bit crazier because you have with native commands you might have like for net.exe, you have the first parameter determines what your next set of parameters is and you have all these trees of parameters that end up. But I wanted to mention this because in just a second, we're going to use native completers for something that they probably weren't intended for.

Completion Order

Yeah, I think we have a little bit of time for this. Let's talk about the completion order precedence. So earlier I said you can override the completion behavior of any command and any parameter. Well, I may have said any parameter. If I did say any parameter, I was actually technically wrong. If a parameter has, if it's an enumeration type or if it has a ValidateSet, you don't get to override that. You can put an ArgumentCompleter, register one all you want, but it's not going to let you do it and if you think about it makes sense because in those cases, you have to pick one of the valid values, and so there's no need to even return a completion result. There's also next up after that, PowerShell will look for the type and the attributes. After that, it will check the normal completer dictionary and whatever command you're running, if there is an ArgumentCompleter that's registered for that command and that parameter combination, then it will use that completer. If there isn't one registered for that, it'll check to see if there's a completer registered just for the current parameter name. If there is, it'll use that. So after checking that regular dictionary, if no results are returned, it may have found a completer registered, but if no results were returned from either of those checks, then it falls down to the next step here. And the next step here is something I didn't talk about and this is a version 5 only thing. I think they---well so there is an ArgumentCompleter parameter attribute, so if you don't want to call Register-ArgumentCompleter, you can actually put your ArgumentCompleter the exact same syntax that we were using earlier with the ScriptBlock, just decorate your parameter with it when you're defining your function. But if you have one of those set and you then you call Register-ArgumentCompleter later, it should be the case that ArgumentCompleter you registered would win out because this is a called after that. Next, there's the completion engine has some hardcoded completion behavior. So earlier when we were looking at Get-Command and you saw how name and noun had completion stuff, that was in there. If you ever notice like when you call new object, it does type, name completion, all that stuff, there's essentially it's like a giant switch statement that says well none of these other things returned completion results, so is the CommandName Get-Command and is it this parameter, okay, then I know what to do. If there's still by this point hasn't been any completion results returned, then it falls back to those native completers that we just talked about, so it checks that dictionary. So even for a normal PowerShell command, it'll check the native argument completer dictionary for that. And then if you still don't have any completion results returned, then it just kind of does the whole, let's see, yeah, it'll just fall back to completing for the current file system, right. And in here, there's some demo code that kind of shows registering multiple completers and then going in and watching how they're called and it sets up the little log file on the side.

Abusing Native Completers

But instead of going through that, I want to actually show you a way that you can, I call it abusing native completers, I doubt, I mean it may be the case that native completers were intended to be able to do this. It may also not be the case, so results may vary. But I think is kind of cool and in my session on Wednesday I show kind of an actual use for it. So I've got a real simple, very rudimentary function that I wouldn't trust outside of this demo that goes through the AST that's passed and tries to do like some fake parameter binding like what the user actually passed in, right, and it will allow, it will look at the command metadata if it's available, but it will also allow like a parameter that doesn't exist in that metadata to kind of be bound. We do this--- we're going to set up a little log over here and then what we're going to do is I'm creating a command that has, it's just show native completer extra info. It's going to have two parameters and down here, this is the important part, we're going to register our native completer for it and inside the native completer, we are going to, you see command name is not available, remember that in the ParamBlock it's only going to get three parameters passed to it, so the CommandName like in a normal completer is not available, but we'll get that from the CommandAst. And one thing to note about this, native parameters, so with a normal completer, you could register if an alias is called, the normal completion engine will be smart to know like you called this alias, but you were actually looking for this command and it will pull the right completer essentially. With a native ArgumentCompleter, you're kind of stuck. If you make an alias after the fact, it's not going to get called here. It's literally looking for whatever command you just typed, it checks the dictionary, so it checks for the alias name and if it's not there, it doesn't run the completer. You can register your alias if you'd like, but... Next, it goes through and calls that Helper function, the parse parameters, and it takes the cursor position into account because it needs to know what the current parameter name is as well. Then we get the ParameterName from that and it's just what the helper function output, we get the fake bound parameters, and then we're just going to output all that information. So this thing doesn't do anything, except output information, so Show-NativeCompleter, extra in, did I not, maybe I did not define that. Hold on. Alright, so we can come in and we can type -Parameter1 and you can see, so we have that information, we have the CommandName up top, we have the ParameterName, the words are complete, I can type that in. You could see earlier this is just for that little helper function, you see that it's bound, it's true. It's treating it like a switch statement because it saw that there's a parameter name, but there's no corresponding, no value that matches that, but what's neat, so you can do that, right, we can come in and say Parameter2, which was something else so you can now see in the fake bound parameters at the bottom that it knows something's there for Parameter1 and for Parameter2 and the logic could be changed so that it, if it knows where the cursor is and nothing's been provided yet, it could just not put that in the bound parameters. What's neat though, is you could say something like -fakeparameter and you can do that, and so it'll do there and another one. And so you can see in the very bottom where it's treating though as you're typing them, you could actually have your completer go through and check for… Got a question. (Audience Question) Oh, so that's just all I'm doing there is I create at the very top here, I just create a file and then I do Get-Content -wait and I'm just doing it in a separate process right here. But yeah, that's kind of neat. We're running, let's see what time we've got. We've got a couple more minutes, so any questions about this? I actually do have kind of a use case for this in my session on Wednesday and if anyone's curious about other stuff with this, you can use it. And again, it might not be the best idea. I do a lot of stuff that's just playing around at first and then hopefully good ideas come from that. Sure. Do you have any suggestions on how to parse the arguments for native commands? Not really. So yeah, I'm not super familiar with doing with that stuff. What I would say, the TabExpansionPlusPlus module. If you look into that, they have and it's on GitHub, I've seen other completers that other people have made, so there's a ton of native command completers. So yeah, I actually originally wasn't even going to cover native command completers and I was like well I kind of need to do that and then playing around with it was when I discovered this. This was like two weeks ago and I was like, oh that's pretty neat, I need to put that in. And I was able to take it and apply it to something else I was working on, but unfortunately, I'm not--- it's just going to be---the CommandAst will tell you, you can easily---let's see. Let's come in and modify this guy to, so we have access to that Ast outside of the completer. So with any luck, so if we do what was it, Show-NativeCompleterExtraInfo, we'll treat this like it's net stop wauserv, right, so when we do that, with any luck, yeah, so we have the CommandAst there. So this is what we would have had access to inside that completer, so you can see that it's, GetType somewhere, it's a CommandAst, and so on a CommandAst, you have there's a Get-CommandName, but you also could just take this CommandElements, so you can see that there are three elements and that's right because we said CommandName, the word stop, and then a ServiceName. So you could always say that the first CommandElement should be the CommandName, the next one, if you were doing one for like for net.exe, you could check the next one and you could see alright it stopped or it start, so I need to go in and start doing stuff. I need to get a list of services and you could say it stopped, so let me go get a list of services that are running, and you can get as complicated as you want with that. The one thing you want to be careful of, anything that's expensive, IntelliSense will time out after, I don't know, it's like a couple hundred milliseconds, so you don't want to be the cause of it like hanging. Sometimes it times out and it's super quick and everything's responsive again, other times it kind of hangs and has trouble stopping. So if you're doing something like expensive that's going to take a little while to get results for, you may want to try to cache that somewhere. What we didn't get into was how to tuck this away into a module scope. If you have a module and you register your completers all inside of it, that part's easy, it should just work if you want to do that after the fact. And also, if you want to use it with TabExpansionPlusPlus and this is kind of in the notes on that last demo script. It'll explain that TabExpansionPlusPlus, will strip off, if you're bound to a module, it will strip that binding and then bind it to its own module, so you have access to its helper functions like New-CompletionResult, stuff like that. So it kind of covers a way that you could, you can just call Get-Command, you can get the current command's info and get its module scope and execute stuff in that. So I think that's about it. Any questions? Alright, well thanks everybody. I really appreciate it.

Assume Breach - Designing for the Inevitable

Introduction

Alright. Everybody, I'm Lee Holmes if we haven't met before. I've been an engineer on the PowerShell team since the beginning and always been involved deeply in security. So how many people here are involved in some degree of creating, creating any infrastructure, creating any software, creating any websites, do any of that DevOps stuff. Okay, so everybody, you know, we're involved with creating stuff. That's what we do day to day. Now when you start talking about creating stuff, you're going to be really happy at the end of I created this thing, it's beautiful, put it up on the web, it's so shiny, and you call your mom, it's like hey mom take a look at this. I've got stuff on the web and she comes and takes a look at it and she goes why does it say hacked by Russia? So the thing is, as happy as we are to create new stuff, other people are just as happy of breaking new stuff. When we're creating new stuff, we've got to keep in mind that there's another side than just the glorious creating of new things. There's also making sure that the new stuff that you create doesn't become the next newspaper headline. So we're going to talk about how you define and how you design software infrastructures and components and designs that are robust and able to prevent some of these attacks that you keep on hearing about. Now the key for doing this is called assume breach. If you were at Jared's talk just before, he talked about part of his job is customers. He comes in and helps them find attackers on their network. Now we used to be in this place of designing software where we said we're going to set up this really strict, strict wall on the outside. We're going to put up a web server and we're going to have like people with AKs like watching for everybody to come and were going to just like lock that down and that's about where we think we're done. Where we have to move, it is absolutely impossible to truly, truly defend that boundary. Where we have to move when it comes to software design is assume breach. Assume that people will come in and then take steps to design your architecture in such a way, design your systems in such a way that you can limit the damage that happens from that breach. Now you're saying Lee, you've got cars up here, so let's talk about cars. There is a company or there this is an institute out there, a non-profit institute called the Institute for Highway Safety. It's actually driven by insurance companies. It's non-profit and they look at real data in the real world about how people are getting impacted in terms of collisions and everything else, so they obviously work to reduce the number of collisions, so making day time running lights and that kind of thing. But what's important is that they also constitute and they consider the crash scenarios. They want to make sure that when the crash actually does happen that you're more safe than you were before, that you're more likely to survive one of these crashes. So they were funded in 1959 and they've been doing some work since then. Now they have a couple tests. So you tend to see the straight head-on crashes, two cars smashing together. They found through data that that's not really the kind of crash that happens. What often happens is they call it the overlap test, so you really have two cars, maybe they're going head on, but at that last minute they swerve and they only crash 40% of the bumpers. They've found also that there's another type of crashes where people just like drift a little bit in their lanes and they have a very, very small overlap in their crash, so 25%. Maybe they hit a telephone pole or a tree. They, of course, they do side crashes, rear impacts, things like that. So these are the kind of things that they test for and see how well vehicles survive. Now in 2009, they marked their 50 year anniversary. They crashed a 1959 Chevrolet Bel Air, which was a regular car at the time, with a 2009 Chevrolet Malibu, which was a pretty good performing car in terms of crashes. So this is what 50 years of assume crash brings you in a world where people's lives are in danger. So you got the red team at the left, crash! This looks brutal. You never want to see this. Both cars look like they've been destroyed. You see the front ends completely crumbling, bits and pieces everywhere. This is going to be a bad traffic day. What you see here on the right, the Bel Air, this is where it gets interesting. Not only the front crumple zone collapse, but so did the entire passenger compartment. Now if you take a look from the side of the driver, no seatbelts, no airbags, steering column comes right at him. He's not being protected by anything in the assume crash mentality. If you take a look at this, they think that this probably would have been a fatal collision. This is a 40 mile an hour collision. This is not that much. Now you're taking a look at the Chevrolet Malibu. This is a very controlled crash. You see the airbag deploys, the persons impact and recovery within the car is very well controlled. What they found is that this person probably suffered a minor foot injury. So when you take the assume breach mentality, so the assume crash mentality, when you go from just this idea of hardening the outside to you know what crashes are going to happen, we need to take steps to make our systems and our cars capable of remediating this. Then you get from a situation of a life-ending collision at 40 miles an hour to somebody getting a minor foot injury. We can do the exact same things with our systems. So you see here, just cosmetically at the top, they look pretty bad, but take a look at the passenger compartments. The Malibu, like the door opens still. The Bel Air, you know, you're completely crushed. Now an interesting thing too is they just started the small frontal overlap testing, 2012 they introduced this getting new data that these are very dangerous collisions. There were some poor performers. When they started crashing cars with the small overlap, a lot of fatal injuries. In only two years, they started doing some research. What are some of these cars doing that are making these crashes more recoverable? So in 2014, they had a journal publication taking some common design practices from the auto industry for people who had been very successful at this small frontal overlap testing. They called it the stop car crash journal and it's actually the guy's name. I think he was born for this because you don't want to start the mess for sure. So here we're at some of the techniques they used. So nearly everybody did a bunch of reinforcement around the doorframe. So obviously, when you've got a small frontal overlap, the doorframes and the car frame takes a lot of load. A lot of people added a side frame that was also kind of very strict and rigid. The ones that did the best integrated well with the doorframe so that there was an additional load path during an impact to spread it throughout the whole frame, rather than just crumpling that front corner of the wheel well. They added engagement extensions here you see at the bottom left. Now the trick for these things is that these things were actually more robust to a crash than the rest of the car. So this thing engages first, sends the car kind of more sideways and more of a spin, and what that ends up happening is you don't have as rough of a deceleration frontally and it gets kind of sent into some side spin. So everybody who was successful at this had to make sure that the side spin didn't send you out of the way for avoiding the airbags themselves, but a couple kind of design principles that people could use and now you started being able to see these design principles coming into other car models and doing a great job at vastly improving their ability to survive one of these attacks. Now we're not here obviously, we're not the auto industry. We're talking about cyber, right, cyber threats, cyber intrusions, all that kind of stuff. So the Verizon Enterprise Data Breach report is really good. I find it really good reading to understand the context of the industry and what are the kind of attacks that we're seeing. A lot of times you're going to notice here down in the near the bottom there's the professional column. That's kind of the stuff that we'll be involved with a lot. What you'll see here is cyber espionage is getting a ton of the types of attacks. You also see crimeware is being used in a ton of ton of the attacks. So this is kind of the thing. This is kind of like your small frontal overlap and your large overlap and your side crashes. What can we do to start preventing some of these cyber-attacks and some of these crimeware attacks? Now when somebody finally gets in with crimeware, what does it look like? Crimeware is a pretty big thing. We call it malware, but malicious stuff ultimately get used for something else. Now when somebody clicks on a malware, they get infected by malware, the malware is most commonly going to be used for further command and control within that infrastructure. Maybe it'll be used for a commanding control infrastructure for another attack, but it's likely that you're going to have persistent interaction with an attacker. Sometimes it's used just as a denial of service, bot, or a host for something else. Another one here is cyber espionage. So this is like the APT kind of stuff, right, the bad actors from Canada. So mostly what they're after are secrets. They get into your enterprise and they're going to be looking for credentials, databases, all that kind of stuff. So we can take a look at the kind of attacks that are happening in the industry and start to let this inform the way that we talk about our systems and the way we consider our systems.

Your Turn

So now it's our turn. We're actually the ones who are sitting up here defining new infrastructures, creating new software, defining new networks. This is us, right. It's our job to figure out how we defend against these attacks. Now what's the solution? Is it going to RSA and going shopping at the vendor floor? You could try it. Everyone loves to sell their genie in a bottle that solves all your problems. The answer is hard work. Just get out there, put in some elbow grease, and think. That's my daughter welding, by the way. Now the key is to realize that breach, it's not a binary state. You don't go from like happy, happy days, the suns out, the suns shining to like oh I'm breached, it's all over, things happen in stages. You have, for example here, let's talk through one. You've got an initial attack. Somebody gets a suspicious email, they double-click it, they open it. A regular phishing attack. They're just brutally, brutally effective. Maybe they have some access to some test infrastructure, so this is they take the attacker now who's got control of the account that they just phished, takes those credentials, and accesses some test infrastructure you have. Now maybe that infrastructure once they've compromised that, they've got administrative access to this test infrastructure, maybe somebody else coming into this infrastructure has administrative credentials to the production infrastructure. So now they steal those credentials, start to deal with the production infrastructure. So now they transition to production off of those stolen credentials, right. Now they're into production. This is where they're starting to get close to their endgame. Now they start stealing data, they have to collect it, zip it up, do whatever. They're searching through your infrastructure for sensitive things. Exfiltration. They're finally taking that data, sending it to somewhere that's important, maybe they're doing this through, you see a bunch of outgoing web traffic from a host that's never done it before, they've got to get the data out of your system somehow. And then finally, even still, once they have the data, it's not game over. They do have an endgame. The data that they steal, maybe they're going to use this for intellectual property rights, maybe it's a patent that you're about to file and then they can beat you to market with it, maybe it's a user database, maybe it's some other sensitive information. So you see here that the breach goes from a very, very early stage. You have the ability to write software and architecture that can very, very, very much slow these down at all phases. Now when you're starting to take a look at how do I design my systems to be protective against these kind of issues, you see a couple common themes. Now successful architectures, they follow the principles of least privilege, least capability, so only let your systems do what they should do. Data that you store is a liability, really, like when you start taking a bunch of data. If you don't need it, don't store it. And the other thing is don't let success be your downfall. Now if you think about what are some situations where what if everybody was doing it? That's a good thing to put what if every host had this or what if every company in the world had this. People tend to when they're creating their own unique snowflakes not think that this at scale could be a very, very big danger. So least privilege and least capacity. The idea here is that these attacks as we showed in that kind of breach diagram, people aren't just running through and like get in, do one thing. They're compromising some bit of your infrastructure, then they get that, then there's kind of checking other parts of your infrastructure and they're getting that. They're taking this and they're snaking their way through everything. They could only do what your infrastructure is capable of doing. If your infrastructure, if you've, for example, set up network lockdowns that prevent your web server from ever hitting your back-end databases, you know if that needs to go through a malware layer, then an attacker can't take over your back-end databases from the web server. If you're limiting your own functionality, they can't do what your architecture can't do, what your systems can't do. Now we're well aware in the kind of PowerShell community of just enough administration. That's another great example here. When you're having systems talking to other systems, don't do it over unconstrained protocols like straight raw WMI or anything else, do it over just enough administration endpoints where your system's connecting to other systems can only do the commands that they're supposed to do. Now an attacker takes over any of those systems and then they've got a restricted set of things that only that service was supposed to be able to do anyways.

Data Is a Liability

Now data is a liability. You might be thinking what does that mean? So there was a very recent breach, a very interesting breach from the Ashley Madison website. So they're an adultery website. They probably thought that their most precious resource, their most precious commodity was like the PayPal information that was inside their databases, credit card information, but realistically, people are used to those things getting breached and they get their notifications and they're like oh that sucks, I've got to change these things again. Their most precious commodity was trust. When you're doing a website that's all about flying under the radar, the minute you lose trust, you're done. The Ashley Madison site got breached. All of that sensitive data that really was not crucial to the Ashley Madison business then became a liability that leaked and within 6 months, they had lost 80% of their business. I checked again a couple days ago and it's gone down again, so their now down to more like 90% of their business. The 10% that remains, if you take a look at the web traffic, is from countries that don't speak English. They just weren't part of the news cycle. So data is clearly a liability. If you don't need the data, then you shouldn't store it, right. This comes true when you talk about storing customer data. If you don't actually need access to this data, encrypt it. Let the customers be the only people who have access to the data. That solves you from the dangers of an attacker coming and stealing a bunch of personal information that you don't actually need access to. A perfect example of this is just the way that's appropriate to do with passwords. When people create login systems for websites, well you don't store literally the usernames and passwords, you used to, but if that database gets breached, now you have everybody's usernames and passwords. So that the technique there is instead to store their username or whatever and then a hash of their passwords. Now the hash of their passwords that you prove that they knew the password without ever storing the password itself. So there's an example of treating data as a liability. Now success. Don't let success be your downfall. Some interesting things to ask yourself, right. So what if every host in my entire network had this bit of software? What if every person in the world was connected to my service and was subject to the same rules or whatever else. A good example would be think about some obscure Android distribution that has a picture taking application to share it up to Twitter and maybe they hit a bug that was including the GPS information of where the person was when they took the picture. That person might be like, that distribution might be like, well that kind of sucks, we'll send out Apache at one point. If this was Instagram on iPhone, and if all you had to do was go to Instagram and you could find out the GPS coordinates of everybody and everybody's picture, that would be a recall class vulnerability. The only difference there is success. Don't let success be your downfall. Design your systems in such a way. A great example is cryptography, right. So cryptography, everybody assumes that everybody knows the algorithm. If you're writing something that requires some secrets like ooh, if this DLL ever got into the public or this implementation ever got into the public, it'd be game over. You need to design systems that don't have that issue and that's kind of what drives things like cryptography. Now that's been a lot of like here don't do this, don't do this. People talk about the way to get around these things is threat modeling. So they kind of look like this. You've got dashed lines for trust boundaries and boxes for servers and that kind of stuff. What a threat model is not is a big effort that you spend in Visio. That's not the benefit of threat modeling. Threat modeling is talking through your architecture, talking through your design. A lot of times, this is what they look like. You sit in front of a whiteboard, but the important part is that you have structured discussion with peers about, well what happens if this thing gets popped, what happens if this thing gets popped, thinking through those is what helps you create a secure design. So the threat model template itself, what does it look like when you actually sit down and start doing threat modeling? This is the thing that everybody can do, it just takes directing your thoughts. So the first thing you start off with is just a diagram. This can be in Visio. It kind of feels like you've made an official corporate document once it's done in Visio, but it can be absolutely on a whiteboard. The important point is that you've identified the components, the hosts, the data stores, being able to talk to these things and talk about them one at a time. The one thing I would point out there is in this ideal threat model, the traditional approach is called a stride model. So for each one of these little arrows, you talk about spoofing, what happens if the person isn't who they say they are. Tampering, what happens if somebody changes his data in transit? Repudiation, so what happens if they say didn't do it. Information disclosure, which is what happens if this connection is sniffed on or whatever. D, denial of service. What would happen if somebody killed this connection or made it impossible to use? And elevation of privilege or what would happen if somebody stole credentials and managed to become not the user we thought they were. Wouldn't you start taking that systematic stride model over every one of these things? You just feel like you're caught in a hamster cage And there's a much more structured way. So we talked about take a diagram, talk through the components, what do they do? Now this is a thing that you if you're at all involved in the infrastructure, you're able to do this, right. So somebody points you at this web server here. I come up to you, I sit at your desk or your cube or whatever and I point at that web server, I go what does that do? You don't need to be a security expert to say well, that's our web server. It serves up pages for the campus website. People can enter their course descriptions and search for things. Like this doesn't need to be a terribly complicated engineering exercise. This is you talking to a peer, but just taking a security aspect to it. Now the data stores are very important. We talked about all those dangers of when people are compromising infrastructures, usually, they're after some potentially of gold at the end of the rainbow. A great example is a data store of when people a little while ago breached RSA, company RSA to make all those secure ids, it's likely that they worked. Their end goal wasn't to breach RSA itself. They wanted to use basically the token generation seeds to then breach Lockheed Martin and then steal real IP because Lockheed Martin was using these two-factor authentication codes. They wanted to breach Lockheed Martin so that they could get access to crazy fighter jet stuff. So the data stores. You take a look through your diagrams and you go okay this is a data store. What's in it? If there's some things as you're talking through your data stores and you say this is a combination of people's favorite colors and their dog's names and the day of the week, and also their social security numbers and credit card numbers, maybe you have a good opportunity there to start splitting your data stores where one, you realize that it's incredibly sensitive, you can protect that differently than you might protect something that's less sensitive. Now so you've got the components, you've got the data stores, then you starting talking through, how do they actually interact among each other? So this is what an attacker does, right. They say okay I got the web server, what do I have connections to? What things can I start to go off and so these connections, so they talk to each other. How do they do it? This is a thing that we can all answer. Well it uses PowerShell. Well what's the identity being used during that connection? Did I hardcode a credential somewhere? Is it like a service account? These are things where you can help decide, you know what it turns out that this connection is being done as an account that's trusted across the entire domain. So if somebody steals one of these credentials, now they're able to steal it and use it across my entire infrastructure. So that's a bad idea. You can start to see what happens when these things are breached. So there's identity, right. That's a very, very important thing. Windows has a lot of great options for proving identity that aren't necessarily a danger when those things are stolen. So a great example is group manage service accounts. Those are domain identities that you can register in AD that make the identity only work from a specific machine that you've declared as trusted. So that machine can go off and fetch the current identity of this group manage service account. It can be that domain user across your whole network. Somebody steals that credential. The minute that one machine is done with the credential, the password is just rotated. So somebody steals that credential. It's no longer useful across the rest of your infrastructure. So there's a great example of how you can compartmentalize identity. You start talking about protocols, so when I've got one node talking to another, if you're doing anything fancy, if you're doing a custom binary protocol blah, blah, blah, blah, blah, that's where you start having some things you've got to really be concerned about. If you've implemented a protocol parser on one side of the fence, well then an attacker might just start chucking bad data down that protocol. If you're doing something regular like I'm just using PowerShell or I'm just using HTTP over SSL and that SSL is like real SSL. I went off and got a good cert from a PKI. It's not like homegrown self-signed certificates. The danger with self-signed certificates, if some people will do this in their infrastructures as they'll set up some SSL communication with self-signed certificates and then they'll set all these flags on the one side that says just ignore SSL errors because I know this is self-signed. So it turns out, that's not a security boundary anymore. It's not a security prevention because an attacker doing this interception can absolutely just give it a new self-signed certificate and they have complete access to that entire communication. So if you're doing regular stuff like regular protocols, you don't need to spend much time here. But when you're talking through your design and if you start, people say oh yeah we've got this custom, that's when you know your questions should start rising up where you know you need to start dialing into that a lot more.

The Hackers Perspective

Now this is the fun part. You get to think about the system from the eyes of an attacker. So we've identified a bunch of hosts, a bunch of data stores, now you take a look at a host and you think okay so I've got the web front-end. What's the worst that could happen if this thing got compromised, like just let your spin for a while, half of threat modeling is brainstorming and creating new things. Now here are some good things to get you started, right. So people will sometimes use that host as a way to exfiltrate data. Maybe if sometimes that host is being used as a bridge between two things and they can embed some stuff that intercepts that bridging capability. Maybe this host is now interacting with some other custom infrastructure, like maybe this is a place where you're bridging two networks and they can use this as a jumping point into another network. So just kind of thinking creatively about if I was an attacker, what would I do with this host? Data. We talked about some of the data attacks from the Verizon Data Breach report. When you have data, people like to do things like hold it for ransom. You've got a database that's storing your backups in an infrastructure that you created, what happens if an attacker takes that over, encrypts everything, and then tells you I'll give you back the key after you give me a million dollars. You don't need to necessarily defend against everything, but thinking through the attacks, what would happen, and as you're going through these things, you're just writing them down, putting them on the whiteboard, bit better if you put them into a document to this thing. Now the thing here that's interesting about value of data is sometimes it can be used to leverage other things. So this is the situation I was talking about where people were taking the RSA seeds in the key seeds, then potentially able to use those for other attacks. Then when you get into there, you start talking about mitigations. So for all these attacks, what are some of the things that you can do to mitigate the attacks? Now the mitigations, you're not always going to have a very clean answer. Sometimes you go, you know what if somebody steals all this data like I'm literally losing my business. If this data goes away, that is an end of business event. That's not a situation you want to be in. This is a very quick thing. These threat models when you sit down and think through in this sort of structure a lot of times, you get the majority of the innovative, interesting ideas in an hour discussion. If you can't spend an hour on your security discussion, you're probably wasting your time on other things. This is a very, very important thing to do. Then you get to detection and response. So the mitigations. If you found a situation where if this breach happens, it's a business ending disaster. Even just acknowledging that this happened would be complete loss of trust or something like that. That's when you set it upon yourselves to change the design in such a way that you no longer have that vulnerability. So there's an interesting distinction between threat and vulnerability. So a threat is kind of a danger of a bad thing happening, and so there's a threat, for example, of somebody compromising your database and stealing all of the data and selling it on the black market. That's a threat. A mitigation would be well that's fine, it's all encrypted. The customers have all the keys. I don't even know what's in there. So there's your mitigation is encryption. So when you have a threat with the mitigation, it's no longer a vulnerability. If you have threats that you think are concerning and you don't have a mitigation, then now you have a vulnerability. Now sometimes, you're not going to be able to completely mitigate all your threats. You're going to have a situation where hey this bad thing, I'm really worried about maybe this server being compromised, maybe it's going to be abused to do some other attacks against this other thing. Now sometimes these things you can solve by just doing some detection. So for example, if one of your servers is making REST calls into another server and you know that it's always going to be the same two calls and you know, you can start to know which maybe accounts are being used or which storage account ids, something like that. If you can figure out a good pattern that these are going for, you don't necessarily need to write massive, massive remediations for these, you can just say that's fine. What we'll do is we're going to start setting up auditing and logging on the way that these communications are happening and we'll create a rule that detects anything that's outside of what we consider normal. That's a valid way to operationally get around some of these threats you might have. But some of the mitigations, they're not going to be there unless you're designing your infrastructure to help, especially when it comes to the detections. We're kind of accustomed to Windows how it's got such great event logging, you know you can see which processes are launching, what were the command-lines, what PowerShell was being used, what was the ScriptBlocks being used. That didn't come for free, right, this was everybody in the infrastructure that we're using said what additional detail would people like to know operationally about how this system is working. Now if we're the ones in charge of writing new software, creating new systems, doing the DevOps thing, we also want to make sure that our systems are equally as security transparent. So you should be able to set up rules that say when somebody is connecting over here to this system using this application, whoa hey let your application or your service start generating event logs and audit logs based on the things that you think are interesting in your application. When you use PowerShell or where they use the ETW events built into Windows, Windows makes it very, very easy to write structured logs into the event log. You can create your own event log channel, just do all this kind of cool, cool stuff. If you were doing custom logs like a text file somewhere, you want to make sure that thing is parsable by machines. You don't want to say hey Joe, when you're trying to detect a breach here, read through every line of this text file and hope you just happen to be awake and an attack happens. You want to use the machines to help you, make it machine parsable, and be very explicit about what that format is. If you need to, write your own parser so that these things now you can go off and write the splunk alerts or whatever you want to do for when some of these events are happening that you want to start detecting. Now for each of these components too it's important to think through, as hard as you try, there will probably be a system where, there'll probably be a situation where you're going to get breached. You did all this assume breach work, this defensive work, there'll be a situation where you get a call from a government agency and they say just thought you'd like to know we saw one of your web servers involved in attacking another customer. That's generally how people find out that they've been compromised. Now absolutely, if you don't have in-house incident response staff, you'll end up calling a company to help out. There's a bunch that do a great job. What you don't want to do is to say I don't know actually how you recover from this. They'll help do a great job of like scoping the breach and everything else, but they're not going to know your application internals. So if you know that one of your hosts have been compromised, they'll help you detect one of these hosts have been compromised. There'll come a point where you say, they'll say this thing needs to get remediated. What does that mean? How do you actually clean the breach off of that system? Sometimes it means, well if it's DFC, fine, I'll just nuke the machine, reapply the configuration, figure out how they got in, make sure that it wasn't from the software I put on, but sometimes it means, oh actually this system is storing a lot of live data, so what we're going to have to do is capture the data, take it off, maybe create an offline image, then bring it back on. You might have a more complicated remediation story. Sitting in front of the whiteboard and talking through these situations helps you be a lot more clear when the situation is actually happening, instead of when you're paying some consultants however many dollars an hour to figure it out on the fly with you. You don't want to sit there and go I don't know how to remediate this. There's an attacker here, our business is down, we don't know how to actually recover it, and then they'll help you talk about data stores. So proactively thinking through the remediation story is very, very, very useful.

Structured Thinking About Security

So the point here is that people say that threat modeling doesn't work and they really get stuck on that document-driven approach. It's structured thinking about security that works. Go back, put these things on a whiteboard. We'll be releasing this as a threat modeling template to help you think through the different aspects of your system. Think it through. Think through the different dangers of your components and your data stores and what you can do to help restrict these and prevent any issues in the future. Now what I wanted to do is talk a little bit about if people have questions about hey I'm designing, I've got this system, obviously make it generic or whatever, but I wanted to sort of talk here about what do people, what challenges are they running into? What are some opportunities for restricting a thing that you're nervous about? Any takers? Yes. (Audience question) Yeah, so the question was when you start going down this path and somebody goes well I'm like 14 layers in and if somebody abuses this API, bad stuff happens. Ultimately, you're going to be constrained in the implementation effort. And so, it's really important to be pragmatic about it. The way we like to focus it on what are your top five worries and what are your top five unmitigated vulnerabilities, and then you just start prioritizing. So you can say you know what, we've got like 10 issues that are way huger, let's not dig too much into this first, let's start looking for things that we kind of in our guts know are higher priority, higher impact. So that's kind of the way that I found it really helpful to just be pragmatic on the threat modeling, rather than diving down to like kind of API levels and that kind of stuff. Anything else? Yes. So the company that I was working for has had all there… Yeah, so the question was, when you have two components that are communicating over PowerShell remoting, should we be doing that over SSL? And that's a good example of somebody saying I've got two components interacting, what is a danger, what are the information disclosure risks and everything. On that perspective, we're actually working at releasing a PowerShell Remoting Whitepaper soon. It's SSL is almost always better, it's almost always better. When it comes to PowerShell remoting, we've taken enormous steps to ensure that PowerShell remoting, even without SSL, is a completely safe thing to do. That's because PowerShell remoting uses Kerberos, which does the authentication mechanism in such a way that you can't really snoop it or sniff or anything else. It's the same thing as talking to SMB shares or anything else. From then on, everything else in PowerShell remoting is encrypted by the protocol itself, and so SSL is important to enable when you're using a basic authentication, for example, connecting to a WinRM endpoint over HTTP, like something off in Azure, that's a good example where you're not within a domain environment. Yeah, that's what I thought. But they were under the impression that… Correct. You notice when you're about to do a bad thing with PowerShell remoting, you get so many error messages about allow unencrypted connections and all these kind of things. But that said, they had a valid concern that we didn't have it documented well enough, and so we're working on that to get that out there so that you can just give them a document for them to kind of geek out on it and understand the context. (Audience question) Yeah, the whole thing has got just tons of subtly. It's really hard to keep them all in mind, especially when you're talking to somebody who is completely involved in the risk mitigation department, and so it's a thing that we're going to work on making sure that there's just kind of a document that people can read through. We're going to put it up there on TechNet. We're going to continue to tweak it and make sure that these kind of concerns can be addressed so that we don't feel like we have to individually fight the good fight. Cool. Well, thanks for your time. Go out and think about your systems and make them secure.

DSC Secured Pull Servers

Introduction

So here's the thing. We're DSC is for configuration management. I've got two sessions during this week. This is on how to make a pull server for your infrastructure where you will put your configurations and your resources. The next session that I'm doing is how to package your configurations and your resources. These sessions have nothing to do with writing configurations in your resources. That was yesterday. Now do you want the good news or the bad news? Bad news. Oh, pessimist. Bad news is this, if you don't know anything about DSC, this is the deep end of the pool, sorry. Now do you want the good news? Yes. Good news is this, you want the code from yesterday? Yes. I'm going to give you the code from yesterday. Yay! Which includes the code that I'm using today and the code that I'm using later in the week and all of the code that we did yesterday, which could show you how to do DSC. So first of all, guys, when this thing decides to come back on, look 10 years of PowerShell, yeah! You're going to see some of these people walking around here and there's going to be Microsoft blue badges everywhere, so say hi and talk to people. Alright, look fast for when it blinks, what's going to come is there's the link to the code. It's GitHub at the powershell.org. There it is, get it quick. Use your camera. Also, you want Steven Murawski's reading list on DevOps. It's an excellent reading list. Okay, you don't look impressed. Alright, let's talk about pull servers. Here's what we want to do. We want to have a configuration that we want to be able to send out to other nodes that are going to automatically configure our boxes for us. We do not want to push this to them because that means I have to manage that. I don't want to manage anything. I want to sit at home in my underwear and drink coffee or whatever and I want it managing itself. So we want to bring up a server that we can put our configs on that all of our nodes that are using DSC can go to grab the config for them and automatically do it themselves. Are you with me so far? Yes. Good. Give me feedback just like that, so I know that you're alive and well. And don't worry about the flickering screen. We'll get through it all. So does that kind of make sense? I want to have this pull server. Now there's three kinds of pull servers that you're allowed to have. Three kinds. I only got three more slides, guys, and then we'll… So you can have an SMB pull server. Here's what an SMB pull server is. Hey, you know what? You guys ever build a file server? Yeah. Do you guys ever create a share on a file server? That's an SMB pull server. Oh, come on. That's all it is. It's a server with a share. So you use though the PowerShell command New-SMBShare to create the share and assign the permissions to it. You can configure the client LCM, which is the local configuration manager to go to that share for the configs, then delete it and do the right thing. Do not use an SMB pull server. You don't have all the features that you need. There's another type of pull server called an HTTP pull server. This is a web server. Yeah. It's an HTTP web server, which means all communications go across the wire like what? Plaintext. Plaintext. Not good. Here's why not good. You don't want somebody sniffing those configs and understanding how you're configuring your boxes. That's not good. So you can configure an HTTP pull server, but then delete it and do the right thing. Are you ready for the right thing? You only have one right option. HTTPS. There you go! Oh, man I love it when you play along. So what this means is, has anybody here ever set up a website with a certificate on it. Awesome! That's all we're going to do. Now, the cool part about this is, you have some help in making this work and we're going to go to code here in a minute and I'm going to show you how this sets up. It's not difficult and I'm giving you all the code, so there's no reason why you can't do this at home. Here's what I recommend and I recommend this all the time when we talk about DSC. I do not develop DSC by pushing it those configurations to machines. The reason is that I need to see the whole process. If you come to my next session on this, it gets complicated fast, and so I always, even in a lab environment, set up a pull server, an HTTPS pull server because it's free, and in my lab environment, I go through the whole process of development, so it's always correct. So I'm going to give you how to build this pull server. It's really easy.

Configuring an HTTPS Pull Server

Now when you guys, don't worry, when you guys get the code for this, I'll wait for it, it's going to look like this. You're going to download this thing called DSC, boy that's really irritating, sorry guys, DSCPreCon and you're going to have all of these sections under there of code. The section that we're starting at is section seven. So you got six earlier sections of wonderful code just for you. Okay, don't be so happy about it. Section seven is the pull server, so I'm going to go in there and I'll show it to you. And here's how I construct everything. You have a demo file. This demo file walks you through the entire process. Ready? Say yes. Yes. Wow, you guys are tough. I'm going to set my directory here. Now here's what I did. I wrote the entire process down for you in a short list so that you can see what the whole process is and then I demo and create the entire thing for you. So here's what I want you to know. Don't panic about this thing flashing. Just get the concept that I'm trying to convey to you because you have the code and I've written it all out for you. Are we good so far? Good so far. So relax. I'm relaxing. Okay, good. I like that. So one of the first things you're going to need is you are going to need a couple of things right out the bat to make a secured pull server. You're going to need a certificate. Now this pull server is an internal pull server. You can use your own internal PKI to put an SSL cert on it. I never recommend wildcard certs, but if you must, you must. So you can put an SSL cert on the box that's going to be the pull server. In my case, I'm going to use a box called DC, which happens to also be a domain controller, don't do that in real life, but you can do it in lab, that we're going to turn into a pull server. This gets a little bit complicated because it's not you making a website and putting a certificate on it. We need some special services in there. Microsoft has bundled this for us to make life easier. So the first thing I'm going to do is I'm going to create a DNS record for the pull server. This DNS record is what I'm going to teach all my nodes how to use, how to get to, to find their configs. I'm going to forget what this is, but it's DSC.Company.Pri, so I'll just add the DNS record. And here's what I want to show you as soon as he's done with that. We need a DSC resource module off of the gallery. The DSC resource module that we need to get is called, and don't write this down because I have it written here, it's called XPSDesiredStateConfiguration. This module gives us a couple of capabilities and some documentation that I want to show you. Now do you want me to give you the truth or the story? Truth. Aww. Because see the story is a nice, easy polite thing and the truth is painful. Okay, truth it is. Blame him. Blame him. So look, let me show you how to find this module. You can find this module, and I've written it out for you, of course. I'll show it to you. Oh, yes please. NuGet, NuGet, NuGet. Come on, Come on. Snap to it, snap to it, baby. At least I think we're going to find the module. There it goes. That's awesome. No, that's not awesome. There it goes. There. So here's the module XPSDesiredState. I want you to notice the version numbers 3.9. Friday of last week, it was 3.8. Wednesday of last week, it was 3.7. I'll get back to that in a moment. But so they're updating it. But you need this on the pull server and the box you're writing the config to make the pull server, so your author box and your target. So here's what I'm going to do. I'm going to get rid of this right now. No, I can't get rid of that. Oh, this is going to cost me... Oh, that's okay, I'll fix it. So here's where, how am I going to do this, I'm going to use install module to install XPSDesiredState. I need another resource module because the way I like to write my configs, install it on my author box, and then I need to install it on the box that's going to become the pull server. The resources must exist there. So you can see the next command I'm going to run is Invoke-Command on DSC to do that exact same thing. Are you with me so far? Yeah. Yeah, okay. So we'll do that. In here, I want to show you something. One of the best things about DSC resources is Microsoft gives you some documentation to help you learn how to use the resource. So what I wanted to show you was right here Where that module is getting installed, out here, and I have to change this to 3.9, out here underneath examples, there's a sample of how to set up a pull server. This is really nice of them, right? Don't you think that's nice they give you a sample? Okay, you don't seem all excited. Let me show you a sample. We're not going to do it this way, but. So they give you a sample. This is a configuration of how to set up a pull server. And here are a couple of things that we need. First of all, we need to have a special Windows feature called DSC service that has to be installed first. When that's installed, now we can start to set up our pull server and I want to show you some of the configuration settings here. These are things that we have to tell the pull server how to configure its website. I want you to do me a favor and look at, the endpoint name is going to be the website name, what I want you to do is look at port, Port 8080. This is the port that the websites going to run on. Now I want a secured website, so what port should this be? Four four three. Four four three. Does it have to be 443? No. See SSL works on any port. Why do we usually set it for 443 though? (Audience Answer) Yeah because users don't know how to type in colon for whatever and there's no way to communicate, so we make it easy because the browser makes it easiest for us. Here's what I don't like to do. I don't like to set pull servers up on 443 and there's a reason for this. I can teach the LCMs, the nodes, how to get to the pull server by giving them the port number, but I might on this pull server want to set up something else on this pull server like a self-service portal or PowerShell web access, things that I need users to access that are on 443. And here's why and some people will go, do I have any Exchange guys in here? When you set up a KAS box, are you allowed to put any other websites on that KAS box? No, sir. No sir. Don't call me sir. No! I want to tell you something. I want you to think about this. Configurations that you put on this pull server are little tiny text files. Even if you have a 1,000 or 6,000 configurations, they're little tiny text files, do you know what a web server dishes out really well? Little tiny text files. Little tiny text files! You know how well it dishes it out? That millions of people can get little tiny text files from a web server. So I want you to, I want you to, this web server you're building is never going to be doing any real work because your nodes are going to ask for a text file and it's going to go… One of these can handle everything you've got. Everything. So I want you to just kind of think about that for a second because he's not working hard, so I like to put other things, management things, on him to use up that resource a little bit more. Does that make sense? Yeah. So if you take a look up here, I'm going to leave it at port 8080. Here's how we're telling it, though, to be a secured website. We need to give it this information. The Certificate ThumbPrint of the certificate and I'm going to show you how I get that to fill that in. Now I want to point out a couple of other things. ModulePath and ConfigurationPath. This is the physical location on the pull server where you will put your configs and your DSC resource modules. You can change this. I do not recommend it. In other words, why are you changing it? If you don't have a really good reason, don't do it. I also want to point something else out here. I skipped over the physical path to where the website files reside and when this comes back up, it's missing something. This is wrong. The documentation is wrong. And I don't know if you'll stay lit long enough, but the documentation is wrong. Does anybody find that funny? No. Oh, I find it funny. I find it typical. Oh, typical. It says iNetPub, but it should say iNetPubwww is the location where the physical files are. Don't worry about it. I'm going to give you a config that's correct. The state is going to be started, other than that, let me show you my config, and we're going to run it, and make a pull server.

Deploying the Pull Server

Let me show you my config, and we're going to run it, and make a pull server. Does that sound like a plan? Yes. Alright, I'm not going to show you the simple one, I'm going to give you the advanced one, why, because I think you're advanced. You get both, but it's the advanced one. I'm a web guy, so I'm real anal about web servers and web servers are under constant attack. So here's what I do is I want to configure my pull server, which is a web server, with the most minimal components possible. I also want something that nobody else seems to like to set up. My boxes are all core. There's occasionally when a web server needs to be managed from the iNet management tools like a developer or something like that. You have to turn on and enable remote management on web servers. On core, you can't do it because you can only do it through the graphical. I need you to think that through. You can't turn on remote management because it requires the graphical on the box to turn it on. Well why can't I turn it on remotely with the graphical? Because it's not turned on. Well how am I going to turn it on? I wrote you the PowerShell code in DSC format that does it. I figured it out. So I'm going to show you how to turn on remote management. Nobody's pull server configs has this in it, so here's what happens. You can't manage the damn thing after you get it set up. Isn't that stupid? So do you want me to give you the good code or the bad code? Good code. Okay, I like that. I'm up for good code. So here's what we do. This is called the config data. I separate my configuration data from my actual configuration. So what I'm allowed to do here is I can put in all of those things as properties right here at the top if I want to change them. If I want to have multiple pull servers, which we'll talk about in a minute, I can just have multiple hash tables here with different computer names in it. Now what I'm setting up is roles. I've got a web role and then a pull server role. So here's all my configuration data. Take a look at this, this is how I get the certificate thumbprint. I Invoke-Command to the box that has the certificate, Get-Childitem to the store, I find the certificate with a Where clause if it stays lit, and then I grab the thumbprint. This is a great piece of code, man. You want this code. This is a great piece of code. So down here, here's how this works. This is the actual configuration itself and I want you to take a look at the line, I import the DSC resources that I need, but I want you to look at the line AllNodes.where Role -eq Web. That's how you separate your configs for the different roles that you want. This makes it real convenient because once I've written this correctly, I never want to touch this code here again. The only thing I ever want to touch are the values in the config data at the top. So I made this much better for everybody. And here's the other thing I did, and I'm not going to make you try to watch the blinky thing. I'll tell you about it and you'll see it here. When you install a web server, you install a Windows feature web-server. It installs a limited set, a matter of fact, just enough stuff so that you can have an HTML Page. No code will run all that kind of thing. Microsoft thought that would be a brilliant idea and they're absolutely correct. So what I do is I do that, but you still need some components for the pull server, so I install those, but here's the thing, that basic set that it installed, there are things in there I don't want on my pull server and I want to reduce the attack surface as much as possible. So not only do I have Windows feature resources that are adding in individual components, I'm also removing the stuff I don't want that the default installed. This gives me the tightest web server with the least amount of tax surface possible. They don't do this if you use their sample documentation. They just go blah and it just barfs out. I'm anal about this because I know it will be a and also this web role is what I use for all my web servers. It's always the most minimum cut down for them and just the application components like ASP.NET 4, 5, that it needs to run the applications. So what I here as this thing is blinking on and off is a list of each one of those. So it's a detailed list, it'll come back, don't worry, you're not missing anything. So pretty long list, but really cool. Let me get to the cool part. GUI remote management, as soon as it comes back up. See the light? Oh, no, of course you don't because it's not up long enough. I just give you… So what I did is I figured out, I did this actually a long time ago through just regular PowerShell scripting. I figured out how to enable and configure the remote management piece on IIS without being on the box. You can do it in a script, but I put it in DSC format. So you have to install the window, the web management service. The problem with this is that once you install the service, you still have to enable remote management and then configure it. Those are done through the registry keys. I figured out the registry keys. Now the registry keys don't exist until you install the management service, but then they exist, so you have to make sure you get them in the right sequence. Please flip back on. You can look at the code when you download it. There's the remote management stuff when it flips back on and the registry keys for it to enable it, get it set up. The other thing is with remote management on IIS, there's a service that has to be started called the WMSVC service. There's a problem. I love this problem. This problem makes me so happy. The service is not started. You guys know how to start a service. You can remotely do a Start-ServiceWMSVC. But here's the problem. The service is set to manual, so when the box reboots, guess what isn't working. The remote management stuff doesn't work and you can't get to it again. I love that! So what I did is I used a service resource to set it to run and configure it for automatic. Yeah. So that way it will always be there. Okay, cool. Well let's run this. Oh, let me go just show you down here. Here's the pull server and what I did is I replaced all those properties I did in the config data as variables down here, so I never have to touch this code again. This code is good code. It puts things in the right place, unlike the sample that's out there. So you should applaud that I gave you good code instead of bad code because Richard Siddoway gives you bad code. No, I'm just kidding. Okay, so let me run this and what'll happen here is it'll create a MOF and I'm going to push this to the pull server. Well why can't you pull it to the pull server? Because I don't have a pull server yet. And I'll show you, I'll let you see this as it runs. I'm going to, I'm also going to change the LCM on the pull server. You don't actually need to see this. This is just because I'm weird. (Audience comment) Yeah, shut up. But here, let me… I'm going to kick off the configuration, so I'm going to hit this and it's going to push it out, it's going to run the config. It's only going to take it a couple minutes, but what's going to happen here is I want you to notice what I'm doing right now. What am I doing right now? Talking to us, jazz hands. Jazz hands! That's awesome! Nothing. In other words, I'm a big fan of I don't like to work hard, so I'm going to let this thing do it. Now here's the best thing about it. If I need a second pull server, what am I going to do? (Audience Answer) Yeah. I may not even really, other than the name, I may not even need to change anything else, just push it out. If I need 50 pull servers, which you would never need, I could just fire it to 50 machines all at once if I wanted to. This is a beautiful thing, man. It's beautiful! It's really, except you can't see it, but it's getting to be beautiful. And once this pull server comes up, he'll be in full operation. Now all you need to do is start putting your configs and resources up and that's the hard part and that's my session that I hope the screen stays on the entire time on I think Wednesday.

High Availability and Load Balancers

But I want to talk about high availability. This pull server, as you'll see he'll come up, he'll be a working pull server. It'll be nice. Is this machine critical? Yes. This pull server is going to hold all of the configurations for all your nodes. Is it mission critical? It might not be because… Well that's a way to split in the middle and not have a viewpoint. If the pull server's not available, it won't get updates to the config, but it's already got a cache puppy if the _____ node. Here's the thing. You're absolutely correct. All the nodes, when they get a config, they copy that config to themselves. They run that configuration. The only time they actually pull anything down from the pull server again is if you made a change up here. So I want you to think about it. If this pull server dies, everybody is still doing what they're supposed to do, you just can't get any changes out. So is this mission critical? Absolutely not. So how many pull servers do you actually need? Yeah, thanks for putting up the right finger on that one. I appreciate it. Now here's the thing. In production, not lab, in production, I'm a high availability freak, so I'm going to set up another pull server and make it highly available and let me tell you why. It's not for performance. It's not because it's mission critical. It's because of two things. I'm lazy and it's cheap. So since it's cheap, if one pull server dies, I don't want to have to care at that moment. I might be out drinking, I might be having fun, and I don't want to care about it. It's so cheap, it's free. Yeah. Anybody here ever set up a load balancer? Yes. Load balancer website. That's all you're doing. So what you do is you bring up another pull server, you put a load balancer. Now what kind of load balancers can you use? Well first of all, how many of you have heard of Microsoft Network Load Balancing? How many of you like to use NLB? Well that's a sorry site. NLB doesn't suck? NLB is actually a great load balancer, but it's a layer three load balancer, which means the entire nick has to die before it fails over. When I come to the websites, I want, if the app pull dies, I want it to be able to failover, so I need a layer seven load balancer. I need a big IPF5. Yes. Yes! So I'm going to spend a 100 grand on two of those because you want to load balance the load balancer for a cheap, dirty pull server. That kind of sucks, doesn't it? Now look, if you've got a spare hardware load balancer lying around that you're not using, go ahead, and use it. There's nothing wrong with that, but I'm thinking it's kind of overkill. This isn't a commerce site for my customers. This is my internal pull server. So I'm going to give you a free piece of software. How much did I say it cost? Free. That works as well as a big IPF5 and here's the deal, I use this in production on commercial websites. I see confused looks. Okay, here's what I need you to do. Stand up. How much time do I have? Stand up! I don't have a lot of time. Come on! Come on! You sit all damn day! Get up! Richard Siddoway is, of course, he's too much of an English gentleman to participate in all of this. I want you to do me a favor. I want you to put your foot upon your chair. I want you to go like this. Put your hand on over one eye, oh come on, put your hand over one eye, and go arr. Arr. Arr. Arr. Alright fine. Now sit back down. Now you'll never forget it. ARR. A-R-R, Application Request Router. ARR is one of the greatest load balancing products in the history of the world and you don't even know that it exists and I'll tell you why. You're going to yell at me too when I tell you why it exists and who it exists from. It Microsoft made it. Was it the IIS team that made it? Yes. It was the IIS team that made that. I'm very, yeah, we're not going to talk about that, okay. We're not going to go through that whole scenario because I'll say the wrong thing again. You know what's going to happen. There'll be tweets and it will be bad. Application Request Router. Here's how you get it. You go into the iNet Config Manager, you go out to the web platform installer, you search for ARR, you install it, you get a new icon or a new folder in your web management tools and you drop servers in there, and it'll load balance them just like that. You can change the algorithms, all that if you want to play with that stuff or you can just drop servers in there and it'll automatically load balance them and it's faster than gas and it works perfectly. I can't believe it was written by Microsoft. No, that's a joke! That's a joke! I was just lightening up. So a free load balancer that doesn't cost you money, that's the one to go through. So to make this highly available, bring up two pull servers, easy to do with this config, and then give it a shot. I'm going to show you how to test the pull server, what it looks like. Now I might need to reboot this because I was removing software in my config. Oh, it worked. Okay, fine. Yes. When you install ARR, ARR, would you install it on one of the pull servers or would you keep that on a… Great question. No, great question. I'm glad, I should've, so let me answer your question. This is a great question. Where do you put ARR? You treat it like you need a box that you're going to treat like a load balancer, so what you do is you have a separate Windows VM or whatever with IIS on it, that's where you install ARR. That becomes your load balancer. The pull servers are behind that. Those are the things you put into the little folder, their names, and your DNS address points to this guy. Now some people will get ridiculous like me and go well I need to load balance the load balancer. You put in a second ARR box and then use NLB between them is the best way to go. Then it's free, it's awesome, and it works. Yes, sir. (Audience Question) Yeah, you'll change the IP address to the IP address of the ARR box, you got it.

Testing the Pull Server

Now guys I want to show you this because I'm just proud of this. This is what it looks like when it works. I know it looks like it doesn't work, but that's what it looks like when it works. And if you come to the next session, you'll actually see it work as we do configurations on it and all that kind of stuff, but I just want to show you just the remote management because I'm so happy that, come on light up long enough on the screen, wait a minute it'll light up, you'll see it, hold on. Connect. Hey, I got management. Oh, come on I got remote management to the web box! And your pull server, that's that endpoint website pull server on port 8080. Now you will teach the LCMs or the nodes how to get there and I will show you that in the next session. Yes, sir. (Audience question) It has an AD binding. If you have a, you should be building this on your own new box, so don't build it on somebody that's already messed with the ports. What I do is I want to tell you this. In the configuration, what we usually do with web servers is we turn off the default site because we're going to make our own stuff. People will say to me why don't you just delete the default website. That's not good. Sometimes we like to bring that guy back, so I just turn him off. I do not turn off the default website if it's a pull server or if it's an ADSC server, if it's a certificate server. Because if it's a certificate server, now you can't get any, and your pull server, there might be reasons for us to use that default website. Now I need to give you some good news, oh no, I can give you that news in the next session, that's when it applies. So let me check the time. Awesome. I know it didn't look like much, but you get all the code to make your own pull server, so you can start your own infrastructure. Does that look cool? Yes. Okay, come to the other session I'm doing because now we'll go through how to put all your configs on there and you'll see it all run because there's some tricky parts to getting all the configs and the resources onto the pull server. Once you have that done, you now have the infrastructure you need for DSC, both for lab and for scalable stuff out there. Now I do want to point a couple of things out. Right now, if you especially come to the next session or if you were in the session yesterday, right now there are a few things that we are, that the team is working on with the pull servers. You notice how I said we're at diversion XPSDesiredState 3.9 and we were at 3.8, that's because they're working on some issues that we might have in situations. So if you come to the next session, we have an issue with configuration names that I'll show you a work-a-round for until that gets fixed. I want you to keep something in mind. When you go back to your office, you should try this and build this. Use my code, whatever, make your own code for this, but follow along what's going on with these version updates on XPSDesiredState because you might run into something that you don't need to fight because it got fixed. If you come to my next session, you'll see exactly what that is and how it's getting fixed and we'll just move on from there. So just want you to know, this is work in progress, but I use this in customers, in their infrastructure. It is a stable, usable pull server. It's a great product. Now if this pull server does not provide you, and I say this about anything in DSC, if you're working with DSC, we assume you're going to start building your own tooling for a while because this is a platform, it's not a whole product, but I have a tendency to say this, if you came to yesterday's and you'll see him walking around here, Steven Marowski, he's a chef guy. He's the chef guy, right, the chef guy wearing an apron and a chef hat. Other options are like Chef and Puppet and Ancible, stuff like that. I have a tendency to go to Chef. When I work with customers that cannot build their own tooling today, I'll direct them towards Chef. If I'm working with customers that already have developers and IT guys that are PowerShell capable like you guys are, that can build their own tooling, building the tooling you need is easy and the types of tools. Well I want to inventory everybody to see what configs they're running and stuff that into a database. That's easy to write! In the future, I can't tell you this for sure because I don't know for sure, but I would assume that probably Microsoft at some point is going to have some more tooling around this, but right now, you can easily write anything that you need. Now if you were here yesterday, you saw a lot of that. You saw somebody writing a lot of tooling and some of the stuff I gave you I've done some additional tooling about it, but this is a really good product to start with and what we see a lot of people doing, even if they think they're going down the Chef path, they're doing the DSC path alongside it. A matter of fact, I'm working with somebody right now that's doing both. How many hypervisors do you guys use? Oh, so you only use Hyper-V, that's good, good answer. (Audience comment) Yeah, use one. So a lot of companies, you have VM, you have Hyper-V, so some companies are doing multiple configuration management systems and that's not always a great idea, but if you haven't gotten into DSC, this is the thing to get into. This product rocks. And configuration management is the most important thing and on Wednesday, I think it is, Michael Green is going to go through all the DevOp stuff. You want to see that. Marowski's going to talk about DevOp stuff. You want to see that.

Q&A

So ask whatever questions you want to ask in the few minutes that we have remaining before you get fed. Well, in there just when you all were setting up the modules on the pull server, the pull server actually went out and got the modules off the internet themselves. Well in my company, IIS servers, unless their customer facing, cannot be web connected. Yep, absolutely. So what you do is you take your workstation, you bring the modules down, and then you just copy them. So I can just pick a folder off of my workstation and drop them in. I work with a lot of government agencies that are in the same way, right. They can't have their machines can't directly access the internet. So all you need to do is bring down your copy and you copy it out the program files modules folder on that pull server. Now I don't want you guys thinking you're going to be deploying resources all the time. The whole point to this pull server is going forward with all your nodes, you're going to put all of your resources, custom resources, all that onto that pull server, those nodes will then pull them down from the pull server, so the nodes don't need internet access. It's just this initial set you'll need to go out and grab, right, and then put them up there yourself. You can just manually copy them. I've got a script that does that for a customer, just updates versions and boom, boom, boom, so because they have that situation. What else? Oh, come on. Do you want to do Desired State stuff? Do you want to do configuration in Azure? Did you gloss over the certificate that uses certificates on the box? Yeah, in this situation, the certificates on the box; however, I did give you code to do the certificate request from your PKI if you wanted to. If you have ADCS set up and you have the policy stuff set up on your ADCS, I gave you the code, it's commented out, but you can actually just run code to go request the certificate automatically and put it on the box. A lot of people manage their certificates, though, a different way, so I don't usually automate that for a customer because they have some other piece in place. Their security team does it a different way. The idea is to get a certificate on the box and then we can pull its thumbprint and go. In your other talk, don't you have the ability to encrypt the configurations themselves? Are you going to cover that in here? We actually did that yesterday and it was about credentials. I missed it. You missed it; however, you got the code for it if you download it. And if you want to ask me questions about it offline, I'm happy to talk to you about it. The code takes you through, don't do this because here's your result, plaintext password, don't do this like they say on the internet because here's your plaintext password. Then it says here's how you encrypt it. You're never going to find this on the internet because they changed how to do it. They added in a document encryption type, you have to make a new certificate, you have to add a new EKU for the document encryption type. If you don't do exactly what I have in there, it never going to work. But I wrote it out all for you, so and I'm happy to talk to you about it and take you through it, that kind of thing. It's changed in WMF5 than what we had in WMF4, so if you see older stuff, I did a lot of older stuff on credentials and all that kind of stuff. It just doesn't work the same way. The process is the same, but you've got to make a change to the certificate. Yeah, but yeah, we do all the credential stuff. Alright guys. Well, you know, I'll hang here and answer questions, but if you want to go potty, pee, or lunch or something like that, please feel free, but I'll stand here for 15 minutes and chit-chat. So you're officially done. Go. Thank you.

Getting Started with Classes in PowerShell v5

Introduction to Object Oriented Programming

Alright. Today, we're going to be talking about getting started with classes in PowerShell v5. So my name's Adam Driscoll. I was a PowerShell VPM, a cloud and data center MVP. I work for a consulting firm out of the Milwaukee, Wisconsin area. I do a lot of system center and Azure kind of stuff. I also worked on the PowerShell tools for Visual Studio, so you might have seen that around, so started that project. Thank you very much. So you'll be seeing that in the next couple versions of Visual Studio, I hope. And yeah, so I'm a senior application developer at Concurrency. So getting started with PowerShell classes. I kind of want to talk a little bit about object-oriented design just because to understand exactly what classes are, you kind of need to understand a little bit about object orientation. So the idea with object orientation is that it's a programming paradigm based on the fact that everything is an object, so you don't just have functions and scripts and variables, you have these objects that have particular properties or fields related to them, so that kind of maintains the state of those objects. You also have functions or procedures often called methods in the .NET world and that is actually processes that take place on that object. That object might be able to interact with other objects, it may change the state of that particular object, but everything is composed of an object. So when you start thinking about classes, you can kind of think of a class as a blueprint. So this is a blueprint for a G.I. Joe Crossfire. So you can look at the blueprint. It's like calling different things out in different states of this object. It has four wheels. It can be on and off. It can be turning left or right. It has a particular speed. Those would be the different properties of this particular G.I. Joe car. The other thing that it does is it has different functions that you can apply to it, so you can turn it left, you can turn it right, you can go forward and back, probably can shoot the gun or something like that. Those are the different methods for this G.I. Joe car. When your actually create an instance of that G.I. Joe car, that is an object. So it's based on that particular blueprint, but you can create many objects based on that blueprint. So when we define classes, that's what we're defining. We're defining that blueprint for the G.I. Joe car and then when we actually create them with cmdlets like New-Object, we're actually creating instances of that G.I. Joe car. So there's a couple benefits of OOP that are kind of important to note when kind of getting into object-oriented programming. The first is encapsulation, and inheritance, and then polymorphism. So they're kind of some of the basic terms that you'll hear when researching OOP. So the first is encapsulation. So this looks like a mess and you probably have no idea what it's doing, it's hard to figure out. You'd have to know exactly what all those wires were doing to be able to do anything with this particular circuit here. The same goes for scripts, so you know, I've written scripts where everything's a global variable and you're just hacking out functions and it gets really messy, really quickly and you hand it off to your coworker, they can't figure it out because they have to know exactly about everything about that script to be able to work with it the same and that's why we have this concept of encapsulation in classes. So the idea being is you have something like this where on the left hand side is the inside of that Wii remote. There are all kinds of circuits, there's different resistors and capacitors, there's the gyro for like moving your hand around and stuff, but you don't really need to know how that works to be able to use a Wii remote. You just flick your wrist, press a button, and that's because all that logic is encapsulated inside of that remote and then we have a simple interface on the outside of it. The same goes for classes. You have simple methods, simple states, but all the logic is kind of hidden within that particular object. The other benefit that we get is inheritance. So not quite as the same as inheriting money, but the idea with inheritance is that you have base classes and base classes have particular properties that you can inherit from other classes. So on the top here we have a wagon. The wagon has wheels, you can steer it, that kind of thing, but what you can do is you can think of other four-wheel vehicles. There are more specific versions of a wagon. They introduce new functionality. They behave a little bit differently. They might look different. So on the left of a car, it's got an engine, it still has four wheels, it can still steer, but it's a more specific version of a wagon. On the right-hand side, you have a four-wheel bike. You pedal it, you can still steer, and from there, you can actually base class it down to even more specific versions, flying cars or Teslas, a bike with a little roof on top of it, so you don't get wet in the rain. But it's the same core concept and the same properties that are available in the base class are also available in all the classes below it. So I want to show you some examples of how we can create base classes and how we can inherit that functionality, so we don't have to reimplement the concept of a wheel in all our classes. Finally, there's polymorphism. So polymorphism is kind of the idea that you can extract away some of the implementation details based on those base classes. So a good example is an example of remotes. So all these remotes do different things. All these, I think the one on the left is maybe a TV remote, middle is a receiver, and the right one is a Direct TV remote. They all have different buttons, they look really complicated, but they all do a similar thing. They all come from a base class remote that can turn on a device, they all have a power switch. So the idea being is you can hand it to someone like grandma and say, I just want you to know about the power script or the power switch. You are a remote that can turn things on and off and that's all you need to know. So we can do something similar with classes where we'll be able to pass base class implementations to other classes and they only need to know about the very basics of the class and not all the other details that come along with it. And I'll get into some examples. And I'm going to run this mostly as a demo, so I encourage you guys to ask questions and I'm just going to kind of go with the flow. But why do we want classes in PowerShell? For a long time, .NET developers, in general, who have gone from like a C# background and come into PowerShell and been like we want classes. And we want classes because we want to integrate with .NET better. We want classes because we want to write something that's familiar to us rather than just scripts and modules and that kind of thing. But the real reason that classes eventually got implemented was for the implementation of DSC resources. So if you've ever implemented a DSC resource, script-based DSC resource, it takes quite a few steps and you have to actually use the MOF file format to author your class and it makes you get out of PowerShell, you have to do two different things to import it and everything, so it takes a couple steps and it's not really PowerShell friendly. So that's how we got classes in PowerShell was to implement DSC resources and I'll show you guys some examples of that. And then finally, we can take advantage of some of those object-oriented techniques I just talked about actually extend from classes in the .NET framework, implement interfaces so that we can call particular we're not be able to call previously in PowerShell. So we are kind of entering a different realm in terms of PowerShell authoring.

Basics of PowerShell Classes

Alright, so let's switch over to a demo. So if June was here, she'd yell at me because she told me never to actually type in a demo, so I'm going to do this entire demo typing. Alright, so the first thing that we're going to do is we're going to use a new class keyword. New class keyword is really straightforward. It looks just like you're defining a function, or a workflow, or whatever, a configuration. And then the name of your type is what follows, so class Bike. I have a bike. So now what you can see is as simple as that, I have a type defined. No reason to call Add type, write C# code, specify all kinds of weird string escape characters and stuff like that. We have a type defined just like a DateTime object. It looks very similar. Except it won't let me cast that. Right, so it's the same concept. It's a type available and it's actually a .NET type, so it actually compiles down to actual .NET code. So now, we want to add some properties to this bike. So what does a bike have? A bike has maybe a manufacturer, a bike has a model, and a bike has a year. So we can see here is I've defined different particular properties of this bike. On the left hand side here you'll see that this is actually a type qualifier. It specifies what type this variable or property of this object is and if you omit that, which this is optional, it becomes of type object and everything in .NET inherits from object because everything is an object. So then you could assign any particular value to that particular property. So now if I wanted to create a new instance of my bike, what I could use is New-Object just like any other .NET type, once my IntelliSense stops, you can see I can create a new instance of that bike and it actually lists out the properties. I have a manufacturer, a model, and a year. Not very interesting because none of them are set, but what we could do is you could actually specify default values. So if I want to say my default value for a year is 2016. So now every time I create a bike, it defaults to 2016, but it doesn't make much sense to have a bike without a manufacturer or a model. So let's actually create a constructor. A constructor is the method that is called when you actually create an instance of a class, so Manufacturer. So by default, there's always a default constructor created where it is just an empty method with no parameters that does nothing. In this case, this is a non-default constructor and it requires that you pass in two parameters. So now what we're going to do is we're actually going to want to set the manufacturer and model on this instance of the object and that's where you use this keyword. This keyword is referring to the current instance of this class, so in this case, I would say this.Manufacturer and then I'd take the value that was passed in for the constructor. So now if I go to actually create my bike, what you'll see is you'll see an error because it says that the constructor was not found. Since we overrode the default constructor, we can no longer create this object without specifying those two parameters. So if we ever use a New-Object to actually create objects, what you can do is you can say -ArgumentList and then pass in two arguments. So now I when I create my bike, it has a manufacturer, it has a model, and it has a year, so that's really cool. While we're on the topic of creating new objects, let's actually check out one of the new operators that's available in PowerShell. So if you haven't seen this syntax before, it's actually a new syntax that was added in PowerShell v5 to actually create objects, so instead of having to use the New-Object cmdlet, you can use this shortened syntax where you say the type name followed by two colons and then new and pass in the arguments that you're going to call the constructor. So I'll just kind of shortens the syntax is another way to create, but does the exact same thing that a New-Object does. Alright, so let's make this bike do something. So one thing that bikes is that they pedal. So let's create a Pedal method. So pedal or methods look a lot like functions. They have the function name Pedal and then two parentheses or you could put arguments and then a method body. You can see I put void in front of there. Void is the return type of this particular method, so with methods, you always need to specify a return type, otherwise it doesn't know what it is. So in this case, we say method void. We're not going to return anything from Pedal, but we want it to do something. So okay. We want it to increase the speed. Every time you pedal it, let's increase the speed. So I create a Speed property and then let's set the speed, whoops, not ComSpec, whoops, I have to say this. I always have to say this. Alright, so now, I increase the speed every time I call pedal. So let's create a new bike, start a new variable, and we can call pedal on it now, and we can dump. Alright, so now you can see that when I pedal my bike, the speed goes up. There's one problem with our speed variable though. I can just set speed to whatever I want. Now my speed is 100, I don't have to pedal anything. I wish my bike at home worked that way, right? But to alleviate that problem, what we want to do is this is where the concept encapsulation comes into play. We want to hide this speed and variable. So now when I try to run this, you can see that speed is no longer available because it's hidden. And it no longer lets me set the speed because it's hidden and only that particular instance of that object can modify the speed and that allows us to control the internal state of this object and not allow people to mess with our speed without calling pedal. But let's say I wanted to get the speed. I do still want to know how fast I'm going, so let's put a little speedometer on there. So we can say int GetSpeed return this.Speed. Alright, so this is a definition of a method that has a return value. You can see instead of void, I'm doing int because I'm going to be returning an integer and I'm using the return statement to actually return that internal variable. So although we can't set speed from outside the object, we can actually return the actual value of speed via this function here now. So with this pedal there, you can see now it returned that my speed is 1 because I pedaled once with that bike. So a couple interesting notes about methods and partial classes, you need the return statement. If I get rid of this return statement, you can see it's going to give me an error that it says not all code paths return a value within this method. It's required to return a value if you are specifying a return type like this. And when it says not all code paths, you have to make sure that if you have if L statements and maybe return something in the if statement, but not in the L statement, it will still give you that error because you are required to return something from this method. The other thing to note is that the return statement itself is required. If you were actually writing a function in PowerShell, you wouldn't have to use a return statement to get something to return from that method, which is another really interesting difference between PowerShell methods and PowerShell functions. So if I had a function that was called GetSpeed and I did the same thing where I just said speed, which would be like 1, and I called GetSpeed, it doesn't give me any errors, I can run this and it outputs 1. But the other thing that's interesting is that I do 1 here an then 1 here and call GetSpeed, what do you expect? It's going to return 2 right because you're jumping 2 objects to the pipeline in this case. So you got 2, 2 1s. If I do the same thing up here where I put 1 in there and call GetSpeed, I only get 1. That's because the PowerShell methods don't actually have access to dump things to the PowerShell stream or the pipeline. So it doesn't---that 1 is just thrown away when that's executed. And what's interesting about PowerShell methods as well is that they're actually ScriptBlocks. They are not compiled completely to .NET code, so you can use things like other cmdlets. So in this case, I called GetSpeed and I actually wrote to the host and said Hey and it returned 1. So there's a couple of gotchas with using methods. So it is PowerShell, but it needs to be formatted in a specific way to kind of play in the .NET world, so that's how that works. Alright, so let's look at one more concept, kind of basic concept that I want to talk about and is that the fact that we can actually create enumerations. I think he has a question. Oh, sorry. Go ahead. Is there any other way to create more of like a read-only type property where you still want to see it as a property, but not have it modifiable? That is a really good question. I did not find a good way to do that. (Audience comment) No, there isn't. Okay. So this kind of reminds me of Java. You can do static, yep. Can you do static properties? Yep. (Audience comment) Yeah, so this would be a static property. No, that's in C#. Sorry, go ahead. Can you do more than one overload, but where you say New bike and you do it with the one variable, rather than two? Yep. So overloaded or nothing exactly. So if you were to just get rid of maybe you didn't want manufacturer in there and just wanted the model. Yep. Yep, you could do that or you could nothing. You could put back in the original with nothing. You could do bike. Oh, yeah right. Like this. Yeah, then you put your default back again. Yep, exactly. One other question. After you've defined the class of what variable, can you still look at the Speed property by just a $bypass speed? No. So if I went Bike.Speed, oh wait, maybe you can. It doesn't make sense though. Do you know why that is Jeff? That seems like a bug? Well normally what I do is I would write this separate function which would just take the parameter and then brings up a class and then call repetitive. Right. If you create a function and it takes a parameter to the bike and then I can just call the internal method of the class. Ultimately, you don't want to have to be mucking around writing code like that. You want to build _____ and you want to build functions that someone can use and then call all this that you're typing now. This is a demonstration. Right. I'm really surprised you could access this because it should be hidden. Can you do status also? I think if you do like bike Select *, it would do the same thing Bike Getspeed down. Oh, I got you. That's interesting. Do a GetMember to that line -Force. Ha ha. Interesting. So it is hidden. It's hidden. It's there, it's available. But then you can Stop, right. I would say yeah, you probably can just hide it too. Did you read class? Uh, huh. The class is being created every time. So it really is hidden. It's not actually private. Right, okay. (Audience comment) Oops. Yeah, not really it. (Audience comment) Right, unless do -force and then it will be there. You could probably do it with Bike.Speed = 100. Yep. Interesting. We're all learning today. You could always make it a static pin. Should be able to. Yep. So then you could go Bike Speed = 100. Yeah because it's no longer part of the actual object. It's part of the class itself and you can mark it static. Alright. Huh? (Audience comment) It returned 0, which is kind of surprising because well it didn't find that, so it probably returned the default value because it didn't know what speed it was. But if I did something like, yep, so I did Bike Speed here and then… (Audience Comment) We found this object. (Audience Comment) Oh, you're right. (Audience discussion) Right. Yeah, pedals broke this. Right. Or we can assign it. Yeah, sorry. Alright, okay.

Extending from Base Classes

Yeah, now that we've experimented with hidden, I'm going to get into another example. Let's create a VideoGameConsole. So a VideoGameConsole has what, a company that makes it, another string, and a string that makes it and a name to it how about. So now we can actually go off and we can create VideoGameConsole. Simple enough right? Let's create a constructor that does that, so Company and Name, and then this.Company = Company and this.Name = Name. Alright, so now I can't create it without those two things. But let's say we wanted to actually create a I guess more specific version of a VideoGameConsole, so just like you can create a video game, let's extend from VideoGameConsole, which is our base class now. So in this case, if I wanted to create an Xbox, what I can actually do is I can say VideoGameConsole and this is the syntax that you use to actually extend from another class, so in this case, Xbox is a more specific version of a VideoGameConsole. It's still a VideoGameConsole, but it has its own properties to it. And you can see what's happening here is the base class VideoGameConsole does not contain the parameter of this constructor. Just like we had an issue when we were calling new before, in this case, it can actually even define this new class because it needs to call the correct constructor and since video or Xbox has a parameter list constructor, it causes this error. So what we need to do is actually call the correct constructor for the base class. So this is the syntax for that. So I say Microsoft Xbox and what this actually does, Xbox, I think that's here. (Audience Comment) Parentheses after Xbox. (Audience Comment) Oh, yeah duh. Thank you. Yeah. Alright, so you can't create a VideoGameConsole. It still gives an error, but now I can create an Xbox because the Xbox because the Xbox actually passes in the values to the base classes constructor. It sets the values and then when it's written out to the screen, we can actually, we have an Xbox. And the thing about this is you can say, well is this a VideoGameConsole? Of course it is. It comes from VideoGameConsole. But is this an Xbox? Of course it is an Xbox because it's a more specific version of a VideoGameConsole. So on the same vein, what we can do is you can actually create a new VideoGameConsole. So if I wanted to create a Playstation, what I could do is replace all the important pieces and now I have Playstation. So is a Playstation Xbox? No. But is a Playstation a VideoGameConsole? Yes because they both inherit from the same base class. So one thing that VideoGameConsoles can do is they can actually let's say draw their logo. So to draw a logo, what do we do? We need to maybe find some ASCII drawing program on the internet that can convert logos into ASCII in PowerShell and then maybe I have that snippet here. (Audience Comment) Alright, so the contents of a draw logo look like this. They take in that ascii.ps1 file, they grab some Xbox.gif that I had sitting around, and then they call that to actually draw this. So let's actually switch out here. Save this because one thing you'll notice is I am using PSScriptRoot in this particular method and that's actually global scope variable for this particular script, so that's why I had to save it. So now if I do, if I want to call that what I can use down here is change this back to Xbox since that's where it's defined and DrawLogo, which is neat. So save that and, oops don't, save that powershell MyDemo and that didn't work the same way that it worked last time. (Audience comment) I would if it would draw in the ISE. Aw, that looks horrible. But you can see it took the actual Xbox logo and it drew it in ASCII inside PowerShell. Okay, kind of neat trick, but now let's say we want to go out and do the same thing in the Playstation class. So like any good developer, what do we do? Copy and paste the code, put it in the Playstation class. All we need to do, right, is change the Playstation logo. So let's say we have Playstation.png, change that, let's change this to a Playstation, save it. We're on our little gimmick again and you can see it drew the Playstation logo rather than the Xbox logo because it's a different class implements that. Could you put that like in the base class and move that down so you can… And for my next trick. So what we're going to do so that we don't need to do that is do exactly what you just said is take that, put it the base class because we don't want to reimplement all that logic again, alright, so that's why we do that. What we can do is actually make that hidden and then let's just have a file path here, rather than having to pass in or have to reimplement all this logic over and over again. What we can do is we can actually just call this. So get rid of all this and we can say this.DrawLogo, pass in Xbox.gif, grab that, put that in Playstation as well, and now we don't have to worry about reimplementing all that logic because twice as much code, twice as many bugs and you change that for whatever reason you have to redo it over again. So you can see now it still works the exact same way, but all the implementation details are up in the actual DrawLogo implementation inside VideoGameConsole.

Extending from .NET Classes

So this is all really cool and you can do all kinds of cool stuff with PowerShell classes, but the other thing that you can do is you can actually extend from .NET classes. So what we could do is you could say class DoubleList and there's a nifty little thing that you can do now, using namespace System.Collections.Generic. So using namespace, have you ever had to type out a really long type name like that before? So what using namespace allows you to do is to not have to do that, so like if I did New-object -type name, I could just do List int right? If IntelliSense ever worked. And it didn't actually, he'll actually go back to that. So that works because I'm using a namespace now, so it shortens all those namespaces, so this is kind of a concept that's always been in C# and vb.net where you can kind of shorten namespaces without having to type it out throughout your entire script. But if we wanted to actually extend from a list like that what we could say is DoubleList. I think there's a bug here I was going to email people about. I'm going to actually take this again and we're going to extend from that class. So now we have this DoubleList class. We say new-Object DoubleList. That works because it is just a more specific version of a generic list that has integer types. But let's say we wanted to double all the input as we add it to that particular list. So what we're going to do with our new method and we're going to say add, we want to take an integers of some value and we want to add it to the list. So there's this really cool syntax that you have to use to actually get to the base class where you actually say List int this .Add value. So what this actually does is it actually casts this variable into the base class' type and then calls that version of Add. Otherwise, you're going to end up calling yourself and it's going to be recursive. So in this case, what we want to actually do is multiply by 2. So every time I call add, call the base class' version, but before we do that actually multiply it by 2. So we create a DoubleList, something like that, like DL.Add 2, DL and I can see it outputs 4. So this is an example of how we can actually extend from .NET types. So we can do all kinds of things like I've seen examples of people extending from Windows forms controls to do new user controls, to do new Windows, that kind of thing, without having to do any kind of add type stuff, so that is an example of that. Alright, so now I kind of want to talk about DSC resources and why we have PowerShell classes at all.

DSC Resources and PowerShell Classes

So if we jump back to here, let's actually go into WindowsPowerShell Modules, this is a DSC resource for Hyper-V. You can see that it has a DSCResources folder, under there it has more folders for the different types of resources. Each one of the resources is made up of a MOF file, so now all of a sudden you're going to be in this MOF syntax where you need to specify all these particular attributes, you have to extend from OMI_BaseResource, it's just really hard syntax to kind of try to remember. And then when you want to implement the script, the actual details of that particular DSC resource, what you need to do is actually create three particular functions, you need to create Get-TargetResource, Set-TargetResource, and Test-TargetResource to be able to actually work with that particular resource and our DSC configuration. But with partial classes, they make it a lot easier. So I actually have a DSC resource I made with the new PowerShell class syntax. So you can see here that the way that you do this is you actually, is that big enough, okay, you use the same class keyword, and in this case, I have a light. So I have some like special lights at my house that I can control from the internet, which is a horrible idea, but you just put a DSC resource attribute on the top of it to signify that this is a DSC resource. One thing that's required is a DSC property that is marked as a key, in this case, it's the name of the item or the name of the light in the house and it can be a string or an integer. From there you can add all the properties that you want. You can specify different types. You can actually specify enum values and then you're required to implement three different methods. The first method is set, so obviously that updates it through the state that you set it to, test tests to see if it's in the state that you require it, and then Get just returns that particular resource in light. So these signatures are required to be in this state for this to work and that is kind of how that works. So like in this case, it's just going out to the internet and actually setting these things. And then the other side of things is just the manifest file that you need to make sure that you actually don't do that. When you create a new module manifest, there's a new property called DSCResources to explore, which apparently is not in here. I've actually installed this DSC resource already. I'll show you an example of how I use it. You know, let's actually open this one, something like that. Alright, so just like you would create any other configuration, you create a configuration, in this case, I'm just naming it Lights, and then you want to call Import-DscResource. So Import-DscResource looks in two places for your DscResource. It is going to look on a system-wide modules folder in system32 and MOF64 and it's going to look in program files for that particular resource. It's not going to look in any of your user level resources because LCM runs as the system user. And then you can see that I'm just using that resource as is. So in this case, I'm using my light resource and I want to turn on the Basement Light, that's the key. This is the URL; it's going to turn Bill's basement light on. And then I want to set it to ON sort of thing. From there, we run lights and then we start the DscConfiguration, so I can run that, and you can see that it is now running that DscResource that we just created. So we actually went out and okay, I want the lights, you can see it created the mof file and we look in here you can see it looks like just any other DscResource. It's actually using that Light resource to send a Url to state and that kind of thing. Yeah, and I ran it on this computer. So that is how you kind of define DSC resources in PowerShell. So it's a lot more straightforward than the actual MOF format that you saw previously. Alright, I feel like I totally missed something.

Q&A

Let's slip back to the presentation. So that was kind of my demo and everything. I want to kind of leave it open for questions. I know there were some in between. Yep. I was wondering about deriving like constructors. Can you do that as well? So like say you had like your base class has got whatever kind of copy or something and a constructor for that, that sets off my properties or whatever, then you want to basically like extend that and add another class with a few more properties. Do you inherit that constructor? In terms of inherit, you would call it from the other constructor. You wouldn't really inherit from it. So you had, just kind of like how I showed before where you call it with that base keyword, so if you had Class2 and Class1, you would actually have to call it with that base keyword where you'd say Class1 base and then call anything in there sort of thing, if that makes sense. Yeah. But yeah. Any other questions? Yep. Probably a great person to ask, do you know if there's ever going to be a merge in Visual Studio code and the ISE? If you were in David Wilson's session earlier today, that's kind of his goal. So I would definitely watch that session when it gets up on YouTube because that's what he talked about. The PowerShell editor services are kind of where everything is heading, so hopefully we're going to have one editor service to rule them all sort of thing. If you actually get the latest preview that's going to be coming out soon, you'll actually be able to say File, instead of New PowerShell Tab, there's a new PowerShell Experimental Tab and that is actually using the new editor services, so that's a good way that the community can start to test those editor services. Let me ask you answer your question more directly. We're not merging the two editors, we're just going to share a lot of the… Yeah, okay. Sorry. I didn't realize you were sitting right there. Mission accomplished. Yeah, right. Cool. Cool, well thanks for coming and I hope you have a great rest of your conference.

Creating a PowerShell Toolkit to Demystify DSC

Introduction

Hey, welcome. So welcome to creating a DSC or welcome to creating a PowerShell Toolkit to Demystify DSC. For those of you who don't know me, I'm Mike Robbins. I go by Mike F Robbins online. I'm a Microsoft MVP on Windows PowerShell, a SAPIEN technologies MVP, leader and co-founder of the PowerShell user group, the Mississippi PowerShell user group, co-author of Windows PowerShell TFM 4th Edition, author of Chapter 6 in the PowerShell Deep Dives book, winner of the advanced category in the 2013 Scripting games. You're not here to learn about me, but if you want to know more about me, see my blog site. I blog every week. I've got about 400 blog articles out there. I've been blogging since 2009 and get most of my hits from Google searches or Bing searches and I get about 2,000 hits a day, so that should tell you what type of content I have. Okay, so I've got some questions for the audience. This one thing of the sessions I've seen this morning, I'm not just going to get up here and preach to you because I want to know what level you guys are at before I start that. I don't want to talk over your head, but I don't want to talk underneath you as well. So how many IT pros do we have in the room? Okay. And how many developers? Okay, looks like we've got a mix of people in the room. So okay, so how many people are using PowerShell V4 or higher? Hopefully, that's going to be everybody in the room. So hopefully everybody here is writing functions. Okay and modules? How many people in the room are using Desired State Configuration today? Okay, good. I'm glad I started with some questions because I probably would have been talking over your head. I would have assumed that everybody in the room is using Desired State Configuration. It looks like we've probably got less than half of the room who's using it. Okay and how many people are familiar with Public Key Infrastructure? Okay, if you're not familiar with that, you're going to use DSC, that's something you want to invest some time in learning because that's how you're going to prevent putting passwords in your MOF configuration documents that are in plaintext and that was a capability that was possible in PowerShell V4, but in version 5, you can actually encrypt the entire MOF document. So how many people are placing their PowerShell code in some type of source control system today? Good, I actually see more hands than I expected. Okay and how many people are using Pester or some type of unit testing for their PowerShell code? Good. These are things if you're not doing today and definitely invest some time in doing them, it'll save you a lot of time down the road. So what about your---I want to talk a little bit about job responsibilities. You're here at the PowerShell summit this week, so who is back on site doing your job this week? You know, if nobody has to be doing your job this week, then are you really needed, you know? And my question too is do you really trust the people that are there on site doing your job. Now hopefully, you've automated the majority of your job and I don't know about you guys, but I don't worry about automating myself out of a job because if I do, then I'll just find another one and do the same thing again. So anyway, what this session is really about is when you figure something out, create a tool to accomplish that task and then maybe you want to create some documentation. My documentation is my blog site. I left early Friday to take my wife out to dinner since I was going to be here this week. Well my boss calls me after work. He says, hey you know that code you wrote to do X, where can I find a copy of that? I said I'll give you one even better than that. Just go to my blog site, put in this key word. It'll be the only article that will come up. You'll not only find the code to do what you want to do, but it'll walk you through it step by step. So that was the end of the conversation and I was able to enjoy the rest of my evening, instead of having to get into the system remotely or spend usually most of these more complicated things, you can spend twice as long on the phone with somebody trying to explain to them than just do it yourself. But once you figure out how to do something right, a tool to accomplish it because if you don't do something for six months, unless you're a lot smarter than me, then you're not going to remember the details of how to accomplish that task. But you write a tool, you figure it out once, you write a tool, you never have to figure it out again unless things change like from PowerShell V4 to 5 with DSC, there are some differences. And I've got some tools that it, I try to make my tools version agnostic so it doesn't matter what version you're using. Okay, so the content we're going to cover in this session will briefly touch on all this, so we'll talk about DSC, Desired State Configuration, we'll talk about the Local Configuration Manager, Push mode, Pull mode, a little bit about DSC resources, some new features in PowerShell V5 as far as DSC goes, we'll talk about functions, modules, toolmaking, automation. It's not a deep dive into any one of those topics, but we'll touch on each one of those. So it's time for the demo.

Building an SMB Pull Server

Okay, so we've got four VMs that we're working with that are running on this laptop computer. They all have PowerShell V5 on them. We've got a domain, the servers, the three servers are running Windows Server 2012 R2, Server Core and no GUI, which is what you really should be running. GUI's don't belong on servers. If you do want to use a GUI to manage your environment or somebody on your team does, that's fine, but manage it from the desktop, don't RDP into the server. So PC01 is a Windows 10 machine that's, they're all in the same domain. So for the purposes of this demo, what we're going to be working with is an SMB pull server and for those of you who haven't worked with DSC, there's two modes of configuration delivery. Even if you haven't worked with it, you probably know this, but push mode and pull mode, and push mode is the default and it enacts the---it results in immediate delivery and the then next the configuration. This script is heavily commented and this script here is actually already on GitHub, and my presentations, repository uploaded it last night. Now I have some bonus content. I may not get to that, but I've got to re-upload that one tonight. So I want to show you, we'll just look at the root of the C drive on DC01. We've got one SMB file share already and we've set the security on that as well, so Domain Admins has full control and domain controllers has read access. Now at one time I was setting my SMB shares for my DSC configurations and my resources, I was setting that to so domain computers had read access, but believe it or not, not every computer in your domain is a member of domain computers, and specifically, domain controllers are not a member of domain computers. So what I've done here, I've actually separated the environmental configuration from the structural configuration and I'm not going to dive into that topic very much. It's what I recommend, but if you have questions about that, I actually have a very detailed blog article that I wrote for PowerShell magazine. You'll find a link in the code here. And then also there's a good article, there's a lot of PowerShell documentation now on GitHub, if you weren't aware of that, you can actually contribute to that, but there is a specific article that shows you how to do this as well. Now they're actually using a different AccessControl DSC resource than I'm using. I'm using a SmbShare DSC resource. I've got the links to GitHub, but the AccessControl DSC resource was written by Rhon Edwards and he spoke on that on access control last year at the PowerShell summit, so you can find details about it on the YouTube videos. So what I'll do, I'll go ahead and define my structural configuration, which is the logic. And the environmental configuration, I'm going to store that in a psd1 file. I do want to show you something here. I'm going to change this just to something else, psd2 because I've seen that, I've actually seen this written in some books that they say hey you could store this in a different file type. That's not true. So if I try to store the data in something other than a psd1 file, I mean you can store it in a variable, and there are some benefits to shorten a variable instead of a psd1 file. So if I try to run that and actually that did work, so maybe my testing was not valid. So let's do text or actually text. (Working) Yeah, so that did work. So maybe it was PowerShell V4 I was testing that on. Okay, for the purpose of this demo, we're going to stick with the psd1 file. And see that's one of the things about PowerShell, if you think you know, nobody knows everything about it, but if you think you know everything about it today, then just wait until tomorrow because they'll change something. Okay. What I also want you to notice is how all this code I've showed you so far is formatted. So format your code for readability. Your future self will thank you for it, as well as your coworkers. And if you're looking for help online you're going to paste your code, to be honest with you, me, or nobody else, if we can't read your code, then you're probably not going to get any help, but if it's really easy to read your code, you're much more likely to get help. Okay, so I'll just show you the psd1 file that we created. That's what it looks like. It's actually a hash table and then it's an array of hash tables for each of the shares I'm going to create. One of these shares already does exist. Okay, we'll create the MOF configuration document and we'll try to apply the configuration with push mode. You'll notice we got an error message. So I was in Kirk Munro's session last and he brought up a good point that read the message. You know, a lot of people they'll call somebody else over to get help from them or even they'll run the command again and they'll get the same error. Read the error message because generally, it'll tell you exactly what the probably is. And here, does anybody know why we got this error message? (Audience comment) Okay, I'm actually highlight it. Yeah, that's better. There you go. Okay and I'll just go ahead and tell you it's because we don't have the DSC resources on the domain controller and remember we're running PowerShell V5 here. So what are my options for getting my DSC resources on the domain controller? I'll open this up. So you can see those are the default modules, not necessarily DSC resources, but that's the location I recommend putting your DSC resource in. It's not the only option, but it's the all users module path.

Push Mode

So with PowerShell V4 and push mode, if I was running PowerShell V4, my only option is pretty much copy and paste those modules out there. But with V5 and push mode, you can actually automate the distribution of resources just like you can with pull mode. Okay, so we'll take a look at the Local Configuration manager settings for DC01. You notice we have a LCM state of a PendingConfiguration and that's because we tried to apply our MOF configuration document. I've also noticed that sometimes when you're in that state with PowerShell 5, you'll get this warning message. There's a cmdlet called Remove-DscConfigurationDocument, so we'll go ahead and run that. We'll check the state of our LCM again and the main reason I wanted to show you this is that all our settings are the default, but you'll notice now it's actually idle and we don't get that message since we've removed that configuration we tried to apply. So what we're going to do, we're actually going to configure the LCM, the Local Configuration Manager on DC01 to pick up the DSC resources from an SMB file share. And one thing, there's actually a bug in this, you actually have to set a configuration id, even though it's not needed for a push mode. So we'll apply the MetaMOF. When you go to configure the Local Configuration Manager, you actually get a MetaMOF file and I don't know if you saw that here, but it says MetaMOF. So I'll query the LCM again. We'll actually query it here. So you'll notice we've got this ResourceModuleManagers property now. So we'll drill down into that, we'll expand that property, so we can see exactly what was set. So this is what was set. And then we'll take one more look at it, just grab the settings that I really want you to see. So DC01, we had to set a configuration id because of a bug in PowerShell 5 and we have it set to a UNC path and we're still in push mode. So what we want to do, like I said this is about toolmaking, so what if while you're here, your coworker needs to make a modification to your configuration management system? Do you trust them to take a DSC resource and put it in the proper file name and the file name is going to be the name of the root module, it's going to be the version of the module, and then it's going to be zipped and it's going to be .zip. So that's usually than me, if I didn't have this created, if I haven't done this for like six months, I would have to look up, okay, what's the details of that. Or what I would do is go look at it and say, okay, what's one like today? How did I previously do this? So the information that you need to create this is actually really simple. You can run Get-DscResource and I'll specify the two resources that we're using. So that one command will actually tell us the module name. It'll tell us the version and those are the resources we're using under name. I'm not a 100% sure in PowerShell 4 that it gave you the version because I actually used Get-Module to get the version with. So now let's--- in PowerShell 4, there wasn't a compressed archive cmdlet, which they added in 5, and I've actually read in several places where you couldn't use the .NET framework to zip the file, but I've been doing that and I've had no issues and you can actually use common object as well and I have an unzipped file function that'll actually check and see if the .NET Framework is up-to-date, and if it's not, it can use com, so it's down level to like PowerShell V2. But anyway, I'll just show you this function real quick. I'm not going to walk you through all the details of these functions. This one here, of course, it's got comment-based help because if you don't have help, then how are people going to know how to use your functions. It requires a certain version of PowerShell. It includes a parameter validation and so on. So now if you're running PowerShell V5, you don't have to resort to something like that. So to get the file name I was just telling you about is fairly simple. I want to show you what the file names would be for these two DSC resources and there they are there. They're ModuleName_theVersion.zip. So all that information is very, very easy to grab. But trying to figure that out manually is a challenge, especially this week while somebody else is trying to do it. So you also have to create a checksum for the zip file and that doesn't matter if you're putting the MOF configuration document out or if you're putting the DSC resource out on the file share or on a web server. You have to have a checksum and the checksum is actually how it knows that an update is available. So I can't tell you how many hours that I've spent that I've updated a configuration or a resource and then I didn't regenerate the checksum. What happens is the server has a copy of the checksum, so it compares the two checksums, says oh the configuration is the same. I have a couple more functions here. So I've got one that publishes the DSC resource to the SMB share. You'll notice all my modules and all my functions I use the prefix administer for Mike Robbins just so you know why that's there. When you scroll down this one, you'll notice I'm using the New-MrZipFile function. You could very easily use the compress archive and then also I'm using the New-DscCheckSum command, but what I try to do, I try to break, I showed you the New-MrZipFile command, I try to break my commands down so that I'm not writing 500 lines of code in 1 file and I'm not duplicating that code between functions. I put it in a module and then if I call it with another function, then it just loads that function and uses it. So I consider that, if you're duplicating data, that's real bad because you're creating technical debt for yourself. Now redundancy of systems like you want to have more than one domain controller on site, of course, but if you've got 50,000 copies of basically like the same database, that's really bad because one's going to get updated and then all the other ones are going to be out of date. So it's kind of the same thought process with these PowerShell functions. And I have another one called Get-MrDSCResourcePath that gets the path to the root module. So for creating a Guid, in PowerShell V4, you didn't have a cmdlet for creating in Guid. In version 5, you've got New-Guid. So I'll show you the one that I created. It's very simple. So I can use this, and notice I've got comment-based help, even though it's very simple, that's it. So and I'll show you this. You know, a lot of times you find commands like that, so let's take one while actually we can still see it on the screen. Notice the built-in one in PowerShell V5 is a function. So what that means is it's not compiled, so we can see what the command looks like that they wrote. So mine was written before they wrote theirs, but of course, we've kind of accomplished the same thing the same way. So what we can see is we can see exactly how they wrote their command. Let's make this a little smaller. So this is the SMB file share that we have defined for DC01. What we're going to do, we're going to publish the resources to that location. So we're going to run this command and you'll notice in the window they're going to show up. It's going to be zip files. It's going to be the right name and it's also going to have a checksum. Wasn't that easier than figuring all that out manually? (Clicking) Okay, so now we'll look at the domain controller. This is the location and let me make this larger so you can see. So it's the C, Program Files, WindowsPowerShell, Modules folder. We're going to pin this open so that we can run a command behind it and still have that window open. We're going to run the same command we ran before to apply this configuration to DC01. So what's going to happen this time is it's actually going to pull those resources down from the SMB file server, even though, we're in push mode and that's, like I said, only available in PowerShell V5. Okay, so I have another command that I wrote. I don't want to have to figure out how to get the which event log, there's only one event log enabled by default on DSC and there's a couple other ones that are not enabled. So I just wrote a command that's a wrapper to a Get-WinEvent to get the commands from the remote machine and it has a, I'm using invoke command. So we'll go ahead and run this. One of the things I like about grid view in PowerShell 5 is you can do a Ctrl+ and make it larger. It'll tell you exactly what happened, that there's resources. You can see here are the PowerShellAccessControl when it was successfully installed and it'll tell you the location, and with PowerShell V5, you have a side by side versioning, so it puts it in a version folder, but the same command I could run on a V4 machine and it would actually put it, it wouldn't create a version folder. One thing I want you to notice here is that it says the PowerShellAccessControl module was downloaded to this location C:\Windows\TEMP, whatever this temp file is, so every time you run this, you're resources that are being downloaded are being downloaded are being downloaded into a temp file and that's not being cleaned up. So if I go look at the temp file folder on the domain controller, you'll notice local time for me is 1:23 or 1:25 PM. Two minutes ago, we had this temp file show up and you'll notice that I've got like 50 million of these because I've run through this demo like 50 million times. So unless I go up there and manually clean these up, then they're just going to keep building. Okay, so what did we accomplish with that configuration? We actually created the SQL and the web SMB file share. Not only did we create those, but we also set the security so that we have a SQL Server's global group in AD that contains our SQL servers and that one has read access and then we also have a SQL admins group that contains our SQL admins that can configure those servers. So in this scenario, just imagine that we have three groups of people. We've got the main admins who are doing the domain controllers, we got SQL admins that are doing the SQL servers, and we've got web admins who are doing the web servers.

Pull Mode

I've got a question. Sure, go for it. The SMB share for your DSC resources on the pull server, in a push mode is there a way to specify alternate credentials to get those resources? I'm not sure to be honest with you. With the LCM, you can configure them. You can configure the credentials on the LCM, but I'm not sure it'll use those in push mode. So if you're in pull mode, I mean, you can do that. What I'll do, I'll actually test that after the session, and we'll find out. Quick question. So if you're using partial configuration to kind of separate which department is handling which configuration aspect on the system, is there any way to define like an order of operation or handle conflicts if they were to rise, like set a precedent for like, okay, I have this server admin portion configuration for always trump whatever conflicts… It depends because if it finds a conflict, it just stops. (Audience discussion) Yeah, that's what I would think too. I haven't actually tried that, but that's what I've been told as well. (Audience comment) Okay, so let's talk a little bit about pull mode. There are some new features in pull mode as well. So we have a SQL Server, we have all the default settings. What we're going to do is set the Local Configuration Manager so that it will be in pull mode. This is actually compatible with version 4 and higher, although, there is a new way to configure it in version 5. And the reason you need to know that is because when you're querying the Local Configuration Manager, if you're designing a tool, you need to check both locations if you're going to make your tool version agnostic. You want to check it to say, okay, is it set here, and then if it's not set, then go ahead and check it somewhere else too. Check it in the new location because the way you can configure this in V4, it's compatible with V5, so the machine could be set one way and you'll---my point though will be a little bit clearer once we get this configured. Okay, so we create the MetaMOF, we go ahead and create it on SQL01, we take a look at the Local Configuration Manager, now we're in pull mode and we have this DownloadManagerName and we have a SourcePath up here as well. Usually, you have to expand these properties to see what's really set so you can see what's it's a UNC path, but it's in a different location on the Local Configuration Manager. So let's take a look at the modules that the DSC resources that are installed on SQL01. So you'll notice that I've got one that I wrote probably a year ago that's called cMrSQLRecoveryModel. I'm going to remove that, so now you see it, now you don't. We're going to define a structural configuration for both DC01 and SQL01. This is the same thing we did earlier, except we're defining one for two different servers, two different types of servers. I've got a lot of comments in here that we're going to gloss over. But like I said, this code is available already. I just want to look at---I want you to be able to see a specific settings and I think this is actually the same thing I showed a couple of minutes ago. So we did set a configuration id and we set it to refresh mode and the SMB path, as well. So what we want to do now, we want to create a tool to automate the deployment of MOF configuration files to the SMB share. So what needs to happen with that is the MOF configuration document needs to be renamed to the configuration id on the Local Configuration Manager and then we have to create a checksum, which I already mentioned. So we'll have a Publish-MrMOFToSMB tool. So if I take a look at that, it actually goes through, it checks several things, it checks to see if it's in pull mode. If it doesn't, it generates a meaningful error message. It checks all these different locations that can be defined in V4 and V5. That way, whoever's using this, they don't have to worry about oh well do this if it's V4 and do that if it's V5 because especially if you're talking to a junior level engineer, the last thing, this is confusing enough as it is without having to know all the manually know if it's V4 do this and V5 do that. Okay, so if we look at the SMB share for the SQL servers, the neat thing about this one, it'll actually read the Local Configuration Manager settings, so it'll get the configuration id, it'll do it all for you, so you don't have to put anything in manually. The only thing we're doing is running the configuration to create the MOF files just like you would normally do, but then you just pipe it to the command I've created an it's going to do everything for you and put in on the SMB share with the checksum and all of it. And if you give it more than one computer name, which I've actually given it two here, if you give it more than one computer name, if one is not in pull mode, it'll give you a meaningful error message, but it will go through like 50 servers, and if it's got different settings to find in the LCM settings on the servers, it'll put it on the different shares. So when I run this and you'll notice we've got a meaningful error message and I know you can't hardly see that, but that's for DC01, it's because we're still in push mode, so it stops and it gives the user of it, it says hey DC01 is not in pull mode, so instead of them spending half a day troubleshooting this. But now we have our MOF configuration documents for the SQL Server and I'll just highlight this again, so you'll be able to see it. So the other thing we need, I'll show this one more time, so we use the same command before and this actually doesn't read the LCM settings, the command, we have to give it the SMB path, but my thought process with that is normally, you're going to have a MOF configuration document per server and I know you can have a group of servers and you can actually use the same configuration for multiple servers, but with the resources, you probably don't want to publish the resource for every single server because those are going to be, I might be using the PowerShellAccessControl module for multiple servers, and I don't want to constantly be publishing that out. So I'll go ahead and publish the required DSC resource out to the share, I'll take a look at the module folder on SQL01, in addition to that, I'll take a look at the configuration folder on SQL01 where the pending MOF is going to end up. So you'll notice we have a previous MOF, we're going to end up with a pending MOF here in a second. We'll pin that folder as well. So we'll run Update-DscConfiguration for SQL01 to force it to pull the configuration and the DSC resource. So you'll notice now we have a pending MOF, which will become current MOF once the configuration is successfully applied. In the background, this is having to load the SQLPS module because I'm setting a recovery model in some SQL Server databases with this resource, so that's why it took a little bit longer than normal. But now you'll notice the current MOF was updated. So if the configuration failed to apply it, it would still be called pending MOF. But also the DSC resource was sent to the server. Okay. It's going to share the same thing in the event logs, the same thing as before, so there's nothing really new here to see. It's putting the resources in the temp folder again. So with PowerShell V5 for a pull server, there's a new location to configure the pull server settings for at least an SMB share. Well I believe for the web server for HTTP or HTTPS, there's a new location as well. (Audience question) I know at least the way I just showed is I know that I could actually run, what is it, Get-DscConfiguration and here on CimSession, you can actually just give it a computer name, you actually don't have to give it a CimSession. So you can run that. You can also run Test-DscConfiguration, and in V5, you've got a I believe a detailed parameter on Test-DscConfiguration that'll show you all the details, not just the tree falls. (Audience question) It's magic. What… (Audience comment) I don't know the answer to that. That's a good question. I've got two things. I actually really enjoy blogging. It's probably one of the things I enjoy doing, so I've got two really good articles I'm going to write from questions in here, so keep them coming. I think it doesn't need the CimSession because the computer name in front of it. So if you don't need your credential… So you can see actually it says SQL01 is it says it's false. Let's look at, let's say detailed. Hmm, that's interesting. (Working) We'll run this one more time. Yeah, so I'll have to research that as well because it says it's not in the desired state. Okay, so what I want to show you now is the DSC settings for an SMB pull server on PowerShell V5. Not sure where I left off, but it won't hurt if we run this again. So we'll create the MetaMOF, we'll apply it, and we'll actually, it's called ConfigurationDownloadManagers, we'll take a look at those settings. So what I'm going to do here is query all three servers and show the LCM settings and I've kind of created custom properties here, so you'll see these are what I would call it, and actually, we've got WEB01 in there twice. So I didn't get an error message, so I'm going to run the command again. I don't know why we had WEB01 in there. I've got gremlins today. You'll see there's like three different paths you would have to check and we've got this pull mode for two of them and then we've got a push mode for another one. So if you're pushing out like DSC resources, you would have to check all these different locations on the Local Configuration Manager.

Cleanup and Bonus Content

Okay, so we've got a few more minutes, so I've got some bonus content. What I also do when I do a demo, just so you guys know is I actually write code to undo everything I just did, so I can actually just highlight this region and we are done with this, so I can just run this and with Desired State Configuration, I can actually put it back in the state it was before I started so that way my demo is clean and I can run it again. Okay, so I'm on my base machine now, so this is live stuff here. Let's go ahead and import this module. It'll take a second and we'll set our, we'll increase our font, so people can actually see this. Okay, so the way I designed my tools, I want to show you this MrDSC module. So what I did I ran a directory of Get-ChildItem on the directory that contains this module. So what I do actually break down all the functions into their own ps1 file and the ps1 file is named the same thing as the function. I'm going to show you what the module and the module manifest looks like and there's some issues you can run into with this. So I have, I created a Pester test, but it's not a standard Pester test. I actually wrapped inside a function and parameterized it because it's a Pester test I want to run on any module and I don't want to duplicate that code. So when I run that test, it's actually going to go out and test and make sure that all the files in that folder match the functions to export on the module manifest. So I'll show you what that, so this is what the Pester test looks like. It's actually a function and I can feed it a module manifest path and all it does, I can give it modeable, so I can use my like entire Get-Repository and have it go through every folder in there and it'll make sure that I haven't left out any ps1 files that are functions that should be exported and that everything lines up, so create tools to create your tools with. So what my module manifest, this is what my psm1 file, so this is where you would normally have your functions, so the only thing I'm doing is going through and .sourcing all my functions and I know June Blender recently wrote a blog article that's somewhat related to this on the SAPIEN blog, so definitely take a look at that. So if you look at my module manifest, what I found is you actually have to specify all the functions in the module manifest when you're doing this because if you use some sort of logic, although it'll work if you manually use import module and import that module, it won't auto import if it's in the PS module path, the cmdlets won't be available. And also if you upload it to the PowerShell gallery, it won't show the cmdlets in the list. Have you thought about writing some things that just build that module every time you update that formula? Yes. (Audience Comment) So what I have here, I'm actually going to copy a command from an old version of my Get-Repository. And by the way, in September, I'll be doing a tech session webinar from powershell.org on Get and using get with PowerShell and you can do all your stuff right from one interface in the PowerShell ISE. So anyway, now that I've copied another ps1 file, notice it fails because we're expecting, and you can't see that, so we're expecting eight functions to be exported and only seven are imported because I copied a ps1 file in there. So I actually have a function that I can run that will give me a list of all the base file names, which by the way are the functions names. There's a problem with Update-ModuleManifest in PowerShell V5 where I could automate the entire thing, but what I can do, I have to do a little bit of copy and pasting, but I can pipe that to clip.exe and then I can open the module manifest and I can actually paste it right here in FunctionsToExport because that's kind of a pain in the butt to update, so then I can save it and I can run my test again. What my test does, my test actually does a force on importing the module to make sure you've got the current one. (Audience question) My tools, I normally don't have any private functions, but what I've see some people do is create two subfolders and they'll have private in one folder and public in another and then that way they can control how what's exposed. Now one thing I am doing, and I'll just tell you what I'm doing, I'm actually when I do Get-ChildItem to get the list, I exclude Pester test and profiles because I had an issue one day where I had a problem with a profile and it's because I kept importing the profile. So you'll notice that's here, so I'm excluding those. So now if we run our test again and we actually that's where we're at, we just ran that. So I want to show you one more little issue here. It's really common when you're writing functions, you'll say -Modules PSDesiredStateConfiguration or whatever the module is. I cannot type. So I don't type in the notes. We'll get it here in a minute. Okay, so it's really common to do that because things I'm doing are going to require the PSDesiredStateConfiguration module. So if I go back and import that again, now I want you to notice that I've got some cmdlets listed. I don't have any cmdlets in my function. Well the thing is when you write your code like this, you'll end up with anything that you're telling, I'm going to run this test again, notice the test failed again, but you don't want to use required modules in your functions if you're breaking them out. You actually want to use, if you go up to the module manifest, (Scrolling) That's it. I've actually got an example here and we're just about done. As you can see my cleanup is there. So in my PowerShell toolkit that I have, I'm actually using required modules, so I have Hyper-V, Pester, PushKit, and PowerShell Community Extensions, so that's what you want to do instead. So you'll notice in all my functions, I actually say hey if you're using this separately, then add this line and I put that in as a comment because somebody can come grab one of my functions and add it to their toolkit without having to grab the whole module. So we'll jump back to the slide deck real quick and then we'll be done. Okay, I'd like to thank everybody who has been on the PowerShell team for the past 10 years. We wouldn't be here today and we probably wouldn't have as good as job as we have if it wasn't for the PowerShell team. We'd still be a click-next-admin. Okay, I've got a slide of resources here. This slide deck is on GitHub already as well. So powershell.org, that's why you're here today and there's a whole list of stuff. And there's my contact information and that's the last slide. Thank you, guys.

Debugging PowerShell

Introduction

Okay, hi everybody. Thanks for coming. I did already push the button, so you don't have to ask to make sure. I want to talk to you guys today about something that I spend a lot of time doing, which is debugging PowerShell. I've been working with PowerShell for 10 years now and I worked on, in past lifetimes, Power GUI as an evangelist and then later on as an architect and as a product manager and I used to work on Power WF and Power SD, so I've been working on PowerShell and PowerShell tools for quite a long time, especially in the past with the tool side of the fence. And so through all that work, I've done a lot of debugging and I've come up with a lot of tips and tricks that I use and I'm constantly, I guess the curious side of me is always creating, I think looking for ways to create new tools that make debugging even easier and tinker around with how things work under the covers and finding cool solutions that weren't necessarily readily available before. So the talk I want to give today is about sharing of that back with you. All of the content that I'm sharing here works on PowerShell V3 and later, so this is not bleeding edge latest and greatest, you have to have V5, otherwise it's no good to you. If you're using version 3 or later, anything I talk about here, you guys are going to be able to use, unless I call it something and specifically from a new version, in which case I'll highlight that, but in general that's the way that I'm going to roll with this. So who am I? My name's Kirk Munro. I go by the nickname, Poshoholic. If you want to contact me, that's my Twitter handle, poshoholic.com, poshoholic@gmail.com, poshoholic@hotmail.com, you kind of get the drift. If you want to look for me somewhere around the web, just look for that handle and then you can find me and reach out to me. I work for, these days, by the way, I work for a company called Provance Technologies, which is we specialize in IT solutions management and I'm a technical product manager there still doing a lot of PowerShell in that space. I've been with them for the past three years and just doing PowerShell as well on the side as a hobby and passion at home. So the agenda is just talking about a collection of PowerShell modules that I've written that help make debugging and defensive scripting easier, some tips and tricks, recommendations, best practices, things you can avoid, and hopefully, you'll come out of this a bit more enlightened in some of the things you might run into in debugging your own work, and so that it'll ring a bell when you run into it later and maybe think of some of these solutions and ideas and be able to get through your issues more quickly. And because it's about debugging, I can't really teach about debugging PowerShell from a slide deck, so all the content that I'm going to do from here on is going to be in PowerShell ISE and doing demos because it's going to make learning it a lot easier to see firsthand. By the way, any questions that anybody has, don't feel you have to save them for the end. Interrupt me at any time. If I don't see you, just speak up and grab my attention. I don't have this scripted down to the wire and timed to the last minute. I leave breathing room for working with the audience and if there are particular things you want me to drill in deeper on or have questions about, bring it up and I'll see where I can fit it and how I can handle it. And if I can't handle it here or can't answer the questions here, I'll be happy to do research and figure out or talk to you after the fact in one of the side rooms or in the hall and see what I can do to help as well.

Dealing with PowerShell Errors

So debugging PowerShell is an interesting thing because debugging PowerShell often starts, well always pretty much starts with dealing with errors. And when dealing with errors, it doesn't really matter who or what level of experience the people who are using PowerShell have. Most people get really hung up on the notion of red error text where you'll find very seasoned people working in the industry for a long time, maybe even have done scripting, they run a command that you gave them, and they say it's red and that's all that they give you. They just it's errored, it's red, and in the error text, if they look at it, the actual solution the text is right there in front of them it tells them specifically what was wrong and so you walk over to the screen and you tell them how to decipher that error text as part of the debugging steps and then you kind of start getting them beyond that, but some people like to actually change the color by default through GPOs and what not, scripts, profile scripts just because it can make things easier for people. So I'm just going to load a helper function here first and you're going to see me flip back and forth a lot between this view and the other one as I run stuff, so if you want me to jump back to something that you want me to explain or detail, let me know. So here's just a simple example showing a typical error. There's a lot of detail in here that's really useful, in particular, this is the kind of example where somebody might just come to you with an error because it failed and says it can't find the path and you go over to the machine and you say well it can't find the path because it says it can't find the path, and so this is the kind of thing that you can run into even when working with seasoned pros. So changing up the color is easy. If you're working in ISE, there's psISE.Options.ErrorForegroundColor, so you can change the color to something that you like a little bit better and I mean that's blue on blue so that's not a great example. Here's another one. You've got a wide variety with System.Windows.Media.Colors to pick the color that you want makes it a lot easier, and actually just even being experienced with PowerShell and reading through the texts through many, many years now, switching the color makes a difference for me too. It's a small thing, it's seems silly, but it makes a difference throwing this in your profile script and just using it for a little while, you might start realizing that the text that's in the error message does pop out more and you start noticing more things that are in it, so it's a cool technique to follow. Now something else you can do with error messages is so how many people here are aware that there's more than one error category view or error view, I guess, one more way that you can view error messages, only---I saw one person half raise their hand, so not many people. So by default, when you look at error messages and you see text like this and the green down here, this is the normal error view where you're going to see some detail text messages and some line and character positions inside of the script that ran category information. You can also change the view to what's known as a CategoryView, which is a much more terse view. So if I get that exact same error message in this CategoryView, you can see here, all it's showing me is what's hidden inside of category info in the view above. So if I select this text right here on this line, this is what it's showing me in the CategoryView and all you do is you set this global variable to ErrorView = CategoryView and you get a more terse format, which is good if you're experienced because it can get to the point more quickly sometimes and especially if you're looking at the entire error variable a many, many errors one after another, this can just give you a line by line, so you can scan it for things. So that's a good tip; however, the CategoryView and the category information isn't always descriptive enough to tell you what's going on. I mean, in this case, it's pretty simple because the error type is ItemNotFoundException. You can see that the error message contains the path, so it really tells you, but there are cases where it's more obtuse trying to figure out what's going on from the CategoryView, in which case, you can just flip it back to ErrorView = NormalView, or technically, you can do ErrorView = Anything at all and anything at all, it'll go back to the, let me run that command again, it goes back to the normal view for anything beyond CategoryView. So inside of this logic that PowerShell uses, it's basically just searching for this string else NormalView, so it doesn't really matter what you set it to. If you forget NormalView, just that it's something other than CategoryView and you'll be good. If you're so inclined and you want to take a round with more error views, there really should be more, I had written more. Small side story, I was installing software on my laptop one day, running out of space quickly as I was installing Visual Studio. Rather than cancelling the installation, which would have been the smarter thing to do and cleaning things up and then installing the product afterwards, I decided I'll frantically go around and clean up some stuff while it's installing. And so then I found myself holding down Shift+Delete on my keyboard while I was looking at my Windows Explorer with my Modules folder selected. Ask myself, what do I do now? And pressing Escape didn't dawn on me in that moment, and so I let it delete and tried using some delete recovery tools to pull stuff back and I was able to get back 95% between that and things I'd emailed other people, but I lost a little bit of content that way. So that's one argument for using version control. But one of the things I lost was I had a module that allowed me to define some more error views and I had some cool stuff and I haven't yet pulled that back in yet, but I intend to. If you have things that you would like to see in error text by default that aren't there, let me know because it's on my to do list to I don't know where I'll get to it, but it's in my short list of things I think about, and so I think there really should be more views, so just please let me know. My email address is right here. And by the way, all these scripts, I will share them so you don't have to jot stuff down directly from here if you don't want to because you're going to get the content afterwards, as well as the slides, as limited as they are. Beyond ErrorViews and the color, and this is going to sound silly, but has anybody heard the expression talk to the bear? (Audience comment) Yeah, same idea. So you've got a rubber duck, or your bear, or your Xamarin monkey, or whatever other stuffed animal you keep hidden in your office put it on your desk when you have a problem and explain that problem to it because a large percent of the time, you're going to find the solution to that problem by just reiterating it yourself. So people who talk aloud, it actually does work. So there's a reason why people talk to themselves and that's one. And it's surprising. I mean how many people have had a question related to an issue and you go walk over to a coworker an you're asking that question and as you're asking them, the light bulb comes on and you know the answer to that question right? That's what the talking to the bear is. So it's just about verbalizing what's in your mind already, so that you can release it and put it in front of yourself by saying it and finding the solution.

Identifying Error Sources

So enough about some lighter weight techniques and changing colors. When you're dealing with errors, it's really important to be able to identify what the source of the error is, and so I'm going to go through a few techniques to help identify what sources of errors are. This first thing, I'm just priming the pump by putting an error in the system, so let me just run this, and so I've got 20 errors that I just through into my Error array and you can look at errors inside of $Error. It's just an array list, so if I run that, it flies by pretty quick and there's 20 errors there, a few different types thrown in with a randomizer just to make it interesting. And so, when you're dealing with this, how many people have written a script and you run it and then you get error, error, error, error, error, error because they're not necessarily all terminating right? There's a notion of terminating errors, which is an error that when your script or your module hits it, stops dead. You've got one error, which is great for a focus because you know exactly where you've got to look, but it also masks other errors that are going to show up probably afterwards as you go through the debugging techniques and steps to work stuff out. So a lot of times though, a lot of errors you get in PowerShell are non-terminating and when you're dealing with it, it can be challenging from a large list. Where do you start? So you can use PowerShell itself to help prioritize what the errors are. And so generally speaking when I do talks, I don't use aliases and what not. Here I am because we're talking about debugging and since we're talking about debugging, I'm assuming a certain base knowledge of PowerShell. If you see an alias that you don't understand up here, raise your hand, let me know, I'll explain it, but most of these are pretty boarder plate. I mean, some people might not know that you can use short hand for parameter names, so -Des is short for description and since it's unambiguous in the Sort Object cmdlet, it'll find it. But the for the most part, I'm just using a few aliases. So one thing that's great when you have a whole lot of errors is to group them, so this command is going to take all my errors, group them together, sort them in descending order by count, and then give me the count and name, so I can prioritize. So you run a command like that and all of a sudden, that list of 20 errors because much more digestible. So I can prioritize by what's most common and very quickly figure out what are things I want to fix first according to how frequently they're happening. I mean, the numbers can be a bit misleading, right, because you could be doing it inside of a loop and you could have one bug that's much higher priority versus something that happens 50 times because you ran it inside of a loop, but it still helps just in terms of digesting stuff and that mental blockage of getting rid of 50 errors at once because of this, you sort of think it is by group, so you can start breaking down the barriers and getting to the end solution more quickly, can be very helpful. So leverage PowerShell's sorting and grouping capabilities when you're processing large amounts of errors to really see what's going on. And the other thing that's cool here too is this is pulling out all the source code, all of the line references, and the character references, and just giving me the text, which is really useful in terms of just reading what's going on, there's nothing red up there and I might be able to figure out from that exactly what's going on. If I can't, I can go into $Error and see what line numbers things happened on and figure things out from there. Questions so far? Okay. So beyond processing large quantities of errors, there are a lot of details in errors that are very worthwhile looking at when you're troubleshooting. So if I run this, okay, is that a good example? Yeah, that's fine. So let me go back again. So we're at this again. So here I'm looking at some error details for one of the particular errors and actually let me do one more thing first that I didn't have in my script. So I'm going to pass the entire collection out to FormatList * because I want to highlight something. (Scrolling) Now they're actually all showing the details, so I must have a load---I've got a module called DebugPX that makes it so when I pipe to FormatList *, I don't have to do -Force because typically when you run and that might still be loaded, let me just, bear with me one second. No, it's not there. So typically when you pipe to Format List * from the error, you're just going to see the exact same text that you see when the error shows up on screen. You're not going to see properties on the left column with values on the right. Typically when you run error and pipe the format list, you just won't see that because the way the PowerShell team has implemented error formatting, it basically says I don't care what you told me you want, unless you force it, I'm not going to give it to you, which is a bit silly and I think it's a bit of a design flaw. So in one of the things I worked on is this module called DebugPX and I will get to that, but DebugPX makes it so I don't have to, or sorry not DebugPX, FormatPX, that's why because FormatPX is loaded. I have a bunch of modules and when I load them, they do work for me that I forget about because I load them underneath my profile, so I forget to clear my profile afterwards. So go back to this. Here's what you typically see, Error 0 format list* shows the exact same thing I would have got when the error came up. There, glad I figured that out. And if I want to see it, the whole thing, I have to pipe it to Format List * -Force and that's because of what I was talking about, it would help PowerShell just implicitly think that I'm only going to give you this error text, but it really is for the look of the properties because when you're dealing with errors, you're often dealing with exceptions, and so there's an exception property that you can dig into to figure out what's going on, if it's coming from internal code in PowerShell, or a binary cmdlet, or a DLL, or something in .NET that you called, well that's going to come in often in the form of an exception, and so getting out that exception detail is important. Target object. That's important because what object was being referenced when this error came up. Was it a file on my system? Was it a string path? So when the error path does not exist comes up, that target object should be set to the path that didn't exist so I can figure out what's going on. The category info, you see that by default. Other useful things are your StackTrace, ScriptTrackTrace, and I'll get into that in a second, and InvocationInfo, but there's a lot of interesting hidden stuff that's there that by default you don't get unless you pipe to force. So you use the force when you're looking at errors or now I'll load the module that I had forgotten I had loaded and wait. I have a load order that's required for one of my modules. Now I'm back normal. And so yeah, with FormatPX loaded, FormatPX recognizes that when you pipe from an error to FormatList * or FormatTable * for some format cmdlet, that you probably really wanted it to give it to you that format, and so it respects that and just works around this one little quirk and FormatPX is up in the gallery. All the modules I work on I put up in the gallery. And so now I get by default all of these full information from my error messages when I wipe the FormatList *. So when you're looking at that, I mentioned there's exceptions, right, so you can do, they're objects, .NET objects, same as everything else, so I can ask for the exception and that again shows me text, not properties. And so I have to pipe that to FormatList and again I would normally have to use the force if I was not using FormatPX to take care of it for me and then I can gather all this information inside of my exception. And sometimes, there are inner exceptions, and so certain times you can run into an error. This doesn't happen all that often, but it does happen where the error happens several layers deep through a whole bunch of logic and DLLs and so you go from your top level error to your exception, which had an inner exception, which may have another inner exception, and so be aware of this inner exception property on exceptions because if the error that shows up is not obvious to you off the top, what's going on, it may be because the text that's exposed at the outside of the error is not descriptive enough to what actually happened under the covers and you have to dig in. So yeah, just go through looking at your error messages and look at these inner exceptions, in this case, there is none for this particular error, but make sure you're aware of the .InnerException and that can recourse to multiple inner exceptions and sometimes, you dig through a few layers and you look at the text that's there and it's clear as day what went wrong, but because of details of code and how logic works and how things don't necessarily always bubble up to the top in the right way, it's just hidden from you. So dig through and you can find a lot of interesting things. Leveraging the StackTrace. So I showed you in the FormatList * there is a ScriptStackTrace and that's great for locations that are inside of PowerShell functions or scripts or modules that are script modules because it will show you the StackTrace of the logic that went through to get to that error, which can be very helpful in debugging and figuring out what's going on and there is also a StackTrace that is for when you're dealing with compiled locations and cmdlets and DLLs that's on the actual exception, or InnerException, or InnerException.InnerException, and so on. So you can dig through and you can look at logic, and so this is a bigger StackTrace because this particular error came from binary code and less from PowerShell code. I mean there was one line in my ScriptBlock that really didn't tell me anything, but I can see, I look at this from the, it's about, right, so the most the call the actually through the exception wasn't lookup command information at the very top, System.Management.Automation.CommandDiscovery.LookUpCommandinfo. So you look at this from a top-down from that's where the error happened to that was the first call in the binary code that I invoked and it can be helpful just to figure out what logic was happening, what things went through when the actual error came up. This is by the way, this can be intimidating to people dealing with StackTrace is and going through the stuff, but don't be afraid to roll up your sleeves, try it, if you can't figure something out, and need some help, you can email me, ask a coworker, go to powershell.org on the forums, people can now help you figure these things out, and you start knocking them off one at a time and working with this stuff, and it becomes a lot easier relatively quickly. The other thing, when you're dealing with a large quantity of errors, rather than leaving them lying around in your $Error variable, I personally like to clean up my error variable because when I like to just be lazy and do $Error and look at everything that's in it, but if I leave it around for a long time, then I have a whole lot of stuff that I don't care about that I already dealt with a while ago, so it's just a collection. You can clean things up. So if you dealt with an error, you can remove that particular error just by .Remove with the actual error itself or you can use index, so that removes just the particular topmost item in the error stack or I can remove an entire range. This is going to remove from 0 to 10. Or I can clear the whole thing. And so these are useful as you go through and deal with errors that you have just so you don't always have older things you've dealt with kicking around because then you can be lazy and do $Error and see you did a catch anything and no nothing came up, so good. So that's dealing with error details.

Troubleshooting with Historical Data

There's a lot of error management and error troubleshooting that happens without necessarily dealing with running through a debugger and putting breakpoints and stepping through code. A lot of stuff could happen just from using tooling to do work for you. So dealing with historical information. I mean I put this in here just kind of as a joke. This isn't really, this is just from my TypePX module where I can do all sorts of interesting things on numbers and relative dates and what not, it's one of the modules I work on, but the module I'm talking about for debugging purposes is HistoryPX. So let me make sure I don't have it loaded. No, I don't. Good. So historical information. I've read a lot of commands inside this session here and I can hit h, actually let me do with the latest 10. So there's the 10 most recent messages, which just shows me what I ran and not even fully because it's truncated with … and it gives me some id numbers so if I want to re-run something, but that's not really that useful, so let me just, I'm going to give you an example here. Let me clear the history, no leave the history that's there because I want to go to number three soon. I'm just going to prime the pump here and I want to throw a few things in that are interesting about historical commands. So this code does the same thing twice, right? The first thing is it calls, it does 1 divided by 0, which is divided by 0 error, it's going to throw an exception, and then it calls just put some string will it blend. And on the second part, which is the try/catch block, does the exact same thing inside of a try/catch handler and it's an important detail to follow when you're dealing with error handling because if you look at the way those two things ran, so the codes all up at the top there and then you see the first error shows up and you see will it blend the text because that text still ran. So even though, 1 divided by 0 resulted in an exception, when I ran it, everything that happened afterwards after my semicolon still ran and went through. But then look at the second example. So after the will it blend, then there's an attempt to divide by 0 exception and there's no string. Why not? Anybody know? (Audience Answer) Exactly. So divide by 0 is a terminating exception, unless you don't put it inside of a try/catch block, which is a gotcha in PowerShell because there a lot of places where an exception terminates and it's meant to terminate, it's an exception basically saying hold on, you're doing something really wrong here, but if you don't wrap it in try/catch, then it's not going to terminate. So if you write functions that have an exception that is terminating exception, but you don't wrap it in try/catch, it'll do everything afterwards and it'll just continue running. It won't stop your function from executing the rest of the code, which may have unexpected consequences depending on what it does. So as a rule of thumb, even though I might sound cumbersome, I try/catch everything. Every begin process and end block in my functions have a try/catch at the top-level. Every single one. Every script I write, try/catch. Absolutely everything because that's the only way when you're writing PowerShell that you can guarantee that you get the same behavior every single time. If you don't, somebody else may put try/catch around it calling your function or your script and get different behavior because theirs is going to stop on an exception that yours let go through and just generate an error, which you may think is just benign and you think, okay, well that's fine. It's generates an error, but still does all this other stuff, except that other stuff doesn't run because they called it inside a try/catch block, which is a bit of a challenge, and so it's better to write your code so that it is robust and runs the same way every single time, so I try/catch absolutely everything. But when you do that, and I go, so I just ran that and I got you different examples, now I'm going to look at this history and this time I'm going to pipe it to from that table. So if you look at the last two entries here 41 to 42, they're doing the same piece of code right? Starting at 41 and 42, oh that's right, wait a second. Because I ran it as a block, it treats it as one command. So I'm going to run it twice, each piece by itself. So I'll run that and then I'll run the other one. Now when I look at the history, there, so 44 and 45, and pipe that to FormatTable, this tells me, it gives me a better example of what actually happened, right. So 44 completed, even though it was a terminating exception that completed because it kept on running versus 45 failed. So by using try/catch, it will also be detectable in your history to see where something actually had a terminating exception versus not. Even though these commands completed, they may have generated errors. You can't see that from this. This is the built-in history. So the only thing you can get here is terminated with error or not from the execution status state, but I've come up with a solution to help work around that. So I'm going to import HistoryPX, that's also in the gallery, and now I'm going to just create a bunch of errors. I think I can run all these together. Yeah, we'll run those ones. So there I just generated a bunch of different error messages and now let me show you the count of the most recent commands I've ran. So now, actually sorry again because I ran them all at once, I've got to do this one at a time. It's one of the hard parts about when you're just demonstrating debugging and you want to run commands one at a time is you have to do it one off. So now that he did that, there. So, HistoryPX takes history and extends it. It will calculate the duration, so if you're doing any kind of benchmarking around your scripts and your commands and you want to see how long one run had verses another and you tweak your code here and there and you want to automatically detract things, well you can just have version A, B, C of a script, for example, and run each one and then look at the durations and figure out which one took the longest. It'll show you success or failure. It'll show you the number of errors. It gives you output, so if you had a command that you ran that generated certain amount of output, how many objects were output, so the number assign at output. The number of signs source is how many commands, how many lines in that command that I ran generated output, and I'll get to that one in a minute. It can be really useful because sometimes you run a command that's unexpecting, so that generates unexpected output. So here's an example. Look at this function, New-ArrayListWithOneItem. It's pretty straightforward, right. I just go in and create a new array list, add integer and value to it, and return the array list to the caller. So I'll load that, and now I'm going to run it, and now when I look at the results, I get two records. Anybody know why? Nope. (Audience comment) Yes. So ArrayList.Add returns the index of the value that was added to the array list and there are methods like this all over .NET. You call into some method and it does the work you want and returns a value that you don't need, but you might not know that unless you look at the method signature and when you're dealing with this kind of a thing in a very large command that you run and goes and gets, maybe it pulls the list of the virtual machines from all of your Hyper-V servers and your infrastructure and outputs one and you're trying to figure out where that comes from, that can be really challenging. so that's where the number of sources is useful because I can run that same command, New-ArrayListWithOneItem. I get the two results back and then I can take a look at the history. So if I, well actually yeah, I'll show you the properties I'm working with first. So inside of my history extension from HistoryPX, there are errors and error counts so you can go and look at those collections in the default output. You only see the counts, but you could actually look at the errors for any particular command after it ran. You can look at the output after it ran. So you ran a command, you forgot to capture the output and it went across the wire and did something, it took a while, and you think oh shoot, I meant to put that in a variable, it's got it for you. Output sources and output sources count. That's what I wanted to talk about with respect to this particular command. So I already ran it, so now I can look at, actually going to do it again here because I want to have it in the right sequence, so I'm going to get the most recent history entry, Get-History-Count1 and look at the output source. And so look at the top one in the text property, al.Add 1 returns something, so it tells me what line returned it, the line number, the starting column number, and so that's all the metrics about where it was inside of my command that I ran, what was actually returned, which is really cool when you're trying to find these kind of issues because it just can be a challenge when you have that needle in the haystack returning something that you didn't expect and you don't want to do a lot of commenting and uncommenting of code to figure it out and try to figure out where that particular piece is. And you don't have to think about turning it on. I mean, as long as you have HistoryPX loaded, which by the way, you have to load manually because HistoryPX contains a lot of commands that are proxies for existing commands, so natively is not going to, when you're on Get-History it's not going to say oh you want to load HistoryPX because Get-History is on the box. So if you want to use HistoryPX and you like this, then download install the module and put import module HistoryPX in your profile and you're good, and it'll always track the data for you. Let's see. Oh, so you can also manage the configuration, so if I run this command. So one of the concerns with HistoryPX that I have heard in the past is that what about if I go and get thousands and thousands and thousands of records. I don't want you storing that all on my session inside of the history variable so that you're building up my memory in my PowerShell session and consuming my resources on my system. So it, by default, takes a fairly light tactic for memory, yeah, for making sure it doesn't consume too much memory, so it'll only store the extended history information for the last 200 commands you ran and a maximum of 1,000 output items per command so that you don't end up having too much of a huge memory footprint, but you can call Set-ExtendedHistoryConfiguration and change this. So it's up to you to figure out how much you want to store, if you want to store more and you can do that, as well, in your profile. Another thing I put in, and this is kind of experimental this one, so when you're dealing with history and you have commands that you ran and you forgot to get results, and it's great that it captures it for you, but there's also, you may have seen Lee Holmes put out a I think it was Tweet or a blog post talking about something about variable for capturing output automatically and that works some of the time, but not in every case, and so HistoryPX takes that a bit further and builds it in. So if I run this command, Get-Service, now I can do $__ because we didn't have enough $__. $__ basically says take the last collection of stuff that you got and show it to me again, which is really cool when you're doing interactive, I'm in the command-line running commands, figuring out where properties are, and modifying my script at the same time, back to the command-line doing some more work, back to my script adding the code because I can do things like look at the values in $__, so I can look at the names and notice that the $__ doesn't change. I'm still working with the same collection of services, but when I'm referencing properties in $__, HistoryPX is smart enough to think well you're just digging around, so I'm not going to take those values and overwrite what I stored, I'll just let you dig around and go on your way. If I call Get-Help, it's not going to store that because who wants to store Get-Help and $__. So it has a collection of things that it tracks and other stuff that it says oh, I'm not going to worry with that. And so, even after I run Get-Help, I still have that same collection I had from before keeping track of things. And so the way that works is I have this Get-CaptureOutputConfiguration because it's all about capturing output, and in capturing output, there is excluded types, oh and also there's capture value types and capture null, so those are both turned off by default because value types are things like ints and strings, so it's just going to ignore those altogether and not store them because, again, it's not really saving any time. But for collections and actual .NET objects that are rich, it'll store them, but you can configure that and you can change how that works. So if I look at the ExcludedTypes, then I can see these are all the types that it's going to ignore by default. It's a system of strings in there, helping if we're short, MamlCommandHelpInfo. It's going to ignore results from Get-Member and Get-Command and Get-Variable because those are things we very frequently call just to go and look stuff up that we don't necessarily want to store and overwrite something that we're working in as I'm doing this rapid back and forth on the command-line figuring out some properties and methods and then off on my script running the work. So I like this a lot because it helps me switch back and forth without always having to think about putting something into a variable to do the work I needed to do, and you can configure this, you can remove things from this, exclude type lists, or add things if you want to, and it'll track or not track accordingly.

Debugging with Breakpoints

Okay, so some debugging with breakpoints. Let me show you something really quick here. I'm going to put a breakpoint and I run this. Okay, so I'm on a breakpoint right now and the breakpoint I'm on is the first line in my script on Get-Service. When you're dealing with breakpoints, most people think of in ISE these visual breakpoints where you go to a line and you just hit F9 to turn on a breakpoint and then F5 to run your script and you work with the breakpoint from there. And that's really useful and there are some things that you should know that you can do, so once you're on a breakpoint, excuse me, and these work for on the PowerShell native console, as well, so if you're in server core where you don't have ISE or just in some environment where you are working on the command-line, you can still have breakpoints and look around and do debugging without it. So you can use L to take a look at where you are and you can see the asterisk on the left-hand side showing you where you're at. You can do, what are the other commands I want to show, K, the call stack, in this case, there's only one call, but it's very useful to figure out which commands were invoked to get me to this point. You can run H to see a list of all the commands I can run while I'm in the debugger and these are good to learn and practice, especially stepInto, stepOver, and stepOut, knowing how those work. StepInto will go into a function if you're calling the function, so you debug inside that function. StepOut will if you're in a function because you stepped in accidentally, and you think I'm done debugging here, I want to go up one level, it'll just run the rest of that code and then continue debugging at the next line after the function call outside of the function. And stepOver is basically saying I don't want to go in this function. I'm debugging something at the level I'm in right now, so that's where you stepOver. So those are really useful, as well as continue basically says let it running or quit, I'm done debugging and I want to stop debugging altogether. So learning how these commands work can be really helpful when you're debugging. And there's also shortcuts, so if you're in ISE, you can learn shortcuts up here in the Debug menu, F10, F11, Shift+F11, all of these, or of course, you can use the menu items if you like. Yes. Have you heard of any way to automatically skip over like certain funky calls that you always wanted more? Like say you have a log line and you log XML, but you never want to actually step into that and debug it, you just want to always skip over that? So there is a property or it's an attribute. So inside of a function, do you own the code? Okay, so if you own the code, at the very top of your function, put System.Diagnostics.DebuggerHidden open bracket, close bracket inside of square brackets because it's an attribute. You put that at the top and the debugger is going to totally ignore it. It won't step in if you want it to. Thank you. You're welcome. So, great question. So alright, working with debugger on these breakpoints is useful, but there are other things you can do beyond the built-in breakpoints support in PowerShell that are kind of fun. So I have this module called DebugPX, so I'm going to import it. And now that I've imported it, I can just run Enter-Debugger and I'm in the debugger. Or I can use BP or breakpoint because those are aliases for the command. So I can now, maybe I've been working on a system where I don't have PowerShell ISE, but I have Notepad because I'm on server core and I'm debugging something and I don't want to do line breakpoints because I don't want to count where lines are, but I have the code. I can crack open my ps1 file in Notepad, type in breakpoint, and then save the file, and then run it, and that'll be the debugger on that breakpoint. And that's only if you're using DebugPX because that's part of the code that I've written and I've managed my own modules. And it's the same the debugger as everything, right. So I can still look around, I can still get the help information, it's just another way to enter the debugger. So now let me quit out of the debugger. So yeah, BP's an alias. This one's kind of cool. It works in pipelines. So if you're debugging something in the middle of it and you want to stop during the pipeline process, you can just inject this breakpoint or enter debugger command right in the middle and that'll do it, and you can also do it conditionally. So in this line right here, breakpoint, I'm going to stop when I'm on the Windows update service in this pipeline. So I can run that and it runs through all my what ifs and then when I hit Windows update, it stops. So if you're looking for some obscure conditions that happen in a pipeline, this can be really useful in figuring out and stopping at that point in time because something actually happened that you were waiting for. Oh, and they can also have reminder texts. So sometimes you might have a situation that happens once in a while, really, really weird. You can't figure it out. It's just a bug that shows up on three or four machines in your environment and it's really, really just infrequent and you don't know what's going on. Well you can put in those scripts if you wanted to, you can put a breakpoint with the condition basically being that weird condition that you're looking for, and some message saying just notifying the person when it actually happens. So if I run this, then I get this text BREAKPOINT ZOMG, it happened, to let me know. You know, it could be call Kirk Munro because it's the issue he was looking for and then I can actually troubleshoot and debug and see what's going on. So for weird stuff, that's kind of handy. I don't use that often, but once in a while it can help. Oh, and also all these breakpoint commands work in unsaved files. They work if you're using F8, and of course, this is all PowerShell V3 or later. PowerShell V5 has a wait debugger command, which is slightly different from this Enter debugger command that I wrote a while back. There are some pros and cons to both. They do some things I don't do with jobs. I do some things they don't do with conditional breakpointing, and messaging, and pipelining, and stuff, so they both are complimentary and you can zoom both with version 5 if you like, as well.

Defensive Scripting Techniques

A few more quick things. So I already talked about try/catch and why they're important. I'll skip over that. This is interesting. If you're working with arrays, this one bites people all the time. If you're working with arrays and you're doing comparisons, make sure the thing you're comparing the array with is on the left-hand side unless you explicitly want to be comparing the items in the array with or any collection for that matter. So for example, looking at this code, I create an array and it's empty, so I'll do that. And now I'm going to say $X = null to see if my array is null and I get back nothing. I don't get back true and I don't get back false, I get back nothing because PowerShell reads this and says okay, I'm going to take every item that is in the collection called X and I'm going to compare it with null, and if it matches, I'll give it to you. But there was nothing there, so I get nothing. So these kind of things that seem like they should work, don't, and it's a common source of problems in scripts. But if I do it the other way, $Null = X, then it actually tells me if the X variable is null or not. So understanding that difference with collections is key because you need to think about are you comparing the content of the collection, or the collection itself, and what you want to do with it. There are a few more examples of that right here that you can run through, more of the same thing, just showing how that works. And I talked about if something returns more than one value and you're trying to---oh yeah, so actually I talked about what particularly figuring out where stuff comes from and I'll show you this example as well. When you run commands in PowerShell, this is especially true in PowerShell V3 and 4 because 5 does some stuff that makes this not so much an issue, but it still is there. If I run a command that I want to have the results stored in an array, I will always, always, always wrap that in array enclosures 100% of the time because it's the only way you're guaranteed it's always going to be an array. If I call Get-VM and there's only one VM, the results come back, I have a single object not in array. In PowerShell V5, I could do .Count and it'll work because they put stuff in to handle that, but not in 3 and I don't think it's in 4. I don't remember when they added that, but just it's worth making sure if you want an array, explicitly wrap in array. It doesn't cost anything. You just wrap it with, so I just do array enclosures and that tells me that I'm getting something and if it's one, if it's many, I want an array. So it's good to know that tip. And some of these are comments that you guys can read over here on your own since I'm running out of time. The only other thing I'll talk about here, yeah so preference variables, so Get-Variable-Name* Preference. Being aware of these is really important when you're debugging to figure out how things, to control the flow of what's going on. ErrorActionPreference, in particular, can be really useful. But with ErrorActionPreference, this is okay, ErrorActionPreference = Stop. That's basically saying in my command, treat any error as a terminating error and stop running immediately or this using PSDefaultParameterValues to do the same thing, but don't do this. People will do this in commands and it's not the right thing to do, ErrorActionPreference = Continue because you're telling PowerShell, I don't care what the error is even though this weird error is happening that I know is okay, so that's why I'm doing this. Any other error out of the sun that comes up is going to go by, unless it's an actual terminating error because of being wrapped inside a try/catch and it'll just continue execution. So don't do that in your scripts. Make sure you're controlling it, either turning it, this is okay, turning errors on a more strict scenario, not the opposite, or use -ErrorAction line by line for the commands that you run.

Advanced Debugging Scenarios and Closing

And the last thing I was going to talk about, but I'm now at the end is some advanced scenarios, which are brand new to DebugPX and you guys can take a look at and ask questions about if you want to dig into them, in particular, to what you were talking about setting that debugger hidden attribute. If it's on code you don't own, but you want to set that, you can do it with this. So I have this Set-CommandDebug mode and I can turn it on for an entire module. So if I'm debugging and calling code that calls other modules that I don't want to debug, I can basically say turn the debugging off for that module, so then I can set breakpoints whether they're command breakpoints or variable breakpoints and steps through only my stuff and take the other stuff as if it's a black box and don't touch it. So it's very handy as well because you're often consuming, especially with PowerShell gallery, you download script modules, you run them, and you don't want to debug those, you want to debug your code that's running those, hopefully, unless they're not very well written modules. So that's all I've got today. If you guys have more questions, I will be around all week. I'm happy to pull, go aside, and talk about some of things in more detail or you can contact me through on my slide deck, which you will receive. This is a link to the modules I talked about and this is my contact information for reaching out to me, so feel free to contact me any time. I'm happy to help.

DSC for VMware vSphere - Outside the Box

Introduction

Okay. Welcome. I'm glad to see there are still people awake after this day of brain-melting sessions, at least for me. I learned quite a lot of stuff. I hope this brings a bit of new stuff or gives you a better insight in what we're going to talk about. I'm probably a bit of the odd one out. Since you saw the title, I'm talking about VMware vSphere. First of all, are there many vSphere users in the audience? Okay, oh wow. Great. As you probably know, I'm quite keen on PowerShell and more specifically, on PowerCLI, which is the snap-in and nowadays nearly all modules, but there's still a few snap-ins left to manage your vSphere environment. Just a minute. Okay, this slide this is a big thank you from the complete PowerShell community and users to the guys who made all of this possible for it's now 10 years ago and without them, we wouldn't be here and we wouldn't have this marvelous tool to automate and manage our environment, so thank you guys. Much appreciated. What am I going to do today here? First of all, it's 4:00, so I'm not going to show you hundreds of lines of codes and I want to avoid the snoring in the room after 5 minutes in, so we'll keep it very light. What I'm trying to go for is DSC, Desired State Configuration. This was something we're all looking at probably. It's very interesting. There are a number of reasons why you would use that and my question, already some time ago was why wouldn't we use this for our VMware vSphere environment? Now unfortunately, at the moment, there aren't any public DSC resources available that will allow you to do that and that's where I'm going to try and help or start at least because this is definitely something that will be so huge once it is usable that it's not a one-person job. So what will happen is I will push some stuff into the community and I hope there will be some participation from the community on setting up the DSC resources. The agenda is very straightforward. I'll just tell you about the long and winding road that's in fact especially winding. Although, I like all the WMF5 previews, I've felt often like one step ahead and two steps backwards, so stuff that you thought was implemented wasn't implemented or didn't exactly work as you expected it to work, but now with RTM, I think we reached the stage where we can start actually using this environment, the 5 version I mean. The 4 was already available long before that. For those, this is a quick introductory stuff. Who is this guy on stage, that's me, look ridiculous. I'm from Belgium, so I had a bit of a trouble to get here, but okay, we got here in the end. I appear from time to time at VMworld. We have this PowerCLI reference for those of you that are busy with vSphere. I suppose you know that book. Second edition just came out last February, so the shameless plug, go, and get it, but okay it's up to you. I'm most of the time, as you can see on the right I put some pictures with my partner in crime, Alan Renouf, which is probably also well known. Who, by the way, is using his vCheck module here? Okay, that's not as much as we normally, but okay great. If you don't know it, you should check it out. It allows you to schedule reports of your complete vSphere environment.

vSphere and PowerCLI

Okay, first of all, okay, I saw that most of you are aware of vSphere and VMware. Just a bit of history on one slide, nothing more than that. PowerCLI was something that they started in 2007. I was very lucky to get on the early beta. At that time, I think they had about 60 cmdlets, in the meantime, the last one that appeared to have over 600 cmdlets. One of my pet peeves is they still have three snap-ins in there. All the rest is converted to modules. And Alan Renouf's excuse, Alan has nothing to do with the remaining snap-ins. That's above his authority, but it will come. They will go module all the way once I don't know when and I'm not working for VMware, so I have no clue about road maps and so on. One thing that is quite important and I was very happy that this thing was in there from the first release that's this Get-View cmdlet. Now Get-View cmdlet, most of you will know it because it is the one that if you're working in larger environments and I mean thousands of VMs and hundreds of hypervisors, you're well off starting to use the Get-View because it performs a lot better than the native to the standard cmdlets Get-VM and Get-VMHosts. But Get-View has another advantage. It allows you to interface or interact with the complete API that vSphere offers you. So that is the one that allows you to go directly to the objects and the API methods that are available under the covers. What PowerCLI is doing, it's doing exactly the same thing. It's packaging these API calls in cmdlets for your easy use and it makes a selection of the properties that it returns to you. There's a lot more available under the cover, but PowerCLI and the development team are trying to do there is this 80/20% they try to cover 80% of your needs by implementing about, yeah what it is it, 20% of the methods in the API, and I think they did a quite good job, okay. There's 600 cmdlets. I think most of the stuff that you do on a day to day basis can be done with the cmdlets, but for this specific environment that we are tackling, or I was tackling the DSC part, I quite soon realized that I had to go to a direct API calls, and hence, the Get-View cmdlet preference. Now to give you just a small example, what you can do with Get-View, if I hope it is a bit readable because I tried to minimize my code and make it large enough. The first screen shot, what you see there is that we extract information for one specific ESXi node and one of the properties that it returns in this case is the number of CPU's in that VM Host. Now with that information, you don't know if it is at one circuit with four cores or four circuits each with one core. But that information is available. It's just not presented through the PowerCLI cmdlet. Now if you get to the API, that's what the second screenshot is doing. It's just a calculated property where we go into the API properties. You can get that information and that's how we know. Okay, this is from a demo environment I was using. It's a four circuit, one core box that you see there, so that kind of information, that extra information you will find by using Get-View and diving deep into the APIs and the properties that it presents. So Get-View is important for many reasons. Another thing, a high level view, I didn't know there were so many VMware users in this audience, but still I will talk briefly about the thing. This is a very high level abstraction of your vSphere environment. The top item on the right, that is the famous PowerCLI icon, and what that does is that uses PowerShell, and through PowerShell, it addresses this API. That's the box that I called VI API. Underneath you have all your vSphere servers. Now one thing that you should remember, the vSphere server can be all vCenter or that bare hypervisor your ESXi node. Both are called a vSphere server and for both you can use the same APIs. That is a big advantage. So you can connect to different environments with the same API methods and find back most of the time all of the same properties. Once you're there, okay, your vCenter and your ESXi nodes they talk with the vKernel, that's where your virtualized hardware is translated into physical hardware and your VMs are running also underneath all these vCenter servers. Just a bit of background. It's important to know what we are talking to and who we are talking with, so it's this VI API. Now why would I consider even looking at Desired State Configuration for my vSphere environment? I put a number of arguments out on that slide. First of all, vSphere is a very complex environment. If you would just look at classic configuration. Classic configuration, you have multiple components, you have high availability DRS, DPM, and so on. Each of these in ten has a lot of parameters that you can configure to control that specific component. So it's a very complex environment and then we fall back into same argumentation why DSC in general is there. It's to monitor these kinds of environments and to correct autocorrect the drift that you would have on such an environment. Somebody makes a mistake, everybody can make a mistake. It's your junior or your senior administrator, they all can make mistakes. So that thing would solve the problem of changes and building changes to your environment. And then the next realization is if you look at DSC, DSC was introduced in 4 already, but it really expanded availability and the possibilities in 5, so PowerShell is there. On top of PowerShell, we have PowerCLI. So everything that we require to start using DSC in vSphere, the toolset is there. So that is already one big step in the good direction. And for me personally, and I think a lot of other people, What is quite important with the DSC, you end up with this configuration files and these configuration files are nothing more than flat text files, which you can put under source control. So you can align your vSphere environment on a specific version, put that under source control GitHub bit bucket, whatever you want. We even use ClearCase for those who still remember what that is. But you can do it and you can see changes between different implementations of your configuration. So that was for me one of the bigger arguments to go for DSC controlled vSphere environment. Okay. The problem is we mentioned those vSphere servers. It could be a vCenter, it could be an ESXi node. VCenter, the early implementation was quite easy that was hosted on the Windows box, but not anymore. You now have the choice to go for an appliance, the VCSA is a Linux box. Secondly, the ESXi node itself is a closed box. You're not supposed to put stuff on there anymore. It's for security reasons, but also for support reasons by VMware. So how would we fix that? We need ultimately we need an LCM agent somewhere. So my idea was to put a box in between and that is what I call this vEngine, short vEng. What we do there is we install PowerCLI on there and we install the DSC agent on there and that is the one that will register itself with the pull server or push server, whatever you want to use in your environment. That is the client for your DSC configuration and that one will be using PowerCLI, hence the icon at the bottom there, to talk with your vCenter, vSphere servers. So you avoid that you have to put anything on the boxes in your vSphere environment. That's the first principle that I started off. So from there out, okay, I recap here quickly the argumentation that most of this already was mentioned. A standalone box rarely use PowerCLI to talk to the VI API, the lifecycle transparency. If you update your vSphere versions, could be vCenter, ESXi, that's transparent from what you do on this vEngine that is independent because ultimately, about the only thing that you have between those two is your DSC configuration. What you and that is something that I try to implement from the beginning, but I had some setbacks, but it's still on my planning. Most of you have probably experienced if you run a PowerCLI script, the first cmdlet that you execute, it takes more time than the second one consecutive cmdlet. That hasn't proved in the latest version, but the reason for that thing is this just-in-time compiler that they use. So there are options where you precompile and you do your engines before you actually start using PowerCLI. And my idea was, PowerShell has an older, big thing that was introduced in 4, if I'm not mistaken, disconnected sessions. What you can do is you can set up a session, a remote session to your local host, in that session, you load your PowerCLI environment, you do the connection to your vSphere server whatever be it an ESXi or a vCenter, and then when you're done, your environment is set up, you could even start a dummy first cmdlet. When that is done, you disconnect the session. Now once your LCM receives the first configuration, it starts, it has to start doing stuff. He has to start executing cmdlets. Now at that point, he just connects to that disconnected session and everything would be already there. You would have your PowerCLI loaded, you would have your connection through your vCenter or vSphere server already present, so you would avoid this over it. Now I had it implemented, I even have a separate module to manage and control such disconnected sessions. Unfortunately, we hit the problem where the first call it works perfectly, the first conflict that you arrive, second conflict is a WMI editor popping up. I'm talking with some Microsoft guys here tomorrow, I think, to look further into that problem, but it's still on my planning. Okay. With PowerShell 5 and the DSC options in there and having lived through all the different previews that we have until we arrived at RTM, I had a few targets that the DSC resources should apply. First of all, it should be class-based. Last year at VMworld in August, I already presented some DSC resources, but they were all skid-based, and first of all, they are slower, and secondly, they still require the handling of this MOF file, which you would divide, so we decided or I decided to go for a class base that was the first thing. The second thing I wanted since there are a number of distinct resources in your VNC environment that you can immediately recognize a datacenter, folders, clusters, and so on, I decided to have a resource module that would be comprised of multiple files. Apparently, there is an issue with that as well. At the moment, you can apparently for Java-based classes only use one file, the root module. You can't use multiple files, multiple pairs and ones this apparently not available at the moment. So I had to merge everything back into one big file. Not a big deal, but hard to maintain, in fact. Then partial configurations was a target I set myself quite early, although, after yesterday and some discussions I had in the corridor here, there seems to be some doubt if that is still a valid concept to apply in your environment. At the moment, I'm continuing with partial configurations. I'm still not 100% convinced that it's always bad, so I think I'll stick with it until the opposite is proven. But okay, I'm open for suggestions. And then the other concept that I wanted to apply, that's this famous what-where separation. How you implement the cluster, that client, that part in your configuration files should be standard filling in the actual values, the name of the cluster, what VMware, what ESXi node should be in there that should be in a separate part, and that is the part that you could ultimately put under your resource control. That is the part that will change between different runs of your DSC configuration.

Test Lab

So these were a few of the concepts. I had to drop some temporarily, I hope. But I started working on this thing and this is just a screenshot to give you an idea of my lab. It's not a home lab anymore. It's sitting in the cloud nowadays, so I can access it here. I can access it at home. It's a cloud lab. The setup is quite simple. I have a very simple Windows environment, one domain controller, one pull server. I have a workstation. I use 10 and 8 to be able to test both flavors of VMF5. And then I have the famous vEngine and I have a very small separate vSphere environment, in this case, it's a VCSA, a couple of ESXi nodes, and some storage. I use FreeNas at the moment because that's the simplest to set up for me. Okay, most of this is, I already mentioned in the previous slides, it was a bumpy road, so like I said I often felt that I took one step forward, two steps backwards, but there was progress. Ultimately, there was progress, especially when RTMF5 RTM became available or available again in February that I could continue actually progressing on my DSC resources. Most of it what I mentioned here is already mentioned on the previous slides. Okay, the virtual, the vEngine box, I started configuring that with the RegistrationKey. I think most of you know, or if you didn't, you heard it in previous sessions today how you can use this RegistrationKey. I think the next slide will show you how it works with your, okay, so you update your Web.config, you point to this RegistrationKey's text, there you put this registration key, so it simplifies your registration, in my opinion, of your machine, of your DSC client with the pull server. This is just background info. For the rest, it doesn't impart your DSC, vSphere DSC resources normally, but that's how I set it up just to make it simple, what I think is simple in this case. Then the partial configurations on the discussions as I said, but what I'm doing there is I tried, and that was one of my arguments to use partial configuration, alright. What we noticed or what I noticed in my environment that the people who manage your storage and the people who manage your network, they start managing your virtual storage and your virtual network nowadays and they aren't necessarily your virtual environment administrator, otherwise, your vSphere administrators. I thought if we pull that apart, it would be easier than the network part how you configure your switches, what settings are on there that would go to your network team, same for the storage part. Same for the virtualization part, the virtual server itself, the cluster and so on that would go to your virtualization administrator or whatever you want to call them in your environment. So hence this separation. This is just a small sample where I have folders and datacenters and clusters in separate part of configurations. And thanks to the DependsOn, we could then determine somewhat what should be there, so you should have a datacenter present before you define a cluster if you're cluster is in a datacenter, of course. So those simple dependencies can easily be configured in this way, so that's what I was playing and still playing with at the moment. This is once you launch this through your vEngine, this is the LCM configuration on that vEngine. Nothing world shattering in there. It's what you saw several times today, I suppose. It's a very straightforward standard thing configuring LCM.

DSC Resource Modules

Then the modules themselves. Remember I said that I wanted to go for class modules, class base resources, so I started in 5, one of the early previews in fact already, I started converting what I had, a scripted resource to a class. Some guidelines that I, but not only for my DSC resources, I use this for all my modules and scripts. I love this region, hash region keyword in your code. It allows you to collapse and expand code, makes it more, you can have a look from a high level on your code, so that's what I did here. You see, for example, for my class folder, C, okay and I know that the C at the beginning is also under discussion. I'm going to review that as well. That one I can understand that is perhaps a bit surpassed and that I should just call it VmwFolder in this case. But what you see is that I have three distinct regions in each class. First of all, we have the properties. We will open it on the next slide. Then you have the standard DSC functions your classical Get, Set, and Test. And then, what I try to do and you will see also that in the next slide is the Get, Set, and Test should be rather simple. So a lot of the functionality I packaged in helper functions. That's the start region. We'll have a closer look at those now. Okay, I hope this is somewhat visible, but just because, okay the first part that's approximately to the line no, it's to the Line 28, that's my attributes, my properties, parameters that I want, no the properties of the class, in this case, for the Forward class. There's a number in there that you absolutely need that ensure the path, the name, and so on. There's the hidden one. Everybody knows what a hidden property is in a class. You won't see it if you use the IntelliSense thing. The intention of that one is remember I wanted to use this corrected sessions. That's the one where the session id would go, in fact. So for the moment, since it's not working, I'm not using it, but that was the intention. Also notice that there are credentials in there and I know that we should be at a good session earlier, no first session this afternoon on the encrypting your complete MOF. This is definitely something that I should be looking at. I haven't done it yet, but we should start encrypting this stuff because there's quite a lot of security in there. You have your connectivity to your vSphere server, which you don't want to be seen in public or on the network where everybody can snoop. Then the next region is like I said, the Set, Test, and Get functions. They are quite simple in this case. In fact, they only contain the functionality to return the correct result, if they return the result at all. And then you have a number of helper functions at the bottom of the start region that I used. Okay, this is the part where I will set back, quite recently in fact, I had the multiple psm1 files that didn't work out, so I had to go back and start using the RootModule in the manifest and put everything, all my resources, all my classes in one psm1 file unfortunately. I hope this is a temporary issue and that it will get fixed at some point because for source control reasons, it's a lot easier if I have it in separate files if I work on the folder class. Now I have to import and commit everything again, then I would just be able to do it on the part that I'm actually working with. So if you start using it or if you start writing it, remember at the moment you can only do it in one psm1 file unfortunately.

vSphere Tree Nodes

Now one thing that gave me a bit of issues from the very early start is the, if you look at your vSphere environment, there's an inventory tree, which comprises datacenters, folders, clusters, resource pools, and so on. This complete hierarchy was conceived historical, so that means some stuff was edited after the initial concept. So there are some things in there that are not that logical or that not that obvious how you would reach. For example, look where you can define a folder. You can define a folder, a yellow folder in this case, you can define a yellow folder on the root of your vCenter environment, you can define it on the datacenter, yeah, on the date and you can go on like that. Blue folders, on the other hand, which can contain only your virtual machines, you can only define in your VM and template infrastructure. So there's a bit of complexity in there. And before I forget, and on top of that, there are a number of hidden folders that you don't even see in your WebClient or your vSphere client thinking of datacenters, VMHost, all these folders will pop-up if you do a Get-Folder, but you won't see them in your hierarchy in the clients. Now one question I had to answer. If you want to create a new resource, you have to give about where it should create this new resource and the parameter in the classes, of course, obvious that's the part one, but the questions I had to ask there and answer more particularly is will I include the hidden folders. Am I going to indicate that home lab is a datacenter in this case or am I just going to give the name of the part and let my function be intelligent enough to know that it should be a datacenter or look. If it is a datacenter, is there a possibility that I have a folder with the same name? So in the end, I succeeded in creating a function that can find whatever node in your vSphere hierarchy tree just based on a path like that. First, let me show you. I made a summary and this is of purpose complex because that's the reality of a vSphere environment. If you look at all the objects in there and it starts at the top with the vCenter, datacenter, and so on, but also all the connectivity in there, a one-to-end, end-to-end, folder can be there and there. This is where all the complex, complex means hard to automate, hard to program in a script, so this is what I used as a starting point to create that all-purpose find my node function that I included in the resources. And this is the function, in fact. It uses some internal helper functions recursively, so I will publish this. I'm not going to run through all these lines. I know it's 4:00. I'm the one sitting between you and your dinner in half an hour, so I'm not going to torture you any further. But be aware that these are all the complex function that can handle all these node retrievals that you need, so it can resolve all these parts that you specify in your DSC resource. And there's one other thing that is quite important and this was a very helpful discovery feature that I used. What it does, in fact, when it finds the node or the leaf for you in your path, it should return normally on an object or a point and the point in vSphere as some of you may know is called a MoRef, Managed object Reference, which is a specific object type that comes with PowerCLI. If I was going to use that in my basic Get, Set, and Test functions, I would have to be including statements with using because they need to know how this object looks. Now the big thing is that you can convert such a managed object reference to a string and the second big thing is you can use that string on the Get-View cmdlet to retrieve the object. So there's no need whatsoever to use a Manage Object Reference object in your basic class functions like Get, Set, and Test. So this was a very lucky feature that I could use to my advantage. And that's the example, so this is from the VMHost resource. This is a helper function. What we're doing is we had the path and the definition in the configuration file. He passes this path to this Find-VmwLeaf function. He comes back with a node, if he finds a node, I need to retrieve the object node as a point. If he finds a node, he needs to retrieve the object, and as you saw in the previous slides, this is a string, but thank God Get-View with id can use a string instead of an actual managed object reference object. So that was, I was quite happy that was possible. Otherwise, it would have become a bit more obnoxious to code on.

API Methods and Clusters

Another thing that I started using quite early is I dropped all use of the PowerCLI cmdlets and I think I mentioned it earlier I went immediately for the API methods. Okay, API methods. Who of you is using the Onyx WebClients or has heard of the Onyx WebClients? So there's a feature where you can see all these API calls underneath the PowerCLI cmdlets. It's a handy feature if you want to write some advanced functions and don't know what they are actually calling. So that is the kind of stuff that you will see appearing if you start using this Onyx WebClient, so you see over the deep down APIs and properties. In this case, what I'm doing here I'm just adding a VMHost and ESXi node to, or standalone server, or one to a cluster, instead the if statement at the end. So based on what leaf type was returned, if it's a cluster that was returned, we can use the AddHost method if it's not a cluster by definition it a standalone server, then we need to use this other method with somewhat different parameters. So a lot of the resources that you will see when I make this public, all of them, in fact, are real API calls and API objects that I'm using. And the reason I'm doing that is like I set this for speed, but also to have more features. But the API methods, I can do everything. There's a lot, there's some stuff with them, but the cmdlets that is not possible. What I also used as a guideline and I think I mentioned an example earlier, if you look at the cluster in the VMware environment, the cluster has multiple components. We have high availability, DRS, DPM. All of these in turn have a lot of parameters that you can use to configure these components. On the other hand, if you got to the WebClient or the vSphere client, you can just right-click on the datacenter new cluster and it doesn't ask you a lot of questions. It just asks you, do you want HA enabled, DRS enabled, DPM. That's it. So what I decided quite early on in writing these classes is I would go for the same approach. If you define a resource, a new cluster in this case, no not in this case, but if you would define a new cluster, you would get all the defaults. And a configuration of HA or DRS or DPM would be a separate resource, but the dependency on the cluster, of course, but I would separate the complexity that way, so that's why I called this slide, keep it simple. So I'd rather work with all these different resources with dependencies in between them, than making one big resource with tens or hundreds of parameters, which is quite complex to code and manage. So where am I now? At the moment, I have a number of resources that are tested and available. Those are the ones that I will publish when I'm back home from this event. It's the standard stuff, folder, datacenter, we have cluster, we have VMHost in there, we have datastore in there. There's one component missing that you would need if you want to actually start using it, meaning start creating VMs as your network part. VSS standard switches is quite obvious. This one is under the development. Distributed switches is a bit further away in the future, I'm afraid. That is quite complex object, but it's on my to do list. So what I'm working on at the moment is the networking part. Like I said, I will publish this quite shortly after I'm back and since the environment is quite complex, I can't do that all by myself, so what I want to do is publish this, give you an idea how it can be done, so there would, by definition, need to be a lot of community effort to make this complete, to be able to configure your complete vSphere environment based on DSC resources in this case. Like I said also in the beginning, I'm not going to show you a lot of code. I have my ISE editor open. If some of you want to see some of the code or have specific questions, feel free to come up after this session or anywhere in the next coming days, I'm happy to run through it and discuss with you what even, even though I prefer to listen to your suggestions on how some of this stuff should be done. Quick question here. If you have available vSphere DSC resources, how many would consider actually using that? And is that no problem with your environment that they say this is external or we don't trust it. Would you have to do a validation? Would you prefer or would you require passive built-in tests? Yeah. If a structure testing. So something that made available if it comes with infrastructure testing that would be more acceptable than just the resources. Yes. Okay, great. So that's new point on my to do list then or anybody else who wants to volunteer for that part. In fact, that's what I wanted to say in this presentation. There's another one on Wednesday where I will start looking or when I will look at the reverse part because a lot of you will have existing vSphere environments. Now how do you get from an existing vSphere environment to a configuration file? That is the interesting part. You're not going to write, I'm not going to write them from scratch, so that's for Wednesday. And in fact, this brings me to the end of my session for today.

Q&A

Are there any questions? Yep. (Audience question) Sorry, can you? Have you gotten some extra VMs themselves, CPUs, memory VA, just substantial things like that? If I write the resource for the VM, yeah, that will be. Have you got there yet? No, that's what I said. I first need to network before I can actually create a VM. I know and I think Brandon is in the audience, he has a resource where you can create a VM. I wanted to start from the bottom-up, he started from top-down, so that's, but I hope that we can reuse some of his code in this environment. Anymore questions? Okay, yeah. Can you see this getting rid of the need for profiles completely? Yeah, I know what the question is. I got this. Does this eliminate the need for host profiles? We got that exact same question at the VMworld where the question was more painful than when I have it here, of course. In my opinion, and since I'm not working for VMware, yes this will eliminate your need for host profiles because this will do a lot more. But my good mate who works for VMware, he will tell you, ah no, not completely, you still will need host profiles. (Audience comment) He's convinced of that, but he can't say that you don't need host profiles. So yes, I think if this is working and it is working 100%, you could because with host profiles it's a passive thing. If we get this working, that's an active thing. That's why I would like this monitoring correct. I don't need to suspect a drift and then take action. I want this stuff to discover the drift and correct it. For the correcting mechanism, would you be putting host in a maintenance mode, moving VMs? That could be one of the options here. In fact, what I'm looking at is just writing the resources that would give you that possibility. It would still be you who writes the configuration files. Yeah. Who decides how far this correction of drift can go. Yep. Anybody else? Okay. I thank you. I know this is the last session of the day and my brain was melted with the previous one, so I guess some of you would also be very tired now. Okay, thanks for coming and enjoy.

The Future of PowerShell ISE and Editor Services

Introduction

Alright. So good morning everyone. It's great to see you all here today. I don't actually have anything planned to talk about today, so I figured we could all just go over to Kirk Munro's session instead. Just kidding. So anyway, I'm excited to be here at the summit to spend some time talking to all of you about the cool things that are going on to the whelm of PowerShell operating tools. For those of you who don't know me, I'm David Wilson, and I'm a member of the PowerShell team, and I'm really fortunate to get to work on cool projects like the PowerShell ISE, PowerShell for Visual Studio code, and PowerShell Editor Services. And in my time working on the projects, I've been able to talk with many members of the PowerShell community, so it's really awesome that I get to meet many of you in person this week. If you see me around the summit in the next few days, please feel free to come by and chat. So my talk today is titled The Future of PowerShell ISE and Editor Services. So my goal with this talk is to show how the future of PowerShell ISE will be defined by the integration of PowerShell Editor Services and how this will improve the larger of PowerShell authoring ecosystem. So let's start with an overview of the current landscape of PowerShell editors just to show how our work fits into the bigger picture. So for many years, PowerShell, I'm sorry, Microsoft has been focused on a single editor for PowerShell, the PowerShell ISE. The ISE is considered the standard PowerShell editor for many people since it comes in box Windows and typically supports all the latest and greatest PowerShell features as they appear. It's simple and understated, but it gets the job down. Its add-on model has allowed for some nice extensions and customizations and integrations with other Microsoft services like Azure agnation and PowerShell gallery. It's also been extended far past its original feature set by the ISE Steroids add-on, which a lot of people seem to like. Tobias Weltner has done a great job of turning the ISE into a much more powerful editor than what it was before for those who needed it. So even though the PowerShell ISE has been serving people well for many years, a new opportunity appeared last year that we wanted to sort of investigate and that is Visual Studio code. So Microsoft announced Visual Studio code during build 2015 and spent a lot of time last year polishing it and improving it to something that's really useful. Its biggest benefit over Visual Studio is that it manages to provide the same level of functionality or close to the same level of functionality without all the extra bloat that Visual Studio has. So around the middle of last year, we were alerted to the development of a new extension model that would allow developers to plug in new language implementations and functionality like IntelliSense, code navigations, linting, and debugging support and we decided that it would be cool to investigate to see what it would take to actually plug in PowerShell to Visual Studio code. It turns out it wasn't that hard to provide a pretty decent level of PowerShell support. So in November of last year, we released the PowerShell extension for Visual Studio code during the announcement where the Visual Studio team, Visual Studio code team announced their extensibility model, and since then, we've had over 30,000 downloads of the extension and four updates with new features and a wide variety of great feedback in the community. So this effort also spawned another project called PowerShell Editor Services, which allows us to provide the same level of capability to pretty much any other editor, but we'll talk about that later. So, we also have Visual Studio and people were asking for a long time why Microsoft didn't actually provide PowerShell support directly in Visual Studio. Well they didn't really need to ask for long because Adam Driscoll came along and made poshtools in Visual Studio and it filled a real need for really good PowerShell support in Visual Studio as demonstrated by the 340,000 downloads as received, at least for the 2015 version, and the utility of this extension was so obvious that the Visual Studio team decided to include it as part of the Windows, I'm sorry, as part of the Visual Studio 2015 installer, so that was pretty cool. Now for those who need something a little different than what Microsoft provides are our third-party editor's store is still pretty good. There's the PowerShell studio editor from SAPIEN technologies. I think June's somewhere back here. It's a really nice tool that seems to go beyond just in general development experience to user interface development, packaging, and deployment and been hearing really good things about it on Twitter and I haven't really had much of a chance to play with it myself, but I'm sure that June would be happy to tell you more of the cool things it does. There's also PowerGUI, which was originally developed by a company called Quest and then acquired by Dell and I think that Kirk and Adam had worked on this at some point in the past. I've heard good things about it, but I don't really see much activity of risk development these days. And we also have the wild and crazy world of open-source code editors. So Sublime Text and Atom are both really popular for developers who primarily use open source technologies and platforms. Their strings lie in their modern style and white ecosystem of packages that give support for various different tools and technologies, pretty much anything that you can think of. So for PowerShell, both of these editors provide packages that contain syntax highlighting, snippets, and some really rudimentary IntelliSense, but no real integration with the PowerShell runtime. And last year, I spent a little bit of time, maybe a couple days trying to get PowerShell Editor Services integrated into Sublime Text to start to get some of these features online, but I still haven't got it fully wired up yet, but hopefully at some point later this year, I'll get a chance to go back to these editors and wire them up. However, over the last few months, I've noticed that a lot of people are starting to migrate towards Visual Studio code, so that may sort of affect the prioritization of the work to wire up these editors. So there's also Emacs and Vim and the common stereotype for Emacs and Vim users is that they're old school UNIX and MIMS with scruffy beards, long funny tails, and stale t-shirts, but the reality is that both of the editors are still widely-used today and there's a sort of resurgence in their popularity based on some new tools for them called Space Max and Neovim. Both of these editors have packages that provide basic PowerShell editing support, similarly to Sublime Text and Atom, but they also lack the richer editing features like IntelliSense, etc. And they also have the same sort of extensibility models that would allow those things to get added later. So since I'm actually a fan of Emacs, I want to go and try to get that thing hooked up with better PowerShell support sometime maybe later this year or next year. I know there's a couple of people on the PowerShell team that are active Vim users, so I wouldn't be surprised to see some activity there as well. So at this point, you're probably wondering why am I mentioning all of these editors. Well it shows that there's value in having different options for different needs, basically. So some people prefer the focus experience of PowerShell ISE, while others need a more complete development experience like Visual Studio, which supports other languages as well. Some people prefer the minimalistic editors like Vim or maybe modern highly-customizable editors like Atom. The important thing to understand is that for each of the editors provide a standard PowerShell development experience. They have to implement a fairly large amount of common code. So what we'd like to do is make it easier to provide the standard experience across most of these editors so that each developer can work in the environment that's most comfortable to them that works better for their workflow. So a little later in this talk I'll describe how we'll be able to accomplish such a standard experience and feature sets so that developers have a wide variety of tools to choose from.

PowerShell ISE Preview

So since this presentation is supposed to be about the future of PowerShell ISE, let's talk about the ISE preview. We made an announcement a couple months ago that we had shipped an alternative version of the ISE via the PowerShell gallery, which gives us the opportunity to try out some new features and new ideas while keeping the built-in ISE stable. Our intention is to ship updates with new features somewhere between a monthly and a bi-monthly cadence. Having said that, you may have notice that it's been a little while since the first release. I think that we released it in February and it's still lacking another release since then, but that's for a good reason though. I've actually been working over the last few weeks to put together a new release that addresses some of the most highly uploaded feedback on UserVoice and also a new feature that I think all of you are going to like. So instead of just showing you a list of what these new features would be, why don't I just give you a quick demo to kind of show it that way. Let's see if I can pull that up. Alright, so one of the first things you may notice that's a little bit different is that there's this little icon here in the lower left-hand corner. So now we have the ability when the ISE preview starts up to do a scan for any PowerShell gallery modules that you may have installed and to check if there's any updates for those because currently there's not a really good way to see which gallery modules you have that may have updates that you haven't installed yet, so I decided to put a little piece of UI here that will give you the information for this. Alright, so when I click the icon, there's a new window that appears that gives me a list of the modules that currently have updates and we can see here that the Pscx module needs to be updated. Of course, this was released in 2015 so it's sort of contrive example. Also, there's PSScriptAnalyzer, which has been updated to version 1.5. So you can see that we're actually gathering the release notes and the version information from the gallery information. You can see that sort of all the details are here. Also, you can select whether you want all updates or just uncheck various updates that you're not interested in. And another interesting thing here is that the Pscx module actually was installed system wide. It wasn't installed for just a current user. So I'm not actually running the ISE preview elevator right now, so it's sort of interesting to see what happens whenever we do this. So I'm going to go ahead and just click Update to update these modules. So since Pscx needs admin level credentials, we actually see an error that sort of shows you. It's basically just an error that comes directly from PowerShell from the cmdlet. And also, you'll see that there's a prompt for the untrusted repository, but I think that should change soon. But now since it's all in PSScriptAnalyzer and if I had other modules that needed to be updated, they would all be installed as well, so pretty easy. Just install like all your updated modules. It's nice for people who actually are module developers so that whenever you push a new version of your module, people who are using the ISE will actually be able to install it, they'll know that it exists. So since this functionality starts automatically with ISE, some people may not want to have like the initial performance hit of having this sort of module scan and update scan happening, so we've actually added a new option in the Options dialog, in the General Settings tab, so you can actually uncheck the Check for module updates automatically option. So that will stop it from happening at start up. And if you ever want to invoke the process, you can go to the Help menu and click Check for updates so that will kick off the update process. So that's sort of the big new feature. There's another thing that we're doing that sort of sets the stage for future work on the PowerShell Editor Services integration and that is the new experimental PowerShell tab. So if you click the File menu, you'll see that there is a new item for an Experimental PowerShell tab, and right now if you open it, there's really nothing different. It's the same functionality that we currently have, but the point is to have the separate tab so that we can start to gradually do the integration with PowerShell Editor Services over time and not disrupt the core PowerShell ISE experience. This also helps people like Tobias Weltner who have ISE Steroids and he sort of reached into the functionality of the ISE a little too much and if I do anything drastic with the existing PowerShell functionality, it's going to break all the ISE Steroids users, so don't really want to do that. But in the future, you'll be able to try out new features that come from PowerShell Editor Services in this type of tab, so as those things get developed, they'll be talking about it on Twitter probably and let people know that new things are available there. Alright, so there's one UserVoice issue, I think it was filed by Bartech and it was sort of an annoyance for people and a lot of people expressed some interest in getting this removed or getting this changed. Let me see if I can pull up the inbox ISE. So right now, if you were to type something like Get-Process, whoops, and you see there's a lot of output and you want to scroll up in the output pane and sort of see what's going on there then maybe you go into the script editor and mess around a little bit, then you come back and click in the output pane because you want to copy something, but it jumps down to the end, which is a little bit annoying. So now I've removed that behavior. So if we type Get-Process here in ISE preview, you'll see the same output a little bit slower because I'm running it in a debugger. And if I scroll up here and click inside of the script editor and mess around then come back, it doesn't actually jump to the end. So there probably needs to be a little more polish on this to kind of deal with a couple cases where maybe you do want to jump to the end or maybe if you click in here, you may want to like hit a button and have it jump to the end, so there's a little bit more polishing that needs to be done. But really what I'd like to do is just sort of ship this and see what they've been think and just get some feedback from the community and see what people want it to do. So that's kind of helpful, a little less annoying now. Also just in general, productivity improvements, for instance, when people want to like comment multiple lines of code out, what they've been told in the past is oh just use multiple cursives, and that's not really the ideal way to do this because most editors give you a way to select multiple lines and comment them out, so I've been working on a feature that allows you to just do that. So if you select a few lines of code and hit Ctrl+K, it just comments all of them out and also Ctrl+Shift+K will uncomment them, but you may notice that the indentations a little bit different now. I need to work on this a little bit, but I think that this is sort of a Windows Core features for an editor that's really necessary that just wasn't there before and I think the reason why it wasn't implemented before is there's little edge cases here and there like detecting whether there are spaces between the comment character, etc. that maybe it just wasn't seen as worth the effort, but I think it's worth the effort, so we're going to try to finish that and get it shipped in the next update. Also, another big problem is if you have a file open in the ISE preview and for whatever reason, the file changes on the file system outside of the ISE, there's no notification. It doesn't tell you that it's been changed, so you might start editing your file here again and save it and then overwrite whatever the external changes were. So now I've tried to get the .NET Framework's file system watcher hooked up to solve this problem, so let's see if it works and doesn't crash and burn. So actually, I'll just go ahead and open this file in Notepad, the same file in Notepad, and let's see. So I'm going to go and just make a change here in this file and you'll, oh, let me just undo whatever I did here. So this file is unsaved at the moment, sorry, it's saved at the moment, so if I go ahead and just add some text here, save it, then you'll see that it, okay, this time it didn't work, but it crashed. So this was a good time to tell you that .NET's file system watcher is a little unreliable and this crash may be my fault. But the point is that when this first comes out, it's actually probably going to be a little flaky, so we're going to have to do a little work to see what we can do to make it a little bit more reliable. So actually, I'll give it one more shot and see if it works, and if it doesn't, we'll just move on. (Running) It happened again. Well at least you can't edit it after it… Yeah, yeah. That's true. Actually, maybe I'll just leave it like this. Yeah, it's a feature, so we'll just restart the session. It's great. We'll just reload the files, no problem. That's right. You guys should all be my marketing team. I'll just let you guys explain all the problems that we have. That's great. Alright, so let me---

ISE Preview Priorities

So let's talk about ISE preview priorities if I can get back to where my presentation was. There we are. So one of the first things I want to do as you may have seen is address as much high value UserVoice feedback as I can. I've started this work, but there's still a lot more work to be done. I mean there's a lot of issues that have been voted up on UserVoice and some of them are definitely possible, others not as possible. I mean there's probably not going to be the likelihood that we're going to change sort of the UI model of the ISE where you can sort of have Docking tabs and stuff just because it's a lot of extra work. I think that ISE Steroids may solve that problem already. But just anything we can do that will be a general productivity improvement or something that the ISE is lacking that other editors just sort of have by default, definitely file feedback for that. What about little things like maybe let's say like where you draw your bracket or whatever and you can't enter it. You know what I mean? It goes like double does the line indents for you. Is it going to be like that stuff? We can definitely look into it. I don't really know how much effort it would take, but definitely worth looking into. So I don't know if there's already a UserVoice item for that, but if someone files it, we'll definitely look into it. (Audience comment) Yeah, definitely that. (Audience comment) Yeah, pretty much every editor has the sort of automatic indentation functionality, so this is something that is a little annoying whenever you go into the ISE and it just doesn't have things like this. So basically, my plan is to work on the highest value improvements that will address long-term pain points and the goal really isn't to make the ISE at the same level as other editors that exist because you can just use the other editors instead, but it's just we want to make this like really solid core editor with a really good extensibility so that the community can come and add more features as necessary. So also, for new feature development in the ISE preview and the full ISE, I'd like to focus on what we can add through the ISE's add-on model to enable new features to be added using our add-on APIs rather than being added into the ISE code base. This may involve just some tadpole improvements here and there to our existing extensibility points, but the goal would be just sort of enable new things that people can do to extend the life of the ISE by adding new functionality. This is in the area that I'd really appreciate getting some feedback on UserVoice about what new extensibility points would be helpful. I'm looking for a high value, low cost improvements that will enable new types of extensibility in the ISE. There's also a new extensibility model that's coming, which is complimentary to the ISE's add-on model, but I'll discuss that a little bit later. So the biggest priority for the ISE is a gradual overhaul that brings in PowerShell Editor Services as their core implementation for all this editor features. This integration will be rolled out over time in a way that should minimize disruption to existing scenarios while allowing everyone to try out the new features and provide feedback. So generally speaking, what this means is that ISE, sorry, PowerShell Editor Services will basically provide the host environment for PowerShell so that all the things like IntelliSense and code navigations, etc, will be coming from there, rather than from the existing code base in the ISE and this will help in some ways because currently we've got some sort of problems with the ISE code base where you might see things like when it crashes or hangs and stuff like that really shouldn't be happening and it's due to sort of a bad throating design, etc, but we can get away from a lot of those problems by shifting to a different core and sort of refactoring the code that's already there. So as I mentioned earlier, the ISE preview release contains an experimental PowerShell tab, which will be the basis for this conversion and this new tab we use editor services to basically provide all these features. And since editor services currently doesn't support as many features as the PowerShell ISE does, we'll have to continue to improve that support over time before we actually make the Experimental tab the default. So remote session is just one thing. We don't really support remote sessions yet. There may be some other little things here and there that I haven't thought about, so we'll have to figure those things out sort of as the migration happens. But once we make the Experimental tab actually the default tab, then we'll keep the old tab, the legacy tab around just to have it there in case anyone hits an edge case in the behavior of the new implementation so that it can use the old style tab for whatever it is they need to do. And ultimately, these improvements should be integrated back into the build of the ISE that we ship with Windows, but we still haven't decided when and how we want to accomplish this since shipping in box of Windows requires some extra work around quality, localization, and other types of work. But the current plan is to try integrating new stable features from the preview back into the built-in ISE sometime after the next Windows Server and client versions have been released, so probably later this year sometime, way later this year. We want to make sure that whatever we end up shipping is of high quality and has been put through as paces by the community just to make sure that it's not going to be a problem when it's actually in box and it needs to be shipped, I mean sorry, it needs to be supported for years and years.

PowerShell Editor Services

So by now, you might have realized that I've mentioned PowerShell Editor Services a few times in this talk, but I haven't actually explained what it is. So PowerShell Editor Services is a platform that can be used to provide a rich, PowerShell development experience in any sufficiently extensible editor. The benefits lie in four distinct areas, language intelligence features, interactive console experience, debugging support, and other platform features, so I'll discuss all of those individually. But first, it's good to sort of run through the set of common features that any PowerShell editor will need to implement. First of all, it's going to need a host implementation, which implements PSHost, PSHostUserInterface, and some and not of PSRawHostUserInterface and for those of you who don't actually know what those things are, it's probably better that you don't know, but these allow the host application to expose a PowerShell runtime through the host application's user interface and they aren't super difficult to implement, but it's a lot of common code that's just reimplemented everywhere. Basically, any time you want to implement a host, you have to implement these things and it's kind of a little bit of a waste. So when you start implementing a PowerShell host, it does more than just execute PS commands, you'll quickly realize that you have to change the way you execute commands based on the state of the session. Running a command in a local session is different than running a command in a remote session and running a command while stopping a debugger is also different than running a command just in a normal session. So this behavior isn't always straightforward to understand or figure out and at some point any PowerShell host out there is going to have to learn how to do these things if they want to have more advanced support in their host implementation. Another critical element of a good PowerShell development experience is interactive console support. So one of the biggest reasons why people tell me they'd still rather use the ISE instead of VS code is that the ISE has an interactive console that's always available. It's a really helpful feature if you're sort of really quickly developing script. Implementing in such a console isn't difficult, but it requires rewriting a lot of common logic all over again, for instance, like invoking the prompt function to get someone's custom prompt or dealing with formatting of error messages, just all this stuff that sort of has been done many times. More capable editors may want to provide debugger integration, which requires a fair amount of work for walking the call stack, getting information about variables that are in scope for each level of the call stack, and even managing the pipeline whenever you want to execute commands while the debugger stopped at a breakpoint. So generally, the point I'm trying to make here is that any editor integration for PowerShell is going to need implementations of all of these items and it'd be great if we could use what we've learned in implementing these things for the ISE and other editors so that we can make this logic available to everyone else and we just so have created a platform that with the intention of solving this problem. So let's talk about the features that it provides. So for language features, one of the primary benefits is improved language feature support. First of all, being the IntelliSense support, which is kind of like the core thing you need for having a pretty good PowerShell development experience in editor. It's the same IntelliSense that the ISE gets because we're using the same completions engine that comes from the PowerShell runtime, but we also had a new area of features called code navigations and these allow you to do things like find a definition of a function or variable, find all references of a function or variable or cmdlet, and also search across all the names of symbols in your workspace or the current file, which is kind of useful in VS code. These features try to be pretty helpful whenever you're trying to navigate around a piece of code that you haven't ever seen before or maybe a module, so they're kind of a value add that the ISE currently doesn't have. We also provide rudimentary symbol renaming support meaning that you can rename a cmdlet or variable and we'll also change all the places where that cmdlet or variable name are used, so that's also pretty helpful. So I know the really cool feature that we have is that we've hooked up PowerShell script analyzer to provide real-time, rule-based script analysis in the analyzer scripts rising type, so as you're writing your script, you'll see little squiggle markers that say that maybe you're violating some best practice. So basically, you're able to select which rules you want to apply to your scripts while you're editing them and then you'll see all the stuff marked up in your file. In the near future, we'll also be able to provide code corrections, which are based on the rules that you've sort of violated so if you have a squiggle marker under something, you may be able to click it and see a little light bulb pops up to actually get suggestions on how to fix it and then if you click the fix, it will fix it automatically for you. If that's sounds interesting, I'm actually going to give a demo of an early version of this feature at the lightning demos tomorrow afternoon, so be sure to see that. So once we make progress integrating editor services into the ISE, I'm going to try to wire up as many of these language features as possible because the ISE doesn't have a lot of these things, so we'll see if we can get some of these things actually built into the ISE. So editor services interactive console experience is getting better with each release, even though it isn't immediately visible within VS code. We're able to do the interactive script execution and have a console experience, which supports custom prompt functions just like the ISE. It even provides a choice and input prompt support even through the interactive console or natively through the editor's UI, so in VS code if you run a command that causes a choice prompt to appear, you actually will see a little drop-down that gives you the options in VS code's command pallet rather than get to showing up in the command console, so this can be helpful for like nicer integration with the editor for various reasons like if you have an extension or something. It's also if you type in a command that you haven't filled in all the mandatory parameters for, you need to be able to get prompted for those things, so we also the input prompt support where this will show up in the UI. And I also just finished adding a $profile support, which should show up in the next release of the VS code extension. For those of you who aren't familiar with profile scripts, they are used to customize your PowerShell session in a given host like the ISE or VS code. You can do things like load modules, create commonly-used functions, set your preferred command aliases, define your custom prompt function, stuff like that. This feature is really the most helpful whenever you are using interactive console from within an editor, so just to make it a little more customized for your purposes. We're only lacking a couple of important features at this point, but I intend to have them done before we hit version 1.0 a little later on this year. The first is the ability to accept credentials securely and that's only missing because we haven't yet found a secure way to securely transmit the credentials you type from the editor to the language service. So since we had this model where the editor is talking to the language service through some channel, we want to make sure that once you type in the credentials in the editor that they're secured in the editor, as well as in across the channel to get to the language service. This is a little bit difficult, especially in VS code because it's a web-based application and if anyone is sufficiently intelligent or crafty enough, they could probably intercept the things that you're typing in, but I think that's just a general problem anyway, but that's sort of why that's missing at this point. The other thing is support for remote in nested sessions. This is something that we just need to have time to do. It's not very hard to achieve, so probably within the next two to three months you'll see remote session support in the ISE so that you can connect to like a Nano server for script editing, debugging, that sort of thing. We also provide a pretty rich, local script debugging experience. You can set breakpoints on lines, commands, and arbitrary expressions. You can also inspect the variables at each level of the call stack. Our interactive console experience also extends to the debugger in which it allows you to run commands while you're stopped at a breakpoint, so if you want to run some of the PowerShell's debugging or if you want to sort of mess around with the values of variables in the running script, you can do that. Keith Hill has done a ton of work to improve the debugging experience that we're able to provide, so if you've never tried out the debugging experience in VS code, you should give it a shot and then tell Keith thank you whenever it blows your mind because it's pretty cool. So there are some other nice features that the platform provides and we designed them in a way that allows you to only use the ones which you care about if you're trying to integrate this with an editor. First of all, our APIs are designed in a layered model where the core functionality is exposed through a set of classes, which are useable at any .NET applications. So basically, you can just pull a NuGet package into your app, and I don't know, 20 lines of code and you're set up and you're able to run commands and use some of these nice editor features. The next layer above that provides a way to host these APIs in a process, which communicate with non-.NET applications through any channel. For now, this host process speaks Visual Studio codes, JSON-based language, and debug server protocols and it does it over standard input/standard output, so it basically VS code launches the process and just connects to the console in/console out and we're basically communicating with JSON messages back and forth that way. But we can also plug in different protocols, but it doesn't necessarily have to be a JSON protocol, it can be something else and I might experiment with this a little bit in the future. This also implies that the API can be exposed both to other processes on the same machine or through a network-based channel like HTTP. Adam Driscoll has helped get the ball rolling on providing a WebSocket support to make the JSON protocol available as a web service that could be used either on the same machine or possibly a remote machine. Once we have a chance to flesh out the support and add the appropriate WebSocket client in VS code, then it would allow the users of code editors like VS code on OS 10 and Linux to edit their code from OS 10, but have all the language features coming from a host app that's running within a Windows VM or potentially like a service running in a cloud, so we'd give you the ability to do PowerShell development with full features on those OSs without actually running PowerShell locally on the OS since we don't have that. But there are a lot of possibilities here, so there's a lot of different ways to integrate this into other things, so definitely see interesting things we could do. Another useful feature is PowerShell gallery integration. This allows any editor to interact with the PowerShell gallery to list, install, and manage PowerShell modules from any gallery repository, so not just the standard PS gallery. Doug Fink helped to get the support started and I'm going to leverage the new code that you saw in the ISE preview for doing module updates just sort of put that together into the editor services to provide like a standard service for interacting with the PowerShell gallery. And lastly, we'll be providing a new extensibility model, which will enable editor commands to be written in PowerShell and to be used across all the editors that leverage editor services. You'll be able to automate things in the editor like the editing text or launching actions in the editor or maybe changing views. We sort of have to figure out what the common surface area is across different editors, but this might be a pretty interesting way for people to use their existing PowerShell code from their modules to provide new editor functionality, so that's a pretty interesting thing. So this feature is actually currently in development, but it's in like its very early stages, so I've got a short demo that I'll show you today about to give you an idea of what is possible with this.

Cross-editor Extensibility Model Demo

So I'm going to show you how this looks in Visual Studio code. So let me see if I can make this look bigger. (Clicking) That's too big. Anyway, so what you see here is a file that I have open and I have a function I've defined, which does just really simple thing, it just writes output, and you see this cmdlet here this Register-EditorExtension and basically this command is running your PowerShell session and it says I want to register a new command in editor with this display name of My Command with function and also this Id, which is useful maybe to identify the command from other areas. Also, you were saying basically that there's a function called Invoke-MyCommand, which is this, which should be invoked whenever the command is launched. So to access these commands, I'll go to the Show additional commands from PowerShell modules option. And if I run that right now, it tells me that no extension commands have been loaded to the current session, which is what we expect. So what I'll do is I will select this code and I'll run it with F8. You'll see in the output console that I've actually ran the code and the command has been registered. And now, if I go back to that same additional commands list, you'll see that My command with function now shows up. So if I run that, you'll see that the right output, whoops, the right output is actually called and basically just invoke my function directly and it's kind of nice because you can actually edit these things in real-time and not have to like restart your editor to be able to see the updated functionality. So now we can also do the same thing with ScriptBlock. So if I have another command registered here or able to be registered, which actually just takes a ScriptBlock and writes an output as well. I've run that one. And now let's go to the additional commands list and now we see My command with script block is here, so run that, same output. So that's pretty basic, not really helpful. You can do things like run Get-process or something, but yeah, maybe you could launch your build scripts or maybe a deployment script. That can be kind of helpful, but the real helpful stuff comes when you start actually manipulating the editor in real time. So to define this function called Invoke-MyEdit and it accepts this context variable and this is a variable that comes from PowerShell Editor Services and it gives you information about the current context of the editor at the time the command was invoked. So we have things like the current file and we have information about like what the current user selection is and where the cursor location is in the buffer. So this current MyEdit function actually will take the current file and will insert text into the file and it will do it with the sort of the full range of the current file meaning the entire file. So we're basically saying insert this string with this entire range, so it basically overwrites the range that you've specified. So if I just type something like WHOOPS and I run this, then I can go and type, whoops, additional commands, and then Apply my edit is available, so I hit that, and basically it just replaces the entire buffer. So that's kind of cool. I mean, another thing you can do is do some manipulation to the actual content that's already here. So if I take this and say Context.CurrentFile.Contents.ToUpper and all I have to do at this point, since the command is already registered is just reevaluate the function itself, and if I run it, it basically uppercases everything in the file. So you can start to see where this can go. You can just basically manipulate the code in place. So let's say if I wanted to actually insert some text into the file, so let me comment this line out and uncomment these two lines. So what I'm doing here is I'm actually creating an object that defines the range that I want to update. This is not the actual desired way that I want people to do this. I'll provide some overloads to this InsertText method, but for now, I'm just doing it this way. So basically what I want to do is in the current file, I want to insert a line of text at, let's see if I can get to the end here, at the Line 24, Column 1, and Line 24, Column 1, so that's like the start location and end location. So basically what I'm saying at this cursor location, I want to insert this piece of text. So if I go in, reevaluate this function and I should be able to run it and now it inserts this new line of text here. So I'm basically going to format the C drive, which is nice. But anyway, hopefully that shows you that yeah, we basically can do lots of stuff with this. One thing you could do is have a command for your project where you automatically are able to insert comment block headers for your files or maybe generate a certain type of function like you have a lot of code that needs to be generated for your project, you could easily generate it using one of these commands. Yep. But where's your plan just like going? Like are you thinking like in your profile or will that command register just stay with this session? That's a great question. So one way is it could go in your profile. Another thing I'm thinking is it would be great if there was a way that if a PowerShell module from the gallery included something that registers a command like this, if we could actually identify that module as having a command and load it automatically in the session, if you have maybe a command in your profile that says automatically load extensions, maybe the way that happens is we have like an extra tag we put on that is like a conventional tag we put on modules in the PowerShell gallery, so I can just do like grab all the modules that have this command that you have, sorry, this tag that you have that are installed and just load them up. For a person who writes a module who wants to actually add some extension commands from their module, they can just check to see if the PSEditor object is available, and if so, they can just call the Register-Command function. So I think it's going to be a little bit of both. Either people have their own sort of comment code that they are going to run for registering commands or modules that you get from the gallery will have extension commands that will be able to run whenever they're loaded into your session. So does that answer the question? Yeah. Yeah. So yeah, I kind of want this to be the new way going forward of providing new functionality to editors. The nice thing about this is that for an editor who wants to provide the same extensibility, they just need to implement a few methods on an interface or potentially respond to a few different messages in the JSON protocol for the host process. So it should be available anywhere that the editor service is used, which is pretty cool. So the first draft of this extensibility model is going to show up in versions 0.6, the next version of the VS code extension, and I expect it will show up in the ISE preview probably within the next two to three months. I'm going to do a little bit of work to get that started in the ISE preview for my next talk at the PSCompu, but it probably won't be fully there yet before the next version of the ISE preview. I've got one question. Yep. With the ISE, how do you intend to get the same functionality of it doesn't have that little bar where you type it in and stuff. Right. Another great question. Yeah, that's another one of the things I want to add to the ISE preview is that same kind of command palette construct. So for people who haven't really used VS code very much, VS code has this really cool command palette where it basically gives you every single command that's available in the editor and you can type something like save and it will show you the save commands. It also shows you the key binding for the command. So I would like to add something like that for the ISE because I think that it gives a way to surface all of the possible operations that are in the editor and allows you to add your own operations at the same level as the built-in operations so that all of the functionality that's in the editor currently, it's all sort of at the same level and it should be pretty helpful. But definitely curious to hear if people thinks that's a bad idea or if they like it. But. I'd like to see that. Yeah, yeah there's a lot of people on Twitter and I did a Twitter poll maybe two months ago or something asking if people wanted to see it and generally, people seemed to be in favor of it. So I'm going to give it a shot and if people don't like ultimately, I can take it back out, but I think it will be helpful. Let's see. So in my mind, the future of the PowerShell authoring experience is clear. Our short term goal will be to use the integration of PowerShell Editor Services into the ISE preview as a way to flesh out its feature support so that the same functionality can be used in many other places. This should help make developer adoption of PowerShell go a lot more smoothly because developers can use the tools that they already like, their editors that they already like to have a really good PowerShell development experience. They don't have to go and learn VS code or the ISE or Visual Studio if they don't want to. I'd also love to work with anyone who's interested in using PowerShell Editor Services in a code editor or any other type of application. The project is open for contributions and other type of feedback on GitHub, so if you just go check out our GitHub page, which I'll show to you later, then you'll be able to see all the things that are happening. If you watch the repo, you'll see all the work in progress. Usually it's just me keep talking on issues. Yeah, and the platform will get so much better with every new contribution and integration, so any amount of interaction there is definitely helpful.

Closing and Q&A

So lastly, I want to mention that we need volunteers for a usability study tomorrow at 4:30 PM, so this is right after the lightning demo session. We need about 20 to 30 people to participate in a UI study for a possible new feature for Windows Server. It's like a prototype that I've been working on. We are looking mainly for beginner to intermediate people with PowerShell, but advanced users are welcome. It doesn't if you're MVP or not. If you're not an MVP, well you can sign an NDA there. If you're interested in joining, please talk to me or Mark Gray, who's a PM on the PowerShell team, to sign up and we'll let you know more details tomorrow about what room it'll be in, that sort of thing. So thank you very much for coming to my talk. Here is some details about myself on Twitter and GitHub and also the link to PowerShell Editor Services on GitHub as well. I think we have a little bit of time for questions if anybody wants to ask questions. All good. (Audience question) It comes bundled with the VS code extension. So whenever you installed the extension through VS code's extension installer, it has everything you need there. So yeah. So basically, it's a dynamic application that is bundled and it just gets launched with the extension. When did the interactive shell get derived? Last time I checked code out, it wasn't. Yeah, so it's been there for maybe two or three releases. It's sort of hidden because you can't actually type directly in it. It's actually an output pane where if you select some code and hit F8, it will run it and show the output, (Audience comment) yeah, so the sort of hacky way to deal with that for now is to just type something not Format-Volume obviously, but like Get-Process, and hit F8, and it will give you the output, but. Will F8 just select the current line by default? F8 runs whatever you currently have selected. I think somebody added the function where, this is not actually selected, it's just highlighted. I think if you don't have a selection, it runs the current line. Yeah, okay. I don't know why it failed at that point. (Audience comment) Yeah, it runs up to where the cursor is. That's a little weird, but anyway. Yeah so what we're waiting on is for VS code to add the console so that we can basically just wire this directly into a console where you can type in commands, so that's coming. I think that within the next couple of months it'll be there. Yeah, that would definitely change my whole view on it. Yeah, that's like the one thing that's missing, I think, until this like way better. But anyway, I think we're out of time, so thank you very much for coming.

LCM Deep Dive - The Unsung Hero of DSC

Introduction

So welcome everyone to the last session of the first day. You made it! I know it's been a very interesting day, a very exciting day. Probably all a little tired and eager to get out and for the meet and greets. (Audience comment) I'm glad everyone's ready for this. We're going to do a little bit of a deep dive on the LCM. My name is Adam Platt. I'm a software engineer in the IT world, which gives me a little bit of an interesting perspective on things. I am used to working at software companies and not seeing the IT end of things,

Desired State Configuration Overview

So I've learned a ton from all the IT professionals that I've had the opportunity to work with and talk to, especially at conferences like this, so I'm really happy to be able to give back a little bit and take what I'm good at, writing code all day long without breaking a sweat and kind of share some of that with you and some of the things that I've learned exploring Desired State Configuration. Somebody else that's important for you to meet, the Local Configuration Manager itself. This is what we're going to be focusing on. Now I'm going to be assuming, since this is a deep dive, that everyone's familiar with the basics of DSC and all of those introductory concepts. However, if you do need to fill in some of the gaps, this morning, Jason Helmick had a great talk. Unfortunately, it's passed, but you can track down the video for that and there's a link here for the code that he released after his talk. He's also got another one on Wednesday, have the details there. If you are interested in filling in on some of the details, if you're a little light on those, definitely go check those out. He's covering most of what I'm glossing over. What we're going to do today. We are going to make a very quick run through an overview of Desired State Configuration, just touching on some of the points that are significant from the LCM perspective, then we're going to get into the nitty gritty. So let's jump right into it.

Partial Configurations

Desired State Configuration lets you write code describing configurations that you then programmatically apply to another computer and this other computer might be another Windows Server, it could be a workstation, it could be Server Core, it could be Nano Server, it could even be a Linux Server, and maybe someday, you can even push to your toaster. We don't know. Now Desired Toast Configuration might be a little silly, but the point is DSC doesn't care what you're endpoint is and the reason it's able to do that is because every DSC capable endpoint is running at LCM. That's what allows all of this to become endpoint agnostic. Now there are two ways of getting configurations onto a node. You can push your configurations to the node or you can configure your node, point it at a configuration repository, and have it reach out on its own and pull the configurations down. Now getting a configuration may not be the only reason that the node reaches out to another server. Configuration repositories for sure are one of the main reasons, but there are also resource repositories that node can use to locate and download PowerShell modules containing custom resources that it needs for its configurations. There is also the reporting server, which allows the node to report updates on its status to a central location, so you can get all kinds of good info. And that's it. And I think I made it in five minutes or less. Now let's get into the heart of the LCM and what can make this thing do. The best way that I found to explore this is to look at the different configuration options. What the LCM does is basically define on how you configure. So you can use the Get-DscLocalConfigurationManager cmdlet to get all these options. What are their current values? There are quite a few and I'm not going to go through all of them, but I'm going to go through some of the biggest ones that have the most impact on the behaviors and the first ones are the timers. There are two timers on the LCM. Each one controls the firing of a separate event and I call them the Refresh mode event and the Configuration mode event. These timers, these settings allow you to control how often those events fire. Now what do those events do? The refresh mode event controls how does the LCM on this node get new configurations. It's got three possible values, push, which we expect you're going to push configurations to the node, pull, which we're expecting to have a configuration repository somewhere and you're going to pull them down, and disabled. The disabled option affects more than just the refresh mode of your node. If you change the refresh mode to disabled, you will not be getting any configurations updated, or applied, or monitored, or anything. It basically turns off DSC. How often this action or inaction happens. Again, it's controlled by the refresh frequency NIMs property on the LCM. Configuration mode. The configuration mode setting controls what does the LCM do with the configuration it's already got. So whenever you get a configuration on a node, the LCM keeps a copy of it cached on the machine, so it always knows what its current configuration is. Configuration mode controls what does it do with it? Again, there are three options. First is ApplyOnly. This option configuration gets to the node however you're getting it there, it's always applied, but then nothing. We don't look at it, we don't care what happens to it after the fact. The next is ApplyAndMonitor. In this case, again we apply, and then we will periodically check on it. We'll say hey is that service still running that I said should always be running and the LCM will report to a report server, if you configure it that way, what the status is and what it's been able to see about what's in compliance and what's not in compliance. The last option is ApplyAndAutoCorrect. I think this is the coolest one. With this option, the LCM will actually actively correct deviations that it sees in the configuration. These are all controlled by configuration mode frequency NIMS. That's what determines how often whichever action you pick is going to happen. So these two timers are very similar, but they do just two distinct functions and we're going to run through a couple of examples to kind of help clarify that. So the first example is probably the most common operation you're going to see. So here we have our node with our LCM on it and I don't care in this example whether it's pull mode or push mode. What happens, configuration mode timer fires off based on that setting you set in the configuration minutes, wakes up the LCM, and the LCM does whatever you've told it to do with its current configuration. This action has absolutely nothing to do with any server outside of your node. It's doing an internal consistency check. Another example and this one we're talking about push mode. Now the most common operation that you do in DSC, in push mode especially is you say Start-DscConfiguration. That action actually has nothing to do with the timers. It sends over and immediately applies your configuration. So I didn't animate that one on the slide. I animated the Publish-DscConfiguration cmdlet, which allows you to send a configuration to a node in a pending state. At that point, whichever timer fires, actually both of them will cause this action. The LCM will wake up, it will find that there is a new configuration in the pending state and it will apply it. Sorry, it will apply in step four. Now it's important to note that this step four isn't connected to your configuration mode, well you could say it is, but every configuration mode starts with apply and, so it doesn't matter how you set your configuration mode, it's getting applied once it finds out that it's there. The last example, we're going to look at what happens in pull mode. So in pull mode, the configuration mode timer does the same thing that we already saw in the earlier slide. We're just going to focus on the refresh mode timer. When that fires off, it's going to wake up the LCM, the LCM is going to reach out to the configuration repository where its configuration lives, and it's going to compare the checksums. So it's going to compare the checksum of its local configuration against the server's copy of that same configuration. If they do not match, we're going to pull down a new copy of the configuration and we're going to apply it. If they do match, we're going to skip those steps, but it's important to note that the refresh mode action whether it is push or pull always ends, now if it's disabled it doesn't apply because you're not doing anything, but if it's push or pull, the refresh mode action always ends with a consistency check, which is the same action that happens when the configuration mode timer fires, so it's almost as though the refresh mode timer is like a superset of the configuration mode. So that was a lot for just a couple of seconds, but now we're going to look at a couple of other settings that are on the LCM. These three settings tell the LCM about server-side components that it's referencing. So ConfigurationDownloadManagers references, where are my configuration repositories? Yeah. So the configuration mode timer, does that still work when it's in a file? The configuration mode timer will fire when you are set to ApplyOnly, but it won't do anything. It basically wakes up and says okay I'm done and goes back to sleep. And then last question. And when it is an ApplyOnly, does it always test true if that one time it did actually apply properly? You mean the first time? Yeah, so in other words, it took the configuration and it applied it. Yes. Now from them on, does it show, if you do a test on it, does it show that it's true? Well if you explicitly issue a test, it's going to invoke an actually consistency check. Oh, okay. But if you just are observing the activities that are happening as the timers fire, you won't see any warning signs or any flags of anything being noncompliant. It'll basically just go under the radar at that point. Okay, thank you. No problem. And we'll touch on that a little later. There's a Test-DscConfiguration cmdlet you can use for that. So we have ConfigurationDownloadManagers, ResourceModuleManagers, that is where you configure your resource repositories, again where the node can reach out to get PowerShell modules containing custom resources and report managers controls which report servers you're reporting to. You can configure multiples of any of these, which I think is really cool because if you have a bunch of different custom modules that you need to use and they're built by different business units in your organization, they could each have their own repository, you can reference them all. The LCM will figure out where the modules are that it needs. It will search them down. And the ReportManagers as well. If you've got complex hierarchy and you want some nodes to report here, some nodes to report there or some nodes to report to three places, you can do that and you really don't have to think about it anymore. So at this point, I want to look at a little code. I know I'm a software engineer, I get excited about looking at code, but I try to keep it down to a minimum because sometimes I go a little code happy. So how do you build a configuration for the LCM? Maybe take a step back, how do you configure the LCM? You use Desired State Configuration to push a special type of configuration. This is called a Meta Configuration and you start by using the DscLocalConfigurationManager attribute at the top of your configuration. This tells PowerShell that you're making a Meta Configuration. What I love about this is that now PowerShell is not going to let you use any of your regular resources. It will yell at you and this is great because it helps you keep your regular configurations and your Meta Configurations separated. The next thing you'll see in this configuration is the settings resource. This is where the majority of the standard LCM settings can be applied for configuring things like your repositories and report servers, they have their own resource, and this is how you're able to apply multiples. You can put three instances of configuration repository web in there and reference three different configuration repositories if you'd like. So that's how you configure your configuration repositories for your module resource repositories. There is a resource repository web resource for your report servers. There is a report server web resource. And I also would like to reference, although I don't recommend, there are also two different versions of the configuration and resource repository resources, so these are the web versions. There's also a share version of each of those as you can use an SMB share for your configuration or resource repositories. Report servers must use HTTP. They don't support SMB.

How the LCM Works

Another really cool thing you can do is make use of partial configurations. These you'll see applied in the Meta Configuration that you send to the LCM. So you'll see here we're using this PartialConfiguration resource to tell the LCM about all the pieces that we're going to be sending it. In this mode, in this example rather, we're using Push mode. So you can use push or pull and I'll show you pull in a second, but this is an example of configuring a Push mode PartialConfiguration, in our case, we have three different pieces. Some notes about using partial configurations. Once you configure the LCM to expect partial configurations, you can no longer use Start-DscConfiguration for that normal default operation that we do to send and immediately apply. PowerShell will yell at you. What you have to do, what they want you to do is to use Publish-DscConfiguration. Just as we saw before, we'll send the configuration over in a pending state. At that point, you can use Start-DscConfiguration with the use existing flag and that will trigger the LCM to check for those pending configurations, suck them in, and apply them. Another really important detail is that the names of your partial configurations matter. Basically, in the PartialConfiguration resource of your Meta Configuration, if you give it name X when you publish a configuration to it, that configuration must be defined with configuration X. The names have to match because that's how the LCM knows what you're giving it. Otherwise, it would just be alright, well now I've got three configurations. I don't know which ones what. To do partial configurations in pull mode, we see a little bit more complexity. We've now added this ConfigurationID, which is a GUID, which we haven't seen a GUID this whole time and I was so happy that we don't have to rely on GUIDs anymore, but in this scenario, we still do. ConfigurationID ties all of your pieces together when you're in pull mode. The rest, pretty similar, we're using again the PartialConfiguration resource and this time because it's pull mode, we have to tell it where it's pulling from so you have a reference. Just like any other reference that you would see, like if you were doing a dependsOn in any of your other resources, you reference the ConfigurationRepository resource that holds the configuration that you're going to be pulling. Lastly, you tell it that we're in Pull mode. So quick question. (Audience question) The MOF file that you place on your pull server? Yep, I'm getting to that right now actually. Here's exactly what it is. So normally, you'd be able to deploy a pull configuration by just putting the friendly name of your configuration.mof in that ConfigurationRepository folder on your pull server. With partial configurations in pull mode, you have to use a friendly name .configurationid on both the MOF and the MOF checksum file. So again, this is kind of, you're using your familiar names, but the ConfigurationID ties everything together so the LCM knows alright, well these are all pieces of the same one thing. Does that answer your question? It sure does. Another cool thing about partial configurations in pull mode, you can actually put the different configurations on different servers. This is one of the really powerful features because if you've got a large organization and you've got different people in different departments and they all want a shot at your nodes, you can give them their own pull servers. Have them set up their own pull servers so your security team gets their pull server and I reference that and all the other teams get their own and you can reference all of them separately, pull the pieces together, and apply one final configuration. One caveat to that, oh I guess I get to the caveat later. It's on this slide. I'll get to that in a second. So partial configurations, we talked about push and pull, but you can also do mixed node. So I can define a Meta Configuration to my LCM and I can tell it well I've got a partial configuration for you. It's got four parts. This one is pull, this one is push, and these other two are pull and it works. I haven't been able to think of a use case for it, but I'm sure there is one, so it's very nice that it supports it. As I said, this is great for splitting up large or overly complex configurations to make them a little more manageable. One of the things I don't care for, although I don't have a better solution, so I'm not complaining is that you have to custom tailor your LCM configuration up front. Your LCM has to know all the pieces that are coming to it. You can't just say alright well here take this configuration and then a few days later decide well I'll take this one too, but don't get rid of the old one, apply them both. You have to know up front all the pieces that you're going to be sending. And here's my caveat, when you have your partial configurations, if they're coming from all over the place, conflict protection doesn't happen until we're applying them all on the node. And if you think about it, that's really the only way you can do it, especially if they're coming from different pull servers, but it is important to remember because it's probably going to be a real pain to debug. In general, when you're making configuration changes to the LCM, important things to remember are a lot of these options are not dependent on one another, configuration mode, refresh mode, they operate independently. So depending on the scenario you have, you can pick and choose only the pieces that you're really going to need in that case. That flexibility also means you can mix and match those settings to suit your need. Configuration mode and refresh mode have three options each. That's nine different choices that you can make just with those two options. That leave me to my other recommendation. There are so many permutations of ways to configure the LCM. If you come across something and you're wondering how it's going to work, build it quick in a lab, test it out. That's actually how I got a lot of the information that I needed for this talk, which coming from a software engineer, it was a lot of fun trying to create a domain and install the certificate authority, yeah, that was fun. But we figured it out. We got everything and I got all the info I needed.

Communication - Configuration, Ports, and Protocols - Outgoing

The ports on these are interesting. So this is a port on a remote server that your node is going to contact, and generally, you're going to be looking at port 8080 for your HTTP or your HTTPS and port 445, 139, whatever SMB is set up to use in your environment. Now 8080 is completely configurable, but it's the only port I've ever seen used in any of the examples, so it's the one that I continue to use, but obviously you can pick whichever port you'd like. Now the protocols that the remote servers, the server-side components that DSC use, is XML or JSON over the HTTP or HTTPS protocol and I say or JSON because in working with the report server, you can specify in your request headers that you'd like your output back in JSON format, but the default is XML. Now I tried using Fiddler to capture some of the packets that were being sent to the configuration repository, but I am not good enough at configuring certificates and getting you'd have to get WMI in secured with SSL in order to be able to run through a local, oh it was a nightmare. Like I said, I'm a software engineer. I'm not very good at that end of things. If somebody would like to help me with that, I'd be happy to lend my coding skills and capture some of these packets. (Audience comment) Oh okay. (Audience comment) I own the whole domain. I'll give it a shot later on. We'll see if we can get it to work. So actually what I want to take a look at now, I want to take a closer look the CIM objects. So you can use the Get-CimClass cmdlet to get the actual CIM class that is used to represent the LCM and it's full of these methods like SendConfigurationApply, Test-Configuration, ApplyConfiguration, which sound a heck of a lot like the things that we make the LCM do every day and it's actually very true because if you look at the cmdlets and their verbose output, we have the Start-DscConfiguration cmdlet, it's actually just invoking SendConfigurationApply. It's invoking the CIM method and passing it all the parameters and the payload that it needs to apply this configuration. Now this is actually a reason I was trying to track it in Fiddler. I was able to call some of the CIM methods directly by using the Invoke-CimMethod cmdlet, but I wanted to try SendConfigurationApply and I could not figure out for the life of me how to encode the payload, the actual MOF file. I can get content on it all day long, but I was just getting errors left and right. Maybe someday I'll be able to figure that out. I thought that would have been a nice little thing to be able to show, but oh well. So my other example, Set-DscLocalConfigurationManager. That's the cmdlet you use to update the LCM settings. It's just calling another CIM method on the LCM CIM class. A couple of things that's important to remember about the communication of the LCM, obviously you have to get all of your ports and your listeners configured correctly or nothing's going to work. Every aspect of the communication for LCM, especially if you just came from Lee Holmes' talk where we're talking about security, this will be on your mind. You can encrypt and use SSL on all of this stuff. It's just really annoying to set up like most things with certificate, at least from my perspective. And lastly, it's great to know what's going on behind the scenes, it's great to understand that CIM is what's driving all of this, but in general, try to stick to the cmdlets, that's why they're there. Next, I want to talk a little bit about logging. The LCM does do some logging, quite a bit of logging actually, and if you're using the event viewer, this is kind of the folder path you can use to find where the DSC logs are kept, but this is PowerShell summit, so we're not going to use the event viewer. We're going to use Get-WinEvent obviously, which is very similar, although you'll notice the actual log name is a bit different. And so what I did is I took a snippet of log entries. So we're going to buy time and I used the timestamps to kind of like grab a batch out of what the LCM was doing and I want to run through real quick what it did. So we can see DSC timer is running some method. So one of the timers fired and it's calling this PerformRequiredConfigurationChecks with the flag of 1. Cool. It's woken up the LCM. Now we've got this job GUID, which is cool and we'll see this GUID a lot because it's on every entry associated with this job. So we're doing a consistency check or a pull started by computer null, which makes sense because it's fired by the timer, not an actual user. We're running a consistency engine. We're checking our resources and you can actually see I was testing my partial configurations here. Ran successfully. Awesome. You have a question? Yeah. That timer is not a scheduled job, is it? I don't believe it is, but I don't know for sure. I think it started that way, but somebody told me that it changed. Actually, yeah, I think you're right. I remember hearing that as well. Again, I don't know for a 100%, but I believe you're correct. (Audience comment) Because that was one of the original problems I ran into trying to do a presentation last year where because they were scheduled jobs, after a reboot, things would just kind of hang for a bit, so I had to alter my demo. But so now, in V5 they are not scheduled jobs anymore. So now we've run our job and we get a nice summary of everything, so we can see configuration mode was apply and monitor. That's the check we just ran. We're monitoring. We can see what's set forth, that it's set for push, a lot of information available to us. But now, as we kind of climb back up that call stack, DscTimer successfully finished and now it's making another call. We're calling PerformRequiredConfigurationChecks again, this time with the flag set to 5. Down the call stack, we start something, we finish something, and then the LCM shuts down. So what the heck was that? PerformRequiredConfigurationChecks was a method it called both times. So I pulled it up on the MSDN documentation and I didn't get much farther. But and I checked that this morning. The page still looks like that and I'm not trying to call anybody out. I'm not trying to be mean. But, (Audience comment) I looked for the Contribute button because throughout the process of building this talk, I actually fixed a couple of typos in the DSC documentation since it's all on GitHub, but this, unfortunately is not available on GitHub or I would suggest some updates to it. But through all the experimentation that I've done and all of the verbose output that has scrolled passed my eyes, I know that the parameter, the flags parameter set to one is the configuration mode action. So that first longer set of log entries was when the configuration mode timer fired and we performed out configuration mode action of apply and monitor. The parameter five, which is what was used for the second one, is what is for the refresh mode action. Now we saw in our log output that the refresh mode was set to push, which means the refresh mode action doesn't really do anything. So actually that log is just showing us that both timers both happen to fire off at the same time. Now there are two other event logs related to DSC that can be useful if you need them. There's the analytic log and the debug log. I went and I grabbed the similar snippet for this about the same timestamps and the analytic log had two entries and the debug log had quite a few more entries. So there is a ton of data available if you need it. The important thing to remember about these two is that they're not enabled by default, so you can use these commands to turn those logs on and start getting that output from then on. To remember about logging, again, only the operation log is enabled by default. That GUID that we saw applies to every unique job. In Acme, I forgot to mention it, but the two separate PerformRequiredConfigurationChecks calls, they each got their own separate GUID. So you can always use that GUID to tie a job to itself. Moreover, that GUID is the same across all three event logs. So if you wanted, you could have them all running, get all the WinEvents, put them all together and sort them by time to get a really nice, full and complete overview of an individual job. And lastly, and this was a suggestion from one of my coworkers, don't forget to turn off the debug log when you're done because it is pretty verbose. So let's see. I've got a little bit of time left. Do people have a lot questions? Because if so, I can skip some of these differences and we can start doing some questions, but if you guys are good, I'll go more into these differences. I have a question. So our session today goes on… Well I don't want to keep everybody. I know we've got some drinking and relaxing and hanging out everybody to do. I'll run through them. (Audience comment) Sure. So it's important to note, I'm not hitting all of these changes. I just picked out some of the most significant ones, especially as it relates to affecting the LCM. So some of the new or updated cmdlets, we have the Get-DscConfigurationStatus cmdlet. This is a new cmdlet and it gives you access to the new status data that they'd made available on the LCM, so this will tell you if it's idle, if it's in the middle of applying a configuration, if it's got a pending configuration and what the results of previous configuration actions were. Next, we have… Yeah. So I noticed that if I do an update configuration that I can't go and test the configuration until that completes. Does this command run if that's the process? No. No. No. Yeah, I believe it will come back and tell you the LCM is busy and you'll get an error. (Audience comment) Right, because you have to wait for it to resolve. (Audience comment) Nope. That's all you get unfortunately. The Publish-DscConfiguration cmdlet that we saw using partial configurations, that is a brand new cmdlet, that didn't use to exist before and I think that's a really cool way of perhaps staggering some updates. If you've got multiple nodes and they're all in push mode, you don't have to send and immediately ApplyConfiguration, you can publish that configuration to all the nodes and wait for the timers to fire because they'll probably fire at different times, so you can kind of save yourself a bit of an update storm. Next, we have the Update-DscConfiguration cmdlet. This is another brand new one and this is one that I wish I had last year when I was preparing for a DSC talk because I had to figure out how to manually trigger some of these updates. You can use the Update-DscConfiguration cmdlet to force an update if you, like let's say you just push a Meta Config to your LCM to put it in pull mode. That's all well and good, but it doesn't do anything after that. It just says okay I'm in pull mode now and then it waits for the timers to fire. But you can use Update-DscConfiguration to tell it no, update now and it will go ahead and run the appropriate CIM actions and get you your pull configurations. The next cmdlet is an updated cmdlet. Test-DscConfiguration used to exist, but it used to be that all you could use it for was testing the LCM's current configuration. Now what they've added is the ability to give it a configuration to use for testing and this is really cool because it allows you to inspect the state of a node without going anywhere near touching its state or updating it at all. So whatever configuration you give to Test-DscConfiguration, it'll reach out to the node and it will basically perform a consistency check and check all the resources in your configuration for compliance and give you a report back on it and that's it. So you could actually build multiple health check configurations to use in your environment and just pull them up as you need and say even hey this machine is not responding correctly. Let's say I've got a XenApp form. We do a lot of Citrix at my company. I want to make sure all the appropriate Citrix services are running. I could build a configuration that says this and this and this service should all be running and boom, test configuration out to that machine and see what's going on. I don't have to do anything more. I don't have to remote into it, I don't have to REP to it, I can get some quick data, and I know I'm not affecting the state. How is that parameterized? How do you get it that information? Trying to think. I think there were two ways to do it. You could give it the name of a compile configuration. So you could, just like you were going to apply it, have a configuration run it into memory, call it with the machine name and then pass that in, pass the MOF file in. And I think there was a second way to do it, but I can't remember off the top of my head how that one worked. Another new cmdlet is Invoke-DscResource. This cmdlet gives you direct access to ADSC resource in a configuration. So let's say you don't want to look at the whole configuration. You know that there is one register resource in there that you're concerned with. You can directly access the Get, Set, and Test methods of that resource using this cmdlet. This is way more fine-grain access than we ever used to have to DSC. And the last one, this is another cmdlet that already existed, but has gotten a nice update. Like we were saying before, the Get-DscConfigurationStatus cmdlet, if it's going to tell you nope, the LCM is busy, you've got to wait, Set-DscLocalConfigurationManager will do much the same thing. So if I'm trying to take my node and make it a pull mode and change the configuration it's got, but it's in the middle of applying another configuration, well I'm about to change the configuration. I don't care what it's doing. I want to force it to stop what it's doing and change its settings now. Guess what parameter they added? Force. So you can now use the force parameter to cancel any action that the LCM is in the middle of and immediately apply your new settings. There are also some new or updated properties on the LCM itself. The first is the AgentId. This is going to come into play a lot when we're using reporting servers. The AgentId is a GUID, it's automatically generated, and you can use it to uniquely identify your node. In the past, pull server configurations used to be identified by GUID and you would use that configuration id on your LCM configuration to tell it what configuration to get. When that node reported to the report server, it would report based on the configuration id, meaning that if you wanted to get distinct reports for every node, you had to create a separate configuration with a separate configuration id for every node. They've done a lot of work on updating that. Now you have the AgentId and you can uniquely identify all your nodes and report on each of them. The LCM state properties, which were added, offer basically a view of the most recent Get-DscConfigurationStatus, so they'll tell you what it was up to and what the result was. The RefreshMode value of Disabled is actually new. It didn't use to exist. It used to just be push and pull. And if you can believe it, the timers used to be required to be multiples of each other, so you'd have to have 15 and 30, 20 and 40, and I don't know the reasoning behind it, but they have adjusted that, so now that's no longer a requirement. Another difference between 4 and 5 is the way you build your Meta Configurations. I was saying before about when you use the DscLocalConfigurationManager attribute, PowerShell won't let you use regular resources. In v4, you could, you didn't have to, but you could mix and match those resources and have a regular configuration that when you compiled it, or when you invoked it with your machine name and all your other parameters, it would create 2 MOF files. It would create the regular MOF file and a MetaMOF file because you had used the LCM settings resources in there. They've really split that out. And they also added the other resources that allow you to specify your repositories and your report servers, which is much cleaner and more modular and allows you to specify multiples, which I think a lot of people are going to find themselves doing, so it's great that they built it this way. And as I was saying, it gives you a better distinction from your standard configurations, so there's a little bit less confusion there. I touched on this a little bit talking about the AgentId. So in the past, DSC configurations used to require you to use a GUID for the name or pull configurations. But now, you can just use the friendly name because the AgentId has taken that place of a GUID in a sense and the LCM will just reference the friendly name and use that to figure out where the configuration is. Because moving from a GUID to a friendly name makes things a little more transparent. They also introduce the concept of the RegistrationKey. RegistrationKey is something that you set up on your pull server and it is a secure string, generally, out of the examples I've seen people are using a GUID, that you're nodes must specify when they try to register with a pull server. So it's basically saying hey I want to go to this configuration repository and I want to get configuration blah. The pull server is not going to let me set up into pull mode unless I give it the right RegistrationKey. This is just to provide a little bit of extra security so that people can't very easily go snooping around your pull service to see what configurations you have and start to decipher how you configure and implement your architecture on your network. So does that only happen is when you set it up, it actually goes and connects to it right there. Yes. Which is different, right, which is kind of cool because you get to see wow it's connected. Now I know it's actually connected to my pull server. And you'll see it in the verbose output too. It'll say successfully registered with pull server. Yeah, that's what I'm saying, so my question is, is that the only time it does that because is there now like a certificate handshake between those two that they use after that? The RegistrationKey is only used that once and if you look at the LCM configuration, it actually gets blanked out after that, so I don't know the details, but there is some sort of an established relationship from there. I think the node's AgentId gets registered internally on the pull server, and so they're able to kind of bypass that pass phrase step in the future. I didn't realize they have it blanked out. I never look at that before. Yeah. That makes more sense because to me in my mind I was thinking well if it's still there, then what difference does it really make? Right. It's there for every time. Then anybody could go on and look at it and see that number. When the Jet database is used or is that with the RegistrationKey in that Jet database. In the Jet database on the pull server-side? Okay. Let's see. Partial configurations are also, those are brand new in version 5. I don't have a ton to say about them. This is kind of a completely new concept. But I think they're pretty cool, and again, they can help you with a lot of your large-scale work. So to kind of wrap things up.

PowerShell v5.0 vs. v4.0

So after what you can do with the LCM, I want to talk a little bit about how it works and we're going to take a little bit of close look at things like this. Incoming communications to the LCM. What are the requirements? What do you need to make sure you have in place so that this whole thing works? All that the LCM requires is the WinRM Listeners. There's a myth that goes around a lot and it makes sense because they're very closely connected, but a lot of people think you need full PowerShell remoting in order to do Desired State Configuration and that is not true. All you need is the WinRM Listeners. In fact, I had a few cases, actually on the plane coming to the conference I had to restore one of my nodes to a snapshot, and I'm on a plane, I have no internet access, the system time is out of whack, Kerberos authentication starts failing when I try and use PowerShell remoting, but I can still push DSC to it. So it's definitely a myth. All you need are those WinRM Listeners. The ports that the LCM is going to communicate on, the WinRM ports, you have 5985 to HTTP, 5986 for HTTPS. Now the protocols and the connectivity of it all is using this CIM protocol through HTTP or HTTPS. CIM is really cool and it's something that we were starting to hear a lot more about and I know Richard Siddoway did a whole session on it yesterday that unfortunately I wasn't here for, but CIM stands for the Common Information Model. So this is an object-oriented data model for describing various components of the enterprise. It's not limited to just computer stuff, even though that's what we're using it for. This standard was created by the desktop management task force in the late 90s and it's actually been in use ever since. Another three-letter acronym that we're hearing a lot with Desired State Configuration is MOF and that stands for Managed Object Format. This, the syntax that is use to represent CIM communications in DSC. So when you're compiling your configurations and you're getting those MOF files, it's basically a packaged up translated version of your commands that are ready to be sent over the wire to another node. Now as unfamiliar as many people may be with these two acronyms is another three-letter acronym that I bet everybody's familiar with. What's really interesting is that WMI is an implementation of CIM. Now there's more to it and I don't know all the details on this one, but I know that WMI rides on top of CIM traditionally with some extra stuff added and here's kind of the way that I like to think of it. CIM and MOF are kind of like abstract concepts. They're standards that were developed and they're maintained by a body and WMI is a concrete implementation that makes use of those things. Richard Siddoway is giving me a face. Have I gotten it wrong? Close. Close. (Audience comment) So let me see if I'm understanding you correctly. WMI is Microsoft's implementation of CIM and the new CIM cmdlets that were introduced in PowerShell 3 are also CIM, but they're a separate CIM implementation? Yeah. Okay. Well, you heard it here. (Audience comment) More standards in here? Yeah. Okay. (Audience comment) Okay, see and this is why I'm sad that I missed a class yesterday because I would have been able to fix my slides. (Audience comment) That's true. So the analogy that I kind of use for myself trying to understand this because I look at things like HTTP and HTML. They're abstract concepts. They are a standard, a protocol, and a language defined by, maintained by a body, and then you have a product like IIS, which is Microsoft's implementation of a product that uses these concepts. And another way that this analogy is helpful is that IIS isn't the only web server out there. In the same way, WMI isn't the only CIM server that's out there. OMI is the Open Management Infrastructure. This is an implementation of CIM open source and it runs on Linux. That is how we can send configurations to an LCM running on a Linux machine and it's probably going to be the same technology that we used to send configurations to the toaster as soon as we get that worked out. As far as outgoing communications go, we know that the LCM will reach out for three different purposes in general. It's going to go to a configuration repo, the resource repo, or a report server. I'm not going to cover setting those up. Again, Jason Helmick's talk from earlier today, he did a great job of covering that. Unfortunately, it's already passed, but check out his GitHub code and keep an eye out for a video on that.

Conclusion and Lightning Round

The more I thought about it, the more I realized that doing a deep dive on the LCM was almost the same as doing a deep dive on DSC itself. The LCM does so much of the work. Also, make sure you know the cmdlets. There's a lot available and you can do a lot of different things with them and they're going to help you get by very well. Also, get used to the LCMs capabilities, how you can configure it, and what you can do with it. It's going to help you be more creative when you have to design solutions in different scenarios. And lastly, between v4 and v5, we know that a lot of things have changed. It's important to be aware what features are available to you based on what version you're running in your environment. I still have environments where we're running v2, so I don't even get to use DSC a lot of the time, but it's a very important thing to keep in mind and one other kind of, I guess it's kind of a speculation, but with all the change that's happened just between these two versions, I would definitely expect this evolution to continue. The report server, for example, is cool, but there's no actual front-end for it. You have to design, you have to build the HTTP request yourself to get the reports out of the nodes. I don't know what we're going to see come out of that, but I would bet you dollars to donuts that something is going to change. Things are going to continue to evolve and it's only going to benefit us. We also are celebrating 10 years of PowerShell. It's kind of hard to believe and especially as somebody who really only, I only started working with PowerShell about three years ago, so it's been incredible, even just these past three years and it's great to be up here in front of everyone helping celebrate these 10 years with all the people who have been involved throughout the whole ride. I've got some links to the references that I used to gather the data. You guys copied all that down right? Those don't matter because you can go to this link. This is the one you would write down and this is a link to this PowerPoint and it's on OneDrive, so any updates I make to it, such as correcting my CIM story should be available immediately. Anybody have any questions? You want to move onto the Lightning Round? Yeah. I collected a couple of the strangest cases that I've seen and I put them together a little bit like a game show. So I've got eight quick questions. If you push a Meta Configuration to a pull mode node that changes the configuration assigned to the node, what will happen if the configuration mode timer fires immediately after that Meta Configuration is applied? Is it A, the node pulls the new configuration from the repository and applies it, or B, the node performs a configuration mode action against the existing configuration. Who thinks A? No. Everybody thinks B? What's your refresh mode? Pull. No, no, is it applying longer or? That's the configuration mode. Set. What's that? That'll change the experience. That is true, but in this case, it wouldn't matter because we're just saying it's going to apply the configuration mode action, but whatever that result is, could be different, but that's what it would do. So the correct answer is B, is that if you tell a node to say your pull node, get a new config. Until that new config is pulled by the refresh mode action, configuration mode is going to continue to look at whatever it's got locally and whatever that action, however you've configured it, that's how it's going to continue to work. The question doesn't define why it changes it to pull or push. Yeah, the assumption is that it's in pull mode the whole time and you just switch the configuration that it's pointed at. That's my bad. I'm not going to be writing test questions for the SATs anytime soon. But again, you could write them for Microsoft. Question three, TBD. Let's say you have a node configured for partial configurations in mixed mode, but you haven't pushed any of the push mode partial configurations. What's going to happen when the refresh mode timer fires? So let's say you've got three partial configurations, two of them are pull, one of them is push. You haven't pushed anything. Refresh mode timer fires, so what's it going to do? Who think A? Okay. Who thinks B? The correct answer is A. And actually, I just learned this earlier today, unless you use DependsOn in your Meta Configuration, that might change the game a little bit, but if all of your partial configurations are just out there, the node will pull whatever it can, whatever it's set to pull and apply it and eventually when you get around to pushing the push ones, then it will apply that one too. Unless there's a merge error. Unless there's a merge error, which you won't know until it tries to apply on the node. Oh, this is a great one! If your configuration mode is applying autocorrect, will the LCM send a report to the report server before or after it fixes any non-compliant resources it finds? (Audience comment) Does it? Just because it's not set to autocorrect, it's… It is set to autocorrect. Ah, sorry. Well the data's not complete until it goes through the cap so… I had to go test this one. It actually, I messed up my configuration. I messed up the state and let it run. It reported all resources in compliance. So the correct answer is B. It actually sends a report after correcting the non-compliant resources, which I guess in a way is good, but at the same time, you don't get any visibility on the drifts happening on your nodes, so that's kind of a good thing to be aware of. Question 4. Can you use Start-DscConfiguration -UseExisting to trigger a consistency check on a node's current configuration? True or False? (Audience Comment) Correct answer is B false. This is kind of a trick question. The use existing flag on Start-DscConfiguration will cause the current configuration to be applied fresh, as though it just came to the node. If your configuration mode is apply only, that's a very different thing than doing a consistency check and you can also see in the output in the logs, in the event log that when it does a consistency check, it is noted as a consistency check, and when it's applied fresh, it will be noted as apply. So again, like a lot of things in DSC, they're similar, but different. Question 5. A node is configured for three partial configurations one, two, and three. I'm just going to go ahead and say Start-DscConfiguration and try to push and apply a completely separate configuration to the node. What's it going to do? Error. That is the right line of thought. It does not let you do that. The command is going to error because you cannot use Start-DscConfiguration at all with a node with partial configurations, unless you're using the use existing flag. So even if I tried to push and apply immediately, one of those three partial configurations, I get the same error. Just a couple more. These get more complicated, I swear. So we have a node using partial configurations, so it's got one in pull, one in push, and another in pull. The LCM is set, the refresh mode pull. What happens if you use Publish-DscConfiguration to send partial two to the node? Now this was a question for me because normally when you set the refresh mode of the overall LCM to pull, it won't let you do anything that's like a push or a publish. So I had to test this one out. Now who do we, do we think A? No. Who thinks B? You're all right. It will accept it and it won't say boo. Another node, partial configuration is complicated in that. It threw a lot of curveballs at me. (Audience comment) It's tricky and I think we may see some evolution of it before it really comes into standard use or we may… It should make sense though that you can combine configurations because then you're basically testing a new configuration before you're setting it anyways. Exactly. Yeah, it does make a lot of sense. So we have again our node with three partials, two in pull, one push. Refresh mode property. Oh, it's still set to pull on the LCM. What happens when you use Publish-DscConfiguration with the force parameter to send a modified version of the first partial configuration to the node. This was a really weird one this morning. So A, even though you use the force parameter, you still get an error because that specific partial configuration is in pull mode, right, partial one is configured in pull when I'm trying to publish it. B, the node takes the configuration, but then the next time refresh mode fires, it's in pull mode so it's going to go not matching checksums and pull the other version back down from the pull server, or C, it takes my configuration and actually updates the settings on that partial configuration to be push. I had no idea what to expect with this one. You say A? Who else thinks A? Anybody for B? Wow. And C? Nobody thinks C. Well that's too bad because it's C. Why does it change into push? Because I just pushed the configuration to it I suppose. I honestly, I just tried that this morning. I had a thought in the shower, I said what if I do that? And when I checked the LCM settings, boom, all a sudden that configuration is now in push. It could be to keep the older version from getting pulled back down and overriding what you just push, so I'm not sure. Yeah. So you had an earlier question about Start-DscConfiguration? Yep. And what it would do if with pull mode. If you use that source, it will reset the push button also. Really? Yes, it will. Oh, you know why, I bet my example was different because I had partial configurations, but if it's just one configuration and it's in pull mode, you can push to it with force and it will slop it to push? Yes, it will. That would have been question nine. This is my last one. Sorry I'm keeping you guys so long, but I hope you're enjoying it a little bit. So if I've got a push mode node and I publish a configuration to it, so it's in a pending state, then I change the refresh mode to disabled, what happens to that pending configuration? Does it sit there in the pending state or does the LCM say no you disabled me, it's gone? (Audience comment) Yeah, people say B that is removed? Anybody for A? Okay. Who's for B just to see? About half and half. The actual answer is A. I did a test with it this morning. It sat there in the pending state. Now when refresh mode is set to disabled, you can't do Get-DscConfigurationStatus, you can't get a lot of information out of the thing, but once I re-enabled it and I fired the CIM method to do a refresh timer command, it took the pending configuration and applied it. So it sat there the whole time just waiting for DSC to be re-enabled. Those are all the questions I have for you. If there's any additional questions for me, I'm happy to answer them. If you think of any scenarios that I haven't covered and you'd like to see, I have my whole little domain environment that I'm still very proud of building right here on my laptop. Feel free to come find me, we'll test them out. I hope you guys enjoyed it.

Old Dog, New Tricks: Digital Forensics with PowerShell

Introduction

Alright. Welcome everybody. This Old Dog, New Tricks Digital Forensics with PowerShell. I'm Jared Atkinson. I'm from Veris Group's Adaptive Threat Division. I used to say that I was the token defender at Veris Group Adaptive Threat Division because I work with very smart offensives lines like Matt Graver over there, but since we've started hiring some more defensive people, so I have friends that aren't just breaking things. I'm the hunt capability lead there, so the concept of hunting is practicing the assumed breach mentality in an enterprise. So the basic idea is you have a really large organization, there's somebody out there that probably wants to gain access to their network, steal data, do whatever they need to do, and if there's somebody that wants to get in, there's a very likely chance that they will be able to get in, and so our job is to go in and look through the network as if they have been breached and try to determine where attackers have been or if there is a presence of attackers. My background is that I was in the US Air Force on the Hunt team there from 2011 to 2015. Probably one of my more proud achievements is in last year I had Black Eye, which is a big hacker in conference in Las Vegas, I won the Minesweeper championship. So before PowerShell was my Microsoft love, Minesweeper was the original. I'm also the moderator of powershell.com security forum, which if you have any security questions, please come visit and ask the questions. And I also developed PowerForensics upper IDS and WMI event, eventing, which are two modules that Matt and I will be talking about Wednesday. So if you're interested in that, we'll definitely be touching on it. I also really love forensics artifacts, which is what led me to talk about PowerForensics, so. Going forward, so I kind of want to set the stage for what PowerForensics was created to do. When you're looking at forensics in general, forensics is a very touchy word because it's typically the like true definition has to do with law enforcement, law enforcement process where chain of custody is very important and all these different things that when you're responding or practicing the assumed breach mentality in a 100,000 node network, you can't necessarily take the time that a traditionally forensic investigator would take. Forensics has taken on kind of a colloquial definition that is doing hard drive investigation or analysis, and so or even memory analysis. So originally back in the day or even today in more law enforcementy-type circles, folks are actually doing the kind of the image methodology for forensics. So they'll take a suspected infected system, they'll take DD or something along those lines, make a bit for bit copy of that hard drive, they'll keep the chain of custody records. This is very good because it has, it's kind of the gold standard for forensics for the past couple decades, very repeatable, meaning that any analysis that I do, I can go back to the evidence and reproduce that analysis for like a court case, things of that nature, and it allows for thorough analysis. One of the problems is that a typical forensics investigation is typically going to take around three weeks or so for a single system, so like I said you have 100,000 endpoints, you don't know if there's a bad guy there, but you need to be able to quickly determine, make a determination of whether there has been some attacker activity. You don't necessarily have three weeks per endpoint and you also lose volatile data if you haven't captured that. Then folks started going into this kind of collection script concept, so the idea is that I write a batch script and then I'm going to script out and use PsExec or something along those lines to script out a bunch of tasks. So maybe one of the tasks is collect all the prefudge files or make a copy of the national file table, things of that nature. I'm going to pull it back and then do offline analysis. So you're not taking a full image because in a terabyte hard drive not all, you know, however many bytes that are really valuable from a forensics perspective. I'm just getting the files or data that's important for my analysis. The problem with this is that it's often messy, not forensically sound, right Matt, so that's one of my kind of big pain points and it often uses third-party dependency, so things like accessing a file, parsing the artifacts, and support for report capabilities. So you may use PsExec to get remote execution, you may use raw copy to copy off a file like the MFT and then you have to use like Analyze MFT to parse the MFT, so there's tons of different tools that you're using that have to be all strung together and they're not necessarily created to be put together in that way. So then we kind of move onto this live response, which is kind of the concept that I'm pushing with PowerForensics. So the idea is that you're able to quickly triage key systems in order to determine where you need to look next and determine if something has indeed happened that you need to investigate. And so the pros are that it's very fast, forensically sound in the manner that I'm going to be doing it, it's self-contained, so PowerForensics, the remote access, the file access, and the parsing are all built-in and they all build upon each other. The cons are that it's not repeatable, so unless you're taking a copy of that data, you're not going to necessarily be able to use this investigation in court. So this is more focused towards your advanced persistent thread to write nation state type of text, so trying to hacks Company X, Company X isn't going to get China to extradite the hacker in order to press charges, right. So ultimately, it's about stopping them exfiltrating data before they're able to reach their objectives. Alright, so my solution to this is PowerForensics. So this is a PowerShell module for live forensics investigation. I chose to write it in C#, so it's a compiled C# DLL binary module for speed and things of that nature. Minimizes the use of Windows APIs, so that whole forensically sound concept, so attackers can use root-cutting techniques to basically lie to you if you're using the operating system's APIs. So let's say you want to look at the SAM hive, for example. If you're using the operating system to look at the SAM hive, somebody can basically intercept that request and know they'll lie to you when you're trying to get data from them, and so I'm trying to parse it all on my own and not rely on the operating system, which can be tampered with. Currently, we're parsing NTFS data structure, so the new technology file system, which is the base file system for Microsoft Windows, and then Windows-specific data structures like the registry, the Windows event log, scheduled jobs, pre-fetch files, so on and so forth. A couple design requirements that I had when I first started is it needs to be forensically sound, if you haven't noticed, that's kind of my thing, so meaning it parses the raw disk structures on its own. So all the parsing code is stuff that I've written myself, as opposed to using some third-party library or some built-in library and it can't alter like NTFS timestamps, things of that nature, so you don't want to actually affect the data that you're dealing with. It can execute on a live running host, so that's really important. I don't want to make an image and then be able to have to investigate that image. I want to be able to do it on a live host without taking it offline. It needs to be operationally fast, which I hope I'll be able to show you that here soon. Lee really helped me on that. So I was at DerbyCon, which is a hacker conference in Louisville, Kentucky. Lee sat by me in a class taught my Carlos Perez and we literally just kind of went through on the MFT parsing. I think it started at like 10 seconds to parse an MFT and we got it down to 4 seconds, so Lee showed me a bunch of ways to kind of speed up the way that the code was running. Everything needs to be modular, and so things like, for instance, I have an invoke forensics DD cmdlet, which is going to allow you to read from a raw disk. That capability needs to be built upon in order to, for instance, parse the MFT and so on, and so forth. So everything is built off of each other, right. And then it needs to be capable of working remotely, which there is, we're at the proof of concept stage with that right now, so we'll talk about that at the end.

Reading a Disk - Volumes Contents

Alright, so the first demo is the forensically sound demo, so I'm going to walk through kind of how you would actually parse NTFS and how PowerForensics handles it. So the first thing that we need to do is read from a disk or volume directly, right. And this is where the only Windows API is used, and so the Windows API that we're using here is the CreateFile API, which allows you to create a read or write handle, in this case, we're doing read only, read handle to a physical disk or a logical volume, so think physical disk 0 or logical volume like the C drive, for instance. And then we're using the FileStream Read method to read from that handle. This is an example of the PInvoke signature that I'm using for CreateFile and I'll kind of show that as we go. The first thing that you're going to run into when you start reading from the physical disk, it's kind of an eye chart, but just to show is the master boot record and we'll actually show how the code parses all this inside here. The Master Boot Record is the key there is actually this partition table, and so, by default, the master boot record has up to four partitions. There's an alternate partitioning scheme called the GUID partition table, which is going to allow more than four default partitions. There's also ways that you can make master boot record have more than four. Ultimately, what we're looking for are things like the partition type NTFS or the status, which is 80 bootable, meaning that's the bootable drive or the operating system drive. And then we're looking at the things like relative start sector and total sectors, which are going to tell us how much or how many sectors, a sector is 512 bytes, are taken up by this logical volume, right, and so now we know where, in this case, the C drive is or whatever your operating system drive is. After that, we're going to look at, in the case of NTFS, we're going to look at the Volume Boot Record, and so this is going to have tons of information about how you should understand the layout of that particular file system or that volume. And so we have things like bytes per sector, which in this case, 200 hex, which is 512 bytes per sector, then you have sectors per cluster, which are 8 times 200 hex, which is 4096 bytes per cluster. Those are just helping you understand the geometry of the volume. And then when you have things like a pointer to the MFT, so MFT first cluster number, that blue one right there, that's going to tell us where the master file table is. And so if we follow that pointer, we then find the actual master file table, right, and this is an example of a single master file table record. The master file table is a metadata file, so it is a file that keeps track of all other files on disk, right, and so there's an individual record. Each record by default is 1024 bytes. If it's not going to be 1024 bytes, the volume boot record is going to tell you that. Inside of that, it's going to have a number of different attributes. So they're not really shown here, but we'll talk about them in a second. Each attribute, like for instance, the file name attribute, I'll let you guess what that keeps track of, right, it keeps track of the file's name. A file in this context is either a directory or an actual data file. In this instance, in this image, we have a data file, so there's a data attribute, which has either contains the data for the file, if it's a relatively small file or it contains pointers to the clusters that contain the data for the file and we'll talk about that going forward. Alright, so this is what a data attribute looks like. This is for a relatively small file. We're talking because so there are two types of ways that the MFT record keeps track of data. It can either be resident or non-resident. A resident data record is going to have the data within that 1024 bytes, and so as you can imagine, a file needs to be around like 600 bytes or less in order to be resident, and so in this case, it just says Hello World, so the data's small enough it fit within there. In other situations, there's going to be pointers that show you where that data is, and so this is an example of a non-resident data attribute to where there's these data runs, which it took me a really long time to figure out how those work, but ultimately, what they're telling you is that hey I offset 68DC04 for 21CO hex clusters there's data that's relevant to this file and this is an example of a fragmented file, meaning that there's the clusters are not sequential, so they're spread out throughout the hard drive, so you have to understand that and then read them all in order. Then you have this guy, so instead of so directories don't have data per say, but they have information about files that are contained within that directory, and so that's what this Index_Allocation attribute is telling you. It's telling you, in this case, you have let's see HelloWorld.txt contained in there and a file named Test. And then you would have to follow it back and figure out what the directory's name is, things of that nature, so we'll show that here in a second. Okay, so now we'll actually start going through this. Okay, so one thing that I wanted to show, I meant to show this a few seconds ago, so WinDD.exe is a pretty well-known DD.exe is like or DD is the Linux capability for disk duplicator. It's supposed to be able to make bit for bit copies. WinDD is a Windows implementation. One of the things that I wanted to show is that when we actually run Get-PE, which is part of PowerShell's arsenal, something Matt wrote, we can actually look at the import, the function imports, and so it uses CreateFile in the same way that PowerForensics is using it. So the idea is that although we're using a Windows API to get access to the actual drive itself, you have to do that at some point. So if an attacker is going to attack you, attack PowerForensics, they're attacking WinDD the same way. So that's the one place where I think an attacker could kind of take advantage of the process, but it affects pretty much everybody equally. Let me just run this real quick. Alright. I've been practicing all the little shortcuts and stuff that I forget.

Reading Master Boot Record

Alright, so I'm going to be using splatting, which for those that aren't familiar, basically we have a hash table, each one of these represents an argument that we're going to pass to a cmdlet, so in this case, we have an InFile argument, which has a value of \\.\PHYSICALDRIVE0 and when we do the @ sign in front of the props variable, that's going to just splat that out there and assume that each one of those items in the hash table represent an argument. So just for readability purposes, we're doing it that way. So Invoke-ForensicDD takes a few different arguments. So InFile is the file that we want to read from, Offset is where we want to start reading from, BlockSize is how many blocks or how many bytes at a time we want to read, and then Count is how many of those blocks that we want to read. In this case, we're going to start at 0, read the first block, which represents the Master Boot Record, and then we're going to output in text so that we can kind of see what it looks like. So this is the master boot record for this hard drive, the physical drive 0 on this virtual machine. You can kind of see down here there's some error messages, invalid partition tables, so on, and so forth. This guy is the partition table, but that doesn't really make sense. I don't know about you, but I don't really want to look at that, and so what we can do is we can take that capability where we're reading that first sector and we're just going to do Get-ForensicMasterBootRecord, and so now I've interpreted it for you, right. You have a disk signature in there. This is the code section. It's actual like boot code that's going to be running when you first start up your system, and then there's three NTFS partitions on this hard drive. Okay, so going forward, we can then do like, for instance, Select-Object -ExpandProperty PartitionTable. There's a PartitionTable array, which is this guy. We just want to see the actual contents of that array, and so there's partition objects. So NTFS we see that this one is bootable. It starts at sector 2048, this is all decimal, and goes through to whatever that sector is 1026047. Okay, so another way to do this same concept is there's Get-ForensicPartitionTable, so kind of skipping past that select statement there and it does just the same thing as the one to show, we'll see, I don't know, Ctrl+I, I think. Question here. Yes. What's the best way to bind to enumerate all of the like physical drive strings? Is there like a WMI class? Yeah, there's a WMI class. I forget what it is, like Win32 physical disk or something like that. Yeah, okay. It's in the help File because I couldn't figure out how… Oh yeah, it's in the help. When I started doing that, I have Jared and I said not all drives are callback, right. I have CND. Yeah, and so to clarify the drive, right now we're looking at the physical disks, CND would be the logical volumes, so we're going to get to that point in a second, but for the master boot record, in particular, we're not dealing with the volumes yet because we're trying to figure out what those are. Okay, so now I want to, I'm going to skip over this one command, but in general what we're going to do here is we're going to, this was just showing, the command above was just showing. We're going to take the bootable partition and we're going to save that to a variable, so that's simple enough. Alright, so that's what we're doing with the master boot record, right, so we had a string of 512 bytes. They're just garbly GIF, we don't know what they were, but then we actually parse them out and so that a human could understand what they were, right Get-ForensicMasterBootRecord.

Reading Volume Boot Record, Master File Table

So now we're looking at this again and we're taking that partition 0 and we're looking at StartSector and we're going to read 512 bytes. So the StartSector is the beginning of the logic, the bootable volume, right, or the C drive in this case. So when we read that, again, not very much stuff. What we do see up here is NTFS, which is a good indicator that we're looking at NTFS volume boot record, but still, not very much stuff to go off of, right. Let's go back to, luckily there's a Get-ForensicVolumeBootRecord cmdlet, and in this case, we're hard passing it C: sorry, C: and then you just run that, and now we have the VolumeBootRecord object, and this is what it looks like. So we have things like total sectors of this volume, the number of hidden sectors, bytes per sector, bytes per cluster, we have the MFTStartIndex, which is going to point us to, and this is relative to the beginning of the actual of volume itself, so the C drive. It's going to point us to the master file table, so we can start understanding what files are on the disk and start doing forensics on those. So just for the sake of needing that later, we're going to store that, and then we're going to go down, we're done with the volume boot record. Okay, now we're back at the master file table, so we're just kind of doing the same thing, you know, expecting the same results, insanity. And so, what we did here, let's go back up, so we're looking at the C volume now, so this is what June was talking about, \\.\C: and then we have vbr.MftStartIndex, then we're multiplying it by the BytesPerCluster, so because the start index is stored in clusters, we want to understand how large a cluster is, and the we're saying hey we want to take the bytes per file record and we want to read one file record, so this is the MFT record for the MFT itself. Somewhere in here, you see $MFT is the name of the MFT file, so the MFT record for the MFT itself is always at offset 0 within the MFT, so we always know it's the first record. And so, within that, let's go back here, we can now interpret that with Get-ForensicFileRecord, right. So now we see hey that's human readable, right. It makes a little more sense. There's some timestamps in there. It tells you who the ParentRecordNumber is, so we see that its parent is RecordNumber 5, which is the root directory. And then we're going to store that for use here in a second. Okay, now I just kind of wanted to show the different properties. So these are proper .NET objects, so we have PowerForensics.Ntfs.FileRecord object and there's tons of different properties more than we're showing, so I have a formatter, which only shows default properties, but you can see a lot more if you ask. So what we're going to do, for instance there's an attribute property, which wasn't shown originally. Let's look at that and this is an array of different attributes. So here's that Standard_Information attribute, the File_Name attribute, which has, whoops, which has the MFT, the name of the file, and then we have this is one of those fragmented files, so we have two data runs, which we have to follow in order to find the data within the MFT file.

Understanding the Data Attribute and the Index Allocation Attribute

And so that ends the Master File Table entry, right. So within that, we want to kind of start understanding these data attributes, right, and so this is that data attribute just kind of showing that we have it there. And so we use Where-Object Name as DATA and then we can go ahead and look at our storage. And then this guy would be kind of a foreach loop, foreach data run in the data run array. I want you to basically store this or write it all out to a file. So Invoke-ForensicDD also allows you to output to a file, in this case, we're just going to write, it's going to go through, in this case, two times and write each time to that file, so let's do that real quick. It takes a little while. (Waiting) Alright, so now we can come back, and so I was writing out to a C:\temp\MFT and if we play that, we see that the MFT has now been written out to disk. So the MFT is a hidden operating system file, which you're theoretically are not allowed to touch, right, you're not allowed to view the data within it. The operating system is doing that behind the scenes. It's locked because there are some changes happening to it all the time. What we just did is we found where the data that is associated with that file resides on disk and we read it and then stored it in a file. So we never touched the MFT, we just touched the data that is associated with the MFT, so kind of a roundabout way there. Alright, so one thing that we've been doing this the hard way. Surprise! There's a bunch of methods that are associated with the File Record object. One of them is copy file, and so we could have or also there's also a GetContent, I think that's what we're showing here. Let's see Ctrl+I. Okay, so now we're just going to GetContent and store that all to the byte variable. And so a lot faster because we're not writing a disks. We don't have to worry about the I/O there. And then I just wanted to kind of show the fact, whoops, oh nope, I put the wrong variable. I changed the variable name and then didn't change it there. Okay, let's see. Alright, so they're all the same size. So that's just kind of showing that kind of three different ways to do or two different ways to do it and we're comparing it, we have the same data, well at least the same size of data anyway. Just trust me this is the same data, I suppose. Alright, so then I just want to store, so previously, we were looking at a single MFT entry, but if we want to just grab the entire MFT, this is the thing that Lee helped me speed up. So we're parsing the entire Master File Table right now for the C drive. It takes about 4 seconds or so, and so just to kind of the wow factor, I guess, there are 305,000 entries in the MFT that we just parsed in 4 seconds. So now we can start going through that. We have all the data stored in the variable there. Alright and then so one thing I want to show is that the index into the MFT array is representative of the index into the MFT for that record itself, so for instance, if we look at MFT5 or the MFT at Index 5, we see record number 5, which is the root directories MFT record. So just kind of a way to quickly go through the MFT there. Alright, so now we have the root MFT record, I just showed that, so we need to start being able to understand how the directory structure is built out so that we can find, I don't want to just go through every single MFT entry to find where like the SAM hive is because that's going to take forever. I'd have to pass all those through the pipeline, 305,000 records that just takes some time. So it's better if we start parsing the directory structure to quickly be able to figure out where like C:\\Windows\System32\Config\SAM is, right, then we only have to technically parse 5ish records. And so I just stored this record into the root variable and so now we can look at the attributes. We want to grab the attribute that's called Index_Allocation, so there it is. It's another non-resident attribute, but this one is contiguous, so just to kind of show there's only one data run meaning all the data is stored sequentially, and so that makes life a little bit easier, I guess, and then we're going to store that. And now we can look at the methods here and again we have this GetBytes method, so we're going to use that to read the contents. And so this is kind of a little hack, I don't actually know why this is necessary, but PowerShell has a weird thing to where sometimes it doesn't know that what you're giving it is an array, so this is just a syntax to say hey I'm forcing you into being an array so that it works cleanly with format hex. So we're going to run that. So basically what we're running is index, which is the Index_Allocation attribute, running the GetBytes method, passing it the \\.\C: so it knows what logically volume to read from, and then we're going to pass the output, which is just a byte array to format hex. And so this is what it looks like. Tons of data. The thing that I kind of want to point out is you have things like Windows, let's see, Users, system, System32, System volume information, these are all records that represent the files that are contained within that root directory, right, so if you parse that out, then you'll start to understand what's going on with the file system. Okay, let's see. And that's the end of that. So at this point, we've talked about how to get from the Master Boot Record all the way down to the MFT and then started to figure out how we can understand what files are residing in what directory, so we can start to kind of find our way around the MFT. Alright. Okay,

Parsing the Directory Structure

So what? What does this all do for me, right? Hey, got another demo. So there's a weird bug that I just found that one of these things doesn't work with the ISE, so we're going to go through the actual shell. So what we're doing here is Get-ForensicChildItem, so functions similarly to Get-ChildItem, but we're doing it in a forensically sound manner, so we're using all that stuff that I just showed you behind the scenes, we're parsing all of it, in this case, we're going to show the children of the C drive. And so in this case, we have, for instance, Program Files, Program Files x86, Users, temp, Windows, but we also have the hidden files that you don't see when you just use Get-ChildItem, right, so we add our $AttrDef, which is record number 4, and so remember we had that $MFT variable that has everything. If we do $MFT and then look at the item adds or the object at Index number 4, we would see the AttrDef file, so we can actually show that real quick. And there it is, so C AttrDef, right. So now we're starting to kind of understand what's going on here. Alright. This is my first time using this little fancy demo thing. I just didn't want to type all this out, so bear with me. Okay, and so now we have Get-ForensicFileRecordIndex and we're giving it a path, right, and it's going to do a lookup and determine what that index is, right, and so, I don't use this very often, but just to kind of show that we can take a path and determine what its index in the MFT is. Alright, and then so yeah, I'm going to kind of speed through this, but Get-ForensicContent is similar to GetContent, but it does it in a forensically sound manner and we're going to basically read the contents and then encode it in Unicode of the AttrDef file, right. Again, this we can kind of make out some strings here, so $ATTRIBUTE_LIST, $STANDARD_INFORMATION, so on and so forth, but there's a lot of stuff going on that we don't know what it is, right? And so luckily, there's Get-ForensicAttrDef and we give it a VolumeName of C and it's going to parse it out and tell us all the information, interpret the information for us. So the AttrDef file contains different attributes that you're able to use within an MFT record, so things like Standard_Information or File_Name or Data, so on, and so forth and there's a bunch of other ones that aren't used very frequently, but they're allowed or defined. Alright, what is this doing? Alright, so now we're going to do Get-ForensicFileRecordIndex for System32\config\SAM, right. So we know the SAM has an offset into the MFT of 278072. Yes. What is the SAM? Yeah, so the SAM is the SAM Hive, it's the registry hive that contains all the information about users and passwords and all that kind of stuff. So I'm going to show in a second that it's a locked file and you can't just randomly go and say hey I want to read the contents of the SAM Hive, but we're going to then do that basically. Alright, so oh this is kind of weirdly out of order, but Lee really liked this, so you can take Get-ChildItem and do like the C drive, so we're listing out all the children in the C drive and then pipe it into Get-ForensicFileRecord, and it will give you the MFT record for each one of those files so it's kind of a cool thing to show how this functions correctly with typical PowerShell cmdlets. Did you have a question? Okay. Alright, so oh I jumped ahead of myself, so right now what we're doing is we want to get the MFT record for the SAM Hive and we're measuring how long that takes. This is following kind of the directory structure, so that we only have to parse out the things that we need to parse and that took 587 ms, so less than a second, about half a second. And then we're going to do the long way, which is basically get every single file record and then we're going to do Where object and it took 5 seconds, so we're talking, I'm not a good math person, but 10 times as long to parse that out. So if you're not actually following the directory structure, then it's going to take much longer to get to where you need to go and this is using like the .where in PowerShell 5, which is significantly faster than where object through the pipeline. Alright, so now what we're going to do is use the built-in Get-Content to try to read the contents of the SAM Hive and not surprisingly, it says hey this is being used by another process, you can't do that. Okay, well if we can't do that, then we'll just do it a different way. And so we'll use Get-ForensicsContent and read the SAM and there's the contents of the SAM hive. Again, this might not mean very much to you, but things like if you're parsing, nk is like the header for the named key property or like the vk is the value key. And so you have these, if you're familiar with the registry structure, you at least see that this is representative of a registry file, so we're in the right track, we're on the right track. So then we can do things like we're trying to copy item. Well hey, same problem, you can't copy it, but what we can do here is Copy-ForensicFile, just kind of showing that concept, and then if I Get-ChildItem, we've copied the SAM out to, whoops I did get ChildItem on the wrong directory. Oh, come on. Oh, no, never mind. So I showing that the SAM file is 65,536 bytes, the copy that we created was 65,536 bytes as well, so we're copying the same size. I would do a hash, but you can't actually hash this in because it's locked, right. So alright, so now building on top of that, we can do something like Get-ForensicRegistryKey right, so now we had that whole blob of registry data, which doesn't mean anything to anybody, but we're able start reading the content. So in this case, we wanted to look in a software hive, right, and this is the root of the software hive, every key in the root of the software hive. Then we can do things like interpreting a specific registry key to understand what it means, so a lot of people are familiar with like the run keys that autostart programs when you set them, and so we have Get-ForensicRunKey, which is going to parse the MFT, determine where the software hive is, or the NTUser.DatHive, which is the HKCU hive or well, there's one, there's a NTUser.DatHive for every user on the system that currently log in users NTUser.DatHive is the HKCU Hive. But we're going to basically go through all of those and find any autoruns set to autostart in the run or run once keys. This takes a little while because it's looking through, oh, never mind, it's looking through like seven different hives to get this data, but here we see, in this case, like I have VMware user process set to autorun, so on and so forth. So now we've taken that and we're starting to actually understand data on the hard drive that we can analyze and determine whether something's going on. Okay, yep so just again showing the fact that everything that's being output is a proper .NET object where we can manipulate house properties and methods, so on and so forth. We also have like, for instance, there's a bunch of timestamps and those timestamps aren't just like strings, they're actually DateTime objects, and so in this case, I'm showing the BornTime of the record for the MFT record for the MFT itself and it's an actual DateTime object that you can manipulate the same way you would manipulate any DateTime object, so that's the end of that one. Alright, so.

Performing a Web Server Investigation

we can look at all this data and start interpreting it, but what's that actually matter if we can't perform an investigation. So let's look at a situation. So there's a guy on Twitter, he goes by BinaryZone and he lives in the Middle East, but he's produced a bunch of forensic challenges to where we has basically, had somebody come in and hack his system, steal data, or do whatever they do, and then he puts them out there on his website, you can download them, and then perform forensics or an investigation, kind of practice your skills, and so we're using one of his images. And so, in this case, it's not technically a live system, but the same concepts would be able to be used against the live system. For the demo purpose, I want it to be static so that stuff wasn't changing and my code wouldn't get jacked up. So in this case, the client does not provide us much information. They believe a web server has been compromised and that's great. That's probably more than you'll really get in real life. And they provide a forensic image. The investigator must find a temporal starting point, so where do we think that the investigation needs to start on a timeline and then determine whether the web server has in fact been compromised, and if compromised, provide leads to incident responders. Whoops, oh that that's the findings. Okay, so going back, that first demo with Start-Demo actually went okay. Let me clear this out here. Alright, so one thing I like to do, so right now it's mounted as H. We're just kind of showing that there's a bunch of stuff in the H drive. One thing I would key in on in this, I didn't have this when I started, but the xampp directory is the like directory that contains like an Apache server, so I might be particularly interested in since the web server might be compromised and I'd be particularly interested in files in that directory. Alright, so we're going to basically store the MFT, there we go. So this is for volume H now, in particular, and then what I'm doing now is I'm storing all MFT records that are in the xampp directory, right. And so now that the MFT is a relatively small file system, but it has 62,400 files, and within xampp, it has 19,473, so we already reduced the scope of our investigation by 50,000 or so, right, so now we're kind of narrowing in because we have, in the case of my VM, we had 305,000 records. I don't want to look through 305,000 records. I want to look through like 100 records at the most, and so we need to start kind of narrowing in to figure out where we're looking. Alright, so one thing we could do is we could take the items, all of the files that are in the xampp directory and we could group them based on their DateTime, right. So when I install an Apache server, a lot of times I'll install everything at the same time and so we can look for files that were installed kind of out of order or at a different time. And in this case, we see that 17,000 of those files were installed on August 23rd and then we have on September 2nd and September 3rd, a bunch of other files that were created, right, and so in this case, I'm going to store those in a variable real quick and now I can start referencing like groups 1 is going to reference all files that were created on September 2nd, in this case. I'm going to sort them by their FMModifiedTime, and so that's the modification time of the MFT record itself, which is one of the more true indicators of when a file is created. And then, so as you can see there's a bunch of session files. So a quick Google will tell you that every time a session is established with that server, a session file is created, right. And like if you look at the times, it depends on if this is like Amazon's web server, but if this is just like an internal web server, people probably aren't connecting to this 200 times in 1 second, right, and so that's kind of strange. Let's see what they did on the third. So we see one last session and then we see stuff like webshells.php and c99.php, phpshell2.php. In the incident response field, if you see webshell, that's probably a really bad thing, right. So let's look at some of that stuff. So we're going to look at the session. This is the session that happened right before all those files were created, and so we see that the username here was admin, which might be interesting, and then we're going to read phpshell2.php, so just for time sake, we're going to look at one of them. There's some weird stuff going on. We see an IP 192.168.56.102, which might be something interesting. We see port 4545. Kind of going on, let's go see what else is happening. So we found an initial pivot. Looks like something happened on the 3rd of September, so we're going to start building kind of a date window around the 3rd of September, so $start now represents the 3rd of September at basically 12:00:00 AM and then we're going to create an endpoint, which is 1 day ahead of start. And so this is really cool like one of the cool things about PowerForensics is that everything that's already in PowerShell, you could leverage, right, so in this case, Get-Date, I'm just building a date window and then I'm able to take the MFT, oh we already have that saved, but apparently we've got it again, and then I'm able to filter down and say hey show me files that were, in this case, modified, oh boy, we've got to hurry up, around that time. And so, in this case, we started with 62,000 and we went down to 22 files, right. That's a lot better. So there were 22 files, which were created on that day. So let's specifically look at those files, and so again, we basically don't see anything super new, but we see that session was created and then we see some temporary internet files that have a reference to that IP address that we saw. We again see webshells so on and so forth. I'm going to kind of speed through because there's some cool stuff at the end. Another thing that we are parsing is the UsnJrnl. So the UsnJrnl is the update sequence number journal, which keeps track of all changes on hard drive. A lot of times this can be leveraged like by backup software to see what files have changed since the last time you backed up or an antivirus to make sure that it doesn't waste time scanning everything and it only _____scans files that were changed since the last time you scanned with antivirus. But this gives us information about every file that was changed, or modified, or deleted, so on and so forth over the past X period of time. Usually, a UsnJrnl will keep about three weeks of information, so we've got to kind of be agile, but we have a little bit of leeway there. Alright, so the UsnJrnl by default had 12,000 and we got it down in that window to 8,000. Usually, it's a much more drastic number, but because this is some sort of group forcing going on, there's a lot of files that are created within our window. Alright, so what we're doing is we're going to look at all the different files. We're grouping by file name, so we see a lot of activity was associated with ServerManager.log, so we might want to look at that at some point. We see these weird name files, tmpudvfh.php. We can see all the phpshell stuff. One thing that I'm interested in because I haven't seen it before is this guy, php3331.tmp, alright. I see that it's, I just happen to know this, I would show this, but we're running short on time. This is the MFT record number for that file, so let's go check it out. Alright, so I just read the contents of that file and its php script that does system and then takes a variable and runs the command, right. Like that's the most simplistic web shell possible, alright. It's just going to run any command that licensed to it. Alright, so this is something that I just added. So there's not a cmdlet associated with it yet because I've been super busy recently, but basically what we're doing is we're taking the Apache access log and we're parsing that out. So we're going to look at every time that somebody tried to connect to the Apache server. And so this is what an Apache access log looks like. We have things like the timestamp, the method that was used in the HTTP method, the status that was returned, you get the referrer, so on and so forth, the username. That one's not a very good example, but. Alright, so now we're going to group them by HttpMethod, and so in this case, post requests or the Post method is how you put data up onto the server, and so I might be interested in looking at what data was added to the server, and so this is all the information. So just down at the bottom, you see c99.php and then act=cmd. That might be worth looking into. Kind of a cleaner way to look at this is, so we're just going to show just that request there, and so instead of getting all of the entire object, we're just looking at the request and again we see like the cmd, which means that somebody might have been executing something on a webshell. So then kind of the crux of the whole thing, so I've been doing all that analysis kind of in a vacuum, but I'm really interested in being able to, oh we're not there yet, sorry. I really want to be able to like look at everything at the same time, right, and so I'm building a timeline that includes all the different inputs, UsnJrnl, MFT, Registry, so on and so forth. So what I do is I, in this case, I stored a file record object for that c99.php and it's just in $record, right, but what I'm able to do is convert to ForensicTimeline and it takes because there's two different timestamps that are up here, it's going to create two different timeline objects representative of each timestamp that's associated and it's going to tell me, in this case, it was modified and changed, in this case it was accessed and born, and so, we can kind of start to get context around what's going on. So now, what we do is get ForensicTimeline. Everything's been building to this point. And so, now we're building a timeline. Right now, it's parsing the registry. It's parsing the MFT of the UsnJrnl, the pre-fetch, something else that I can't think of, the scheduled jobs. I haven't added the Apache access log, but that would be really nice to add into this. And then so like this is just going to show the different source types that we're getting. So in this case, like lnk files, which are those ShellLink or like shortcut files, MFT entries, USNJFNL entries, REGISTRY entries were on this guy, and so now we're going to filter them down on based on that window that we're looking at. Okay just showing how many timeline entries there were 244,000. We filtered it down to 8,731, so that's a really nice thing. And then I was just in New York at a conference and Doug Fink showed his import Excel module and so I thought that was pretty cool. So timelines like 8,700 objects you're not going to be able to just go through like on the console, so it exports Excel if you use the -shell option, it's going to, hopefully I tested this, there we go, it's going to pop up Excel for you and now you can like start going through, you could sort based on the timestamp and then start going through and look at what happened in chronological order and start to build context around that situation. You can see what registry items were changed. So the, I think that's the end of this demo, yeah, okay. So quickly,

Initial Findings

initial findings. There's some sort of brute forcing sqlmap. If you look at the referrer, there's a bunch of sqlmap references. These webshells were created. Probably didn't need to do that. One thing, another thing that I've found that's kind of cool is this gourse is kind of a visualization tool, and so okay I've got to figure out where that thing is, there we go. This is a pain in the butt. Anybody see where my mouse is, alright. Alright, so gourse is a visualization, so basically, I took that timeline and created this visualization to show us files that are being created, changes to the file system, and so as you can see, some stuff is happening down there, right. Something happened that is probably not good. That's that brute force with all the session files being created. And then we see over here in this, we see like webshells.zip and c99.php. It's just kind of a way to visualize the data as it's happening. I've got to find my mouse again here. Hopefully, this works. Oh, man. That's oh well. We're going to do it from here just like this. Okay, so

Future of PowerForensics

the future of PowerForensics basically I want to move into multiple file system support, so why not be able to parse the extended file system on Linux or the hierarchy profile system on Mac or the FAT. I want to start writing parses for like the SQLite databases or ESE databases. WinPE, so Matt Graver wrote a WinPE wrapper module that allowed us to basically create a bootable USB drive that has PowerForensics loaded on it, and so you could go up to a system, plug it in, and boot from that USB drive, and you would have a clean forensic environment that's on a 1 GB USB drive, and so like for instance in my pocket underneath the microphone, but it's literally right there. And then PowerForensics portable, which is the ability to run PowerForensics on a remote system without dropping that DLL to disk, and so I don't know if we're going to be to show this but, it's really a pain in the butt when that messes up there. Okay, almost there. Come on. Oh, man. Alright, so that's skills right there. Okay, so the idea behind this is that there is a function called Get-ForensicFileRecordPortable. I think that's what it says. I'll walk over here so I can see better. Yeah, and so the idea is that this encoded compress file is actually the base 64 compressed encoded version of that DLL. We're loading that into memory and then there's a basically an API for PowerForensics where you can just use the .NET classes. So in this case, you could run Get-ForensicFileRecord on a remote system over like WinRM or PowerShell remoting without ever dropping, without the dependency of that DLL being on the remote system, so you're only touching memory, which is a pretty cool concept. So that's that remoting capability that we're trying to build in there. Yep, and so that's the end of this. Unfortunately, at the end, everything kind of got messed up with the display, but if you have any questions, please feel free to see me afterwards and I'd be happy to kind of demo the portable version or the WinPE or any of those types of things. Thanks, guys.

PowerShell DSC - A Look Under the Hood with Travis Plunk

Introduction

For the PowerShell DSC, A Look under the Hood. My name is Mark Gray. I'm a program manager on the PowerShell team. I've been working on DSC since the beginning essentially, before you guys saw it, so I've been around for a while. I have with me here Travis Plunk. I'm Travis. I'm a senior software engineer on DSC. I've been working from the beginning. I've worked on the engine and the Azure DSC extension for DSC. Alright, so what we're going to talk about today, the format of this first of all is going to be open, you guys ask questions, it's going to be very interactive hopefully. Push the button. Thank you. Jason even came up beforehand and said make sure you push the button. So it should be very interactive, ask questions, interrupt us, let us know. We have four things that we are going to dig into here and then we'll leave it open for questions, so anything you guys want to ask questions about from a DSC perspective, let us know and we'll dig in. If we can do demos with the questions, if not, we'll just answer the questions or what have you. So we're going to talk about a lot of the new features in WMF5, so the encrypted MOF. We're going to dig into what the purpose of those is, what the good things are, what the bad things are, all that kind of stuff. We're going to dig into configuration reuse. Nobody wants to write scripts of the same stuff over and over and over and over again. So in DSC, there are a number of different ways to actually reuse configurations, so we'll kind of dig into those, what the good parts of it is, what the bad parts, some are good for some things and others are good for other things. And then configuration status. I, as Kevin pointed out earlier on, I came from the GP world, so one of my big caveats or one of my big things that I have been pushing for since being on the DSC team was we're not going to build a black box from the PowerShell or from the GP perspective. You had policy and what happened with the policy? I don't know, let's build a tool to figure out what happened with the policy. It's policy so it should have applied, right? You don't know that. So configuration status gives you a good look into what DSC did, what the status of it is, and all that kind of stuff. It's actually a rich set of information in MWF5. So in MWF4, we had some basic information there. We've made that really robust, so it gives you a lot of information, and we'll show you how to dig into that and see all that information. And the last bit is the pull server 2. There have been a lot of sessions already. All day yesterday was on DSC and pull server and all that kind of stuff. We'll touch on some stuff with that, especially as it kind of relates to Devops and things like that. So we'll dig into that a little bit and some things there and then open it up for questions. With that I will hand it over to Travis to talk to you about the Encrypted MOFs.

Encrypted MOFs

Encrypted MOFs. So we got a lot of feedback on previous versions that it was difficult to use certificates to encrypt the MOFs that you were sending to the target node. So one of our goals here was to make it easier to make sure that your MOFs on the target node were encrypted and secure. But the total purpose here is to make sure they're encrypted in transit and at rest on the target node. So we have a couple of use cases securing the MOF in-transit. We have a documentation on that. There's a link in the notes in some of our documentation on our documentation site. So what's new here is there's a new requirement for document encryption EKU and the provider for the certificate must be Microsoft RSA SChannel Cryptographic Provider. That's a very important note because if how many people were using WMF4? How many people moved to WMF5? How many people got broken? There you go, so yeah. So it's a good thing to know if you're moving from WMF4 because we did make a breaking change and that's how you fix it by creating the right kind of certificate. So the big feature here is that we automatically encrypt the entire MOF while it rests on the node once it gets there in WMF5. So is there a big advantage in that type of certificate? Let me answer that. Previously it was possible to generate types of certificates that had tack vectors on them, but if you had a certificate without the proper type of EKU, you could use them in ways that would reveal their private, not their private, their public key. This type of certificate, if you generate it correctly and via the best practice, the public key shouldn't be given away and you shouldn't have the problem of the potential attack against such certain cert. So this was done to increase the security of the encryption. Yeah, I see that. (Audience comment) So how does it work? LCM automatically encrypts the entire MOF using the Data Protection API on the target node. This means non-administrators cannot decrypt the MOF. That's no matter how you generated the MOF. The MOF generated by PowerShell is not encrypted by default. So it's still recommended that if you're using credentials to encrypt the MOF to protect it in transit so that your password is not lying around in the MOF that you sent, that you have lying around before you sent it to the node. So before we go to the demo, do we have any questions? Not where we have the demo. (Typing) So first I'll show you the code if it will come up underneath. So one of the things to point out here as well. We did this work so that we'll encrypt the MOF in place. It doesn't prevent you from the using the old methodology of encrypting the password, so when you use a certificate to encrypt the passwords, that still works, so you can do that and then we'll encrypt the MOF on top of it, so if you really need that extra security of I want my passwords hidden throughout the whole pipeline, you can still do that and that works with this. So. I guess that was my question. Is it necessary then if you're encrypting the MOF? I mean I guess it really just… So we've gotten feedback from customers that there is other things other than the credentials that they want to secure like the location of some of the files they're copying from, so they may want to encrypt the entire thing. Instead of may, we just encrypt the entire thing for you all the time. I know it was the other way, right? If I'm encrypting my MOF, then do I say what you do for my password? You'll see in the demo, but the encryption only happens when the LCM receives it and then writes it to disk, but there is other parts of the process where it's not encrypted. So the MOF created on a pull server itself you're using would remain in under _____? Is that correct? That's correct. And even you encrypt the password, it only encrypts the passwords. Anything else in your MOF generated by PowerShell is not encrypted so like Path SMB shares are not encrypted. And in Azure automation DSC if you're using that, it is encrypted on the service by itself, so when you generate the MOF, they encrypt it, and then save it to disk. And when it's on disk, it's encrypted and then they decrypt it, send it to the target node, and then it gets re-encrypted over there. That's exactly what I was going to ask. Does that mean you no longer need to set the plaintext password true variable under 5 or is that still needed in order to… That is still needed. We are doing some work with Azure automation to make it so you don't have to do it there because they do encrypt it at rest. So I have a very simple configuration. I have a MetaConfiguration here that sets the thumbprint. I have the thumbprint already in a global variable. Then I have my configuration and I use XPSDesiredState and DesiredState, so I'm importing them. Then I create a user and I create a script where I'm not running as that user and I'm going to verbose output the name of that user and then I'm going to run as the user that was just created and output the name of the user and I have a function here to generate the configuration data, pass in the thumbprints that we're encrypted with the same thumbprint. I have a comment here that there's an alternative way to encrypt using a certificate file. I'm not going to use that or demo it, but that's true, and I'm also telling DSC that I AllowDomainUsers. This is the local user, but it's still, we'll deal with that later. So I'll go ahead and import that module. I'm going to run the MetaConfig, apply the MetaConfig, This tells DSC to use the certificate to decrypt any MOFs edit users. Now let's go look at the MetaConfig, and if you see here, I'm going to have to find it. Give me a second here. Where is the… Here CertificateID. We have the CertificateID. We have everything else should be defaults. So we'll move on, unless there's any questions. The now I'm going to prompt for that password for that user we're creating. I don't know why I'm being so strict. The password doesn't matter. So if you notice here, we're using a new type of encryption called CMS. There are cmdlets for it and I'm using the password twice, so it's in here twice. But as I said, everything else is unencrypted. All the code to get the user is unencrypted. So we've already had questions about that, so just showing you that. And then I'll run the configuration. And so we created the user, we ran not as the user, so DSC runs as local system by default, and then we ran as a test user. I'm going to go and get the user, get the configuration. I also, so this basically just shows the same thing. When we do get, we're running as the same users. I'm going to delete the user just so that the user's not on the system. Now we can go look at the MOF. We don't see anything because it's all encrypted and that's that demo. Any questions? Are you saying you could have just made the file by hitting your hand and on the keyboard? Now where's the fun in that? Then the challenge to you would be to decrypt it after.

Configuration Reuse

Alright, so I didn't ask this earlier on. How many of you have been working with Desired State Configuration? Raise your hands. Alright, about half the room maybe. Did you have a question? Yeah. I was curious. So is there a place that we could go for documentation on how to do that sort of if we got away with any of that. There was a link in the slide deck. Are we giving out the slide deck? Yeah, if you can let me help them at the end, I'll make sure to get those to you. Okay. Alright, so I'm going to, we're going to be really short on time here, so I'm going to breeze through some of this stuff. I'm going to go over the configuration reuse. There are basically two use cases, I think, primary use cases for reuse. There is reuse-like functions, so I have a configuration. I'm going to do some stuff multiple times in that configuration, so I want to have a chunk of something that I don't have to write out a bunch of times with for loops in it and all this complicated logic. So there's that use case and then there's the other use case where I have a distributed environment. I have someone that knows SQL. They're creating a configuration that I want to use for my web app or IIS or whatever where I'm not authoring that thing, but I want to reuse it so that I don't have to write the same code again, so there are a couple of different ways that you can reuse configurations and I'm sure there are way more than that, but these are two kind of bigger ones and I'll go through some examples of both of those. And the options that we have for configuration reuse or composite configurations, I don't know if we've done any documentation on this, but composite configurations are basically just using a configuration like a function. So in your script, you say I have this configuration, now I want to reuse that configuration in the configuration below that I use. It's not packaging it up in any special way. There's no versioning or anything like that. It's just I have this code, so a good example of that is before we add the new service set and those new resources that we put out there, if I want to make sure that 20 of my services are turned off in DSC, I don't want to, well for optimization purposes, I probably don't want to have a big configuration that has service, service, service, service, service, so I can create a function or a configuration that is above my other configuration that says I have a for loop, I'm going to pass in all of the resources that I want to or all of the services I want to shut off, it's going to loop through those, create all of the individual resources, and then dump that out in a MOF, so you can create that above your configuration and then call that within your configuration just like another resource. So it allows you to do stuff like that very easily. There are partial configurations, which I think a lot of people jump onto right away when they have a distributed environment that says this guy's going to configure my system this way, this guy's going to configure my system that way. I want to take those things and jam them together on a target node. There are a lot of good things about partial configurations, but there are lot of or there are other things that aren't so good about partial configurations like partial configurations you need to set the MetaConfiguration and say this is how my system is going to look with these chunks of partial configuration. There's no versioning for partial configuration. It's just whatever if my configuration changed, I'm going to apply that and there's what else with partial configs. (Audience comment) Thank you. That was my other point. Yes, the other thing is, so it's going to take all those configurations and doing DSC it's going to say are there any conflicts with these configurations? But it's not going to do it when you compile the scripts. It's going to do it on a target node after it has all the configuration parts. It's going to jam them together and then go something failed, fix it, and you have to figure out what that is. That's one of the negatives to that. Then one of the positives to it is I can be a security guy and I don't care who's out there, I want this configuration on that target node and I want it to apply. So it takes, puts the power in someone's hands who need to make sure that this configuration applies on that system. And the last option is composite resources. It's very much like composite configurations, except you bundle the thing up into a module and say I want to be able to share this with a bunch of different people, so it looks just like a PowerShell module, it just has resources in it that are able to be reused. You can have parameters that you can pass in and do the configuration and stuff like that. One of the key things to know about composite resources is it looks and behaves very much like a resource, but it's different in that if you're using pull, those resources only need to live on the build server. They do not need to live anywhere else. You don't need to put them on the pull server. They do not need to be on the target node because they get exploded before the MOF is generated and turned into those underlying resources and those underlying resources are what need to be on the pull server and on the target nodes. So if you have logic in there that's trying to get information from the target node, it's not going to work because that's never going to be run on the target node. Alright, I have a quick demo here of the different examples. So right here, I have a composite configuration. Can everyone see that okay? Alright on a composite configuration, this is very much like the example that I mentioned. I have an Iis configuration up here and then I three other configurations for configuring WebApps. I'm going to do the exact same thing when I configure each of those WebApps and I'm going to stand up Iis and I'm going to open up a firewall port, so I create a configuration up above here that has a single parameter that's a port parameter, and I'm going to install, make sure the web server role is enabled, and I'm going to open up a firewall port on that system. And then I can call this thing in each of these things just like another resource. So this configuration is Iis. That becomes the type like any other resource in here. So I'd say this type of resource is Iis. You know what it is because I had to find it up above in this script and I'm going to give the unique name of basic to this thing and the one parameter that I have in that configuration is what I pass into the property here and then do that configuration. So I can get each of these WebApps configured very easily, cleanly, someone looks at this, they can know I'm standing up Iis on this system. I can name this thing whatever I want and I'm going to open up Port 80 on this one, I'm going to open up Port 880 on this one and another port on the other one. And so, it gives you that reusability and when the MOF is created for each of these things and I can show you that here real quickly, in the composite config. That App1 MOF, one of the things, so a couple things here, the name of the ResourceID here gets jammed together so when you create composite resources or composite configurations, in the MOF, it generates the unique name for that resource by saying this is the type that I have :: and then this is it. So basically it takes the name of the configuration and then the name of the configuration you're in and puts them together with ::, so if you have multiple layers there, you'll know where that thing came from. So it gives you the ability like in Azure automation DSC, they're going to be able to combine those things together on their porting, so you can say where'd my failure happen and dig down into the individual resources from there, so it gives you that unique name for that. And when you do a get status, it'll have those names in there as well, so you don't have to dig into the MOF to see that. But you get your configuration right there. So the next one, real quickly, and again, if anyone has questions at any point, just yell. How many levels deep do you go in? We don't limit how many levels deep you can go. (Audience comment) It'll keep going. Do word wrap and then you'll be able to see it. Alright, so the next example here real quickly is the composite resource and now you'll start to see kind of these are used in very much a same way as a composite configuration, but I said you'd bundle that thing up into a module and this example right here is a module that I created, so this is a Fabricam_Infra, so I have common stuff that I want to use here. I have a module directory. I have the module psd1 here that defines the thing. I have DSCResources inside of that and with the new versioning and stuff like that, you have a version folder in here as well. So inside of the DSCResources, I have for each of my individual composite resources, I have a folder for that and then I have a psd1 for that, which doesn't do a whole lot, but that needs to be there to define that, and then I have a file, which is just my logic that is IIS, so it's a name of the resource, .schema.psm1 and then you have the same code in there that you would have in your configuration above that. So I have that for IIS, I have one for Security, and then I have one for SQL. I don't actually use the one for SQL on my example. But then I have my psd1s here. So the difference that you'll see here is that importing that new module and then I can use that just like I could the composite configurations, but I have my Iis Basic, pass in my port, and I'm good to go. So when I compile these things, you'll actually see that they look practically identical when you come down to the resources themselves. They have the same naming, so it squishes the two together. It says here's my composite resource. I'm going to squish that name together with how it was used in my configuration, so you can uniquely identify that, and otherwise, it looks exactly the same. (Audience question) Yes, I can. So I go to SQL. Go ahead. So one of the things that happens in here that is nice about composite resources or composite configurations is you can do logic in here. Like this one I have a validation set and a version, so if you have different things that you want to do depending on the version of SQL you want to install, you can do that logic in here. And did I delete that? Oh no. So I have a switch statement says this is where I'm going to get the install for each of these different things and then I just use that in here somewhere in the source thing. So it's a source and I use a source in here, so it's going to install in a different place, so you can do that kind of logic in… (Audience question) Excuse me? (Audience question) It really depends. I will show you an example after this demo real quick on some of the pluses and minuses to each one of them. But it depends on your use case and how you want it to work. So the question was in a distributed environment, which should be used, composite resources or partial configurations? I have a quick question. Sorry. So I find that using the PS Desired State Configuration and an XPS Desired State Configuration kind of confusing. Yes. They're not the same thing, right? Yes, well they're not. So it's like I think I'm putting on the new versions, but I find that I have to call the old one because that's where the cmdlets are, I guess, and I get the warning sometimes I don't, and so what's the difference, I guess. How do I… So the idea with the XPS Desired State Configuration was essentially the things that we thought that we might pull back into the box, we would put in that module, but that has not happened and I don't know if it will happen. So should we call it something else? I mean we can, I don't know. It's a module. You can rename it and call it whatever you want, not to be flippant, but it's potentially. There are a few different things that we're looking into and one of the things is should we open source all of the PSDSC and the stuff that's on the box, shouldn't we just put that up on the gallery and let that be there. So it's, and if we did that, we would definitely squish the two together and make that one. But we are, in general, investigating naming of the modules that are from the community in XDSC. Desired State Configuration is one from the community. Alright. Sorry, (Audience question). Will what work with class base? (Audience question) You composite, yes they will. Composite resources are actually much simpler in class based. I probably should have had an example in here, but I don't. With the class-based resources, you just use a class syntax and then you can have multiple in one module. It actually simplifies it quite a bit. But then you can't do versions of the pull server right? For, that's correct yeah. Alright, so partial configs here real quickly. So I have the same set up with partial configs. When I run each of these, it's going to generate a MOF for each of them and then basically on the target nodes, you have to come in and say I have partial configs for each of these things and in a case where you're pulling, in this case I'm pushing, so I've defined a MetaConfiguration that says I'm going to push each of these configurations to the target node. You define the partial configs that are going to be on that box, give it a description and then when you push the target or the partial config there, it'll get it, jam them together, and then apply it at that point. So each of these will generate a single MOF like you see here in my Partial directory. So it'll just generate a MOF for each of these and then you send those to whatever machines should apply them. Go ahead. Will it wait until it has a configuration for every partial defined before it compiles and does the final jamming or if you only have two submit right now and two are coming later, will it submit the two it has and then allow you to wait for another one. That is a very good question. The question was about how it applies partial configurations. We allow you to define that in the MetaConfiguration. So in the MetaConfiguration, if you have this partial config depends on another partial config, and that's not tab completing, but if you have it dependsOn with another partial config, it'll wait until that other partial config gets there to apply it, and if not, it should just apply it. Well it's not just going to go because you have to, well with pulling it will, with pushing, you push it, and then you have to apply it. So alright, so now I will, I went through those. I'll jump back to my presentation here.

Partial Configurations in Delivery Pipeline

So like I mentioned before, there's some plus and minuses to both composite resources and partial configurations and this is taking a look at what it looks like in kind of a DevOps environment where you're deploying to a pull server and you're going through dev, test, prod, and what have you. In this example, I have the security, SQL, and IIS, partial configurations in a web app that actually depends on those things to actually work. And in a continuous integration deployment kind of environment, you go through and I the security guy create my partial configuration, I deploy it, it's out on a pull server. If there are any nodes that need it, aside from that DependsOn, they're going to pull it down and start applying those things. Assuming you have DependsOn correctly done between all of these, it's going to wait until it has all of them to apply them. So each of those partial configs go out to release. They're out there sitting on the pull server. I deploy my web application. It goes out there. A target node pulls it down, applies it, everything's happy, everything's good. And then my security guy comes along and says I'm going to create version two of this thing and this is a little misleading. There are version numbers on this slide, version numbers don't exist on partial configs like I mentioned before, so when you have dependencies and stuff like that, if you push a new version of this thing, the security to your pull server, the client's going to say, is there a configuration that has changed with this? Yep. Alright I'm going to pull that thing down and apply it. So in this case, I go through my process of, or I the security owner say I'm going to go through my process of build, test, and release, and then push that thing out to the pull server. In the meantime, Mr. WebApp 1 has never done any testing on that. So unless the security guy knows WebApp 1, all of these things are above me, but depend on me, they're going to get affected by this thing when I push that thing out to the pull server. So in this case, you're going to have this WebApp that has a new setting that's being applied that could affect it and you have no idea that happened. So that's one of the things, and in some cases, if I'm the security guy, I don't care if I break WebApp 1. If I have a security issue, I want that thing out there. So there are some situations where you may want to do that kind of thing, but other situations where you're going to be like whoa, I don't want to know about everybody in the world that's going to use my particular configuration here. So that's where you get into the world where you have composite resources. So I have a composite resource, same kind of situation, these three composite resources, I'm using them as the WebApp guy, so they are all in my WebApp. I'm using them. All things are good. That WebApp that I own that I'm using those things go through the dev, test, prod out there, everything's working good and the security guy comes along and goes I'm going to make this change to the security configuration, that doesn't affect my app at all. Nothing happened because this happens, these get pulled together at compile time. So when I'm ready to release my new version of my app, I do my new version of my app. It's reusing the new versions of those composite resources, I go through my test production and deploy that thing and everything's happening. So if you don't know about all of the up services that are making use of your resources, then composite resources would kind of play into that a bit better, so there are different situations where you want to do the different things. Does that make sense? Alright, I see some heads nodding, other one's yawning and sleeping, but head _____. Is it the plan to take one path moving forward or are you going to continue to--- No. Like I said, I think there are good circumstances for using either one. Like the security example is a good example. If you're the security guy, you want to make sure the settings are set a particular way and if it breaks somebody, then they're going to come back to you and yell and you're going to tell them what they're doing wrong. But if you determine that you need to support both of these methods in order to support those different scenarios, whereas you could recode and support all those scenarios in one feature. Okay. That's sort of a question. Yep. Okay, it's a feature request. Yes, it is. Put it on UserVoice. Okay, Get-ConfigurationStatus, Get-DscConfigurationStatus. We had some feedback from the previous release that it wasn't easy to determine the status of previous configuration runs. So the purpose of this is to get the status of previous configuration operations. A couple of use cases. If you call it with no parameters, it'll just get the status of the last operation. You can add the all flag and get the status of all logged operations and you can add the CimSession and get remote operations with the previous two combinations. I've also updated our public module of xDscDiagnostics and provided access to historical verbose output. This is not a feature DSC provides, but the information is there and it works a significant percentage of the time. I'm not going to make for promises that I can't live up to. It doesn't work all the time, but it's useful. So let me start my demo. So first, I'll just show you the all. You can get that it'll just show one, but not all. So I've had several runs and the first one I'll get is I'll go get the first initial attempt and get it in the status object and I'll show you what that looks like. So my first initial attempt was success with three resources, but there's more details in the status object, several rich objects. I also, like you see here, there's many different types, Consistency, Initial, Reboot, LocalConfiguration. I'm not going to go through all of them. I'll go through a couple during the demo. But let's look at all the settings. We have MetaConfiguration here, so each time it runs, we record what the MetaConfiguration is, so you can go back and see maybe it didn't run because the MetaConfiguration was wrong and we have the resource if this is a success, so we only have resources in Desired State. So let's look at the rich object MetaConfiguration and we have, I was pushing to the machine, everything's pretty much the default state here. Let's go look at those resources in the Desired State. So I had three things here, the testUser, DontRunAs, and the RunAs. This is the demo I did earlier for you. So let's look, we have a JobId. This is how we track, when you send us an operation, we track it by a JobId. So now I'm going to run that cmdlet that I added and we'll get the details. So we have all the verbose output. This gets it from a JSON file. I ran it with--- Question, Travis. Yeah? I had a quick question on how is that historical information attached? Like is there any information on how far back, even a limitation on how far back in time you can go? Yeah, I believe it's set in the MetaConfig StatusRetentionTimeInDays. It's cache. I'll get to it later in the demo on where it's cache. I believe I will, unless I removed it. So I'll go through a reboot. We store this for reboot operations as well. So I didn't show you the status object, but the status object is pretty much the same, so we have the verbose or an operation that counted as a reboot. So this was something you couldn't get before. And yeah, here like this all of this information is stored in this directory in here under System32\Configuration\ConfigurationStatus. We have MOFs that store what the end box cmdlet is giving you and JSON files, which store that verbose output. And if you notice, not all the MOFs here have the verbose, so if they don't have the JSON file, we don't have the verbose output for those operations. So here's a failure just to get something interesting. We have resources not in Desired State, so these objects are pretty rich. We have the exception details with StackTraces, the full serialization of the exception object. So you can go back and figure out why this failed, what line it failed on, etc. Here I just did a throw, so it's not that interesting and it was in a ScriptBlock, so it doesn't actually have a line number. But that's ConfigurationStatus. Alright, so next I'll give

Pull Server V2

you guys a choice here. So next I have a few things with the V2 pull server. There are a bunch of sessions on the pull server including all day yesterday and thing like that that's going on, so I can talk about this. There is one thing that I want to show, but if there are questions that you guys have, that you guys want to ask and see how we can answer them, I'll do that. Otherwise, I'll go into this and kind of talk about that one thing that I want to talk about. Is there a question over here somewhere? Just one question. Yep. You mentioned a DSC is going to encrypt on a target. Are there any plans to move that into pull server if we don't want to use any DSC? Potentially, but no commitment right now. (Audience comment) What's that? What about the data source, so we can do that? There is a UserVoice for that already, so go plus one that if you want to. Yeah. Will the DscConfigurationStatus do have something that's actually imported to a compliance server… So in the version that comes with WMF5 and is with 2016, all of that data that Travis just showed you gets sent up to the pull server, so that's one of the things that I'll show you here real quickly, but we don't do a good or we don't do a great job of giving you access to that data. It's all stored there. It's in the database. But we're not going to tell you how to get to it, but we kind of tell you how to get to it. So I will actually jump down. There are a bunch of questions here that we've heard from a bunch of different people, but I'm going to kind of because of time jump to my demo here real quickly. So I'll touch on a couple things with a pull server real quickly just because they're here in my face. So with the pull server, the best way to configure the pull server as JSON and others have said who's used the resource, we have x resource out there. Use that to do the configuration and don't try to do it on your own because it'll take forever. This resource makes it much easier and JSON's script actually makes it even better and work better. So use the configurations that are out there to do the configurations of a pull server, your life will be a lot better. The TargetNodes itself. That's on GitHub you're saying? What's that? The ones that are on GitHub? Yeah, and the ones that are on gallery, so as part of the xPsDesiredStateConfiguration, there's a x webservice, I think it's called, use that to do the configuration of the pull server. So one of the other things to keep in mind if you're using the new functionality in the pull server where you can say I don't want to use a configuration Id, I want you to say this is the name of the configuration I want the TargetNode to get. The name is only used in this, yes, there's a MetaConfig. So you can, in the MetaConfig, say this is the name that I want. That is only used at registration time. It's never used after that again. Whatever that node is registered with the server with, will be used to say this is the configuration that the TargetNodes get. The idea there is that you should be able to go to the pull server and say I want to change that and not have to touch the client. The client should just get the new configuration and apply that. So the RegistrationKey, there may be a bug with that, according to Travis, but the RegistrationKey, once registration is done, gets deleted, and if you put it back, that's the trigger to the LCM to say hey I need to register again. So with the current implementation that we have, the way to actually change what's in the database on the pull server is to go to the node, put the RegistrationKey back in and change what the configuration name is, and then that node will register to get a different configuration. In Azure Automation, you don't need to do any of that. You don't need to have this configuration name at all. You can just have the RegistrationKey in here. It registers with Azure Automation DSC and then in Azure Automation DSC you can say I want this system now to get this configuration or what have you and it'll just get that configuration, so it makes that a bit easier. And if I have my way, we'll make that easier with the on-prem pull server as well. So the next thing is these resources here define where to go get the configuration, so this one is where to get the actual configuration. This one is where to get the resources. This one is where we report the status, all that rich status data. You don't need this one. If you don't define this, wherever it gets the configurations, it's going to go try and get resources if it needs those. You do need, although you shouldn't, you do need this one to tell it where to report. The idea with these is to give you the flexibility to say I want to get configs from here, resources from here, and report over there, but right now as it stands, the minimum that you need to configure pull is the configuration repository and the web repository or report web repository. Can you expand on the required resource real quick? Yep. They look identical. So each of the places where you go to get resources and configurations and pull, you need to register if you're using the new registration or the new configuration name method. And the thing that tells the LCM whether to use the configuration name or the configuration Id is the configuration Id. If you have the configuration Id set in the settings, it's going to use the old way of doing it. If you don't have it there, it's going to use the new way of doing it. Clear as mud? Yes. Is that part of the reason that the configuration Id isn't wired when you're doing the partial config in pull? No. That's a bug. That's a different thing? Yes. And I'm angry about that bug. Alright, so the next thing that I want to show you is I told you that the reporting is not exactly easy to get to, but we do provide a way of getting to the reporting data. So I will cruise through here real quickly, but I basically what I have here is just the script that's going to go through how to access the APIs that we provide that give you access to the reporting data. It is not something that you're going to necessarily build tooling on top of, but it does give you the access to the reporting data, and the reason I say that it's not going to be something you're going to build tooling on top of is we have key in the APIs that we call that is the AgentId, which is essentially the Id that is generated either the ConfigurationId if you're using that method or the AgentId that's generated by the LCM itself. You have to pass that in to get the reporting data for that node. So yeah, basically if you have a bunch of nodes, you have to iterate through each of those things with the known AgentId. And the reason. Excuse me? Is this documented? It is documented, yes, and I will put these scripts and stuff like that up on GitHub and we'll share them soon. So basically, what I'm doing here is I created a couple of functions and those functions just basically allow me to parse the JSON that is returned. So this pulls the generic JSON data out and this pulls the status data out of that included JSON. So I will make sure that these things are in memory here and they're opening doors now. Alright, so I will run that. Oops, I need to do that over here. So I will do that here. I will F8 that, that'll run, and I will skip over these two. These two basically just pull up and show the schema for the pull server, so you can actually, if you know anything about code data, you can dig into that and figure out how to actually do the queries and stuff. I'll just do a couple of the queries. You can get the node information about the node that is configured on that system. So it's just an Invoke-WebRequest and we have the pull server, we have a nodes location, you give it the AgentId, and then you get Agent information and that'll pull the information about the agent. We have the configuration names and then the primary one is the reports, so you can pull the reports down for that node. So I'll actually jump to that, so you can at least see that. So a couple things here. I'll run that guy, then you can see the reports that get generated from that, so this basically just took the last one and people are lining up already. So this will take the last one. Basically, by default, we return all of the reporting data for a node when you use this query, so if you have a month's worth of data, we're going to return it all when you call that. We give you a way through OData to filter that out, so you can say I only want to get, in this particular case, I only want to get this high level data without all of the bloat of all the reporting data, so I can dig through that and say I care about this one job. So in this particular case, I have, I get the data and I'm just getting these particular columns, so I'm not getting all of the big status data that Travis was showing you. I can figure out, I care about this particular job. You can get this JobId and then you can call back into the API with that and get the original data from that. So we do give you the tools to do it. It's a bit manual right now, but we do allow you to do that right now. (Audience question) I don't know. That's a good question and I will look into it whether you can use the OData tools to actually generated cmdlets for this. Is there any other questions? We'll be up here for a few minutes, but thank you very much.

Real World Test Driven Development with Pester

Introduction

So welcome to Real World Test-Driven Development with Pester. My name is June Blender. I'm a technology evangelist at SAPIEN Technologies in Napa, California. I spent some time writing help with the PowerShell team and I'm now honored to a Windows PowerShell MVP. Thank you. So this is really different than most of the talks that I give, which tend to be sort of I know all about this. Here's the syntax, here is how you do this, here is some examples, right. Instead, I'm going to talk about my experiences learning Pester, which is really a different view and I hope that this will help those of you who don't know Pester to learn it and those of you who are teaching Pester to have some insight into what it's like to learn it and this is real world stuff. So I tested the code and information on the slides to these versions of Windows and these versions of PowerShell and particularly Pester 3.4.0. When I started putting together this talk it was Pester 2 and they're ready to release Pester 4 now. The versions will vary, so this one is 3.4.0. So we talked a little bit in the pre-session about why test and I'm going to call it moot. Alright, I'm going to say that folks who are here understand why we test, but I'm always happy to entertain discussions. But for a while, even those who wanted to test did not have a professional framework to do so. And then along came Pester. And Pester is really an extraordinary contribution to PowerShell because it's a professional test environment that's built as part of the academic discipline of code testing. The people who built it really knew what they were doing and rather than developing some helter skelter test system, they built on the academic discipline of test-driven development and it's a huge contribution on, when you learn the domain-specific language in pester and by domain, I don't mean like domain controller, right, I mean the academic discipline of testing, and you go to another language and look at its test framework, it will look very similar and familiar to you. So when you see the folks who contribute to Pester, thank them, buy them a beer, right. It's really an extraordinary contribution. So it says up there that Pester is a BDD style testing framework, so let's talk about that a bit. Test-Driven development is decades old. It is certainly not as old as programming, but people like Kent Beck and others decided that the way to approach testing is to write the test before writing the code and this was pretty revolutionary, okay. They said that you write a test, you write only the code that you need to pass the test, and then you go onto the next test. The first time you test that code, run the test, it should fail because there's no code, right. But you write only the code that you need to pass each test. And in theory, the code is simpler because you've only written enough code to pass the test, everything is tested, sort of a priori, and the developer can be a little more bold because they don't have to worry about regression errors. They can put in a new feature and if they accidentally break something, the test will pick it up. Behavior-driven development grew out of test-driven development and it's the theory that you write tests that describe the behavior, not the implementation of your code, right. So that way, you can change the implementation and do novel things without having to rewrite your test suite. It is a really hard thing to do in practice. I failed. You'll see. But in addition, your tests should be independent of the environment in which they were written, right, it doesn't have to, you know the it works on my machine is not a good excuse, right, and they're also independent of each other. You're testing the code segments in isolation. You should test all of the features and you ship your tests. Your tests become part of your code. But that didn't answer the question of which tests do I write? Right, I'm supposed to write my test before my code. How do I figure out which tests to write? Yet, a lot of the tests that I've seen are so easy, right, they're so simple to pass that of course the code looks great when you test it and then hand it to a real user and it breaks all over the place, right, so there's a real art in writing tests. And it was about this time last year that I heard Dave Wyatt's talk about Pester and this was the first question that I hit that I could not solve. So this is where this talk will start.

Help-driven Development

So what I did was to do what Adam Platt calls help-driven development. I started with my help file, right. I used my help file as my spec for my code. Okay. It's just an easy way to do it and it means that I always have a spec. But what I did was I realized that my help examples are my contract with the user about the behavior of my code, right, and by code I mean scripts or modules. Okay. And when I make that behavioral contract, I as part of that behavioral contract, when I write my examples, I show sample output. That's my expected in my test. We're comparing actual to expected. So I show an example of running my code and showing expected output. So it's very easy to use the help examples as the test spec and the test as the code spec, of course, it supports behavior-driven development and there's a feedback loop because it makes my help better. So for this, I picked what I thought was the simplest possible little example. I used Windows PowerShell profiles, but when I'm testing or doing demos or writing shared code, I disable them because I don't want something in my profile to affect my test or my code. So I had little snippets of code all over the place that disabled my profiles and re-enabled them and then would get the disabled profile, so I didn't freak out and that's the little module we're going to be testing with. So I actually started with a help file. Let me show this to you. We're going to switch around here. Okay. This is PowerShell HelpWriter. I showed it last year. And I started with a help file for a Get-Profile, Disable-Profile, and Enable-Profile cmdlets and they all have these nice examples. Okay, and then I went over here to PowerShell Studio. Do I need to use zoom it, can you guys see? Okay. And I did New, New Module from Help File, I went to my Help Files, and there's my module. Okay. And so what PowerShell Studio did for me was it just mocked up little functions for my help file. Okay. It used the output type because I was careful to specify that and it gave me a little export module member statement at the end and a module manifest, and quite soon, it will also give you a test file. Okay. So that's basically where I started.

Pester Syntax

Then I create a test for every example. Now I'm going to take you through about seven minutes of syntax. Are we good so far? Everybody happy? Okay. I'm going to take you through about seven minutes of syntax where we're going to discuss the Pester syntax and its natural language. You've often heard these called DSL, or Domain Specific Language, right. It's just its natural language for comparing actual to expected values. So my hint to those of you who are learning Pester is to either learn the syntax if that works for you and forget it or don't even learn the syntax, just go with the natural language. Okay and I threw, these slides go up on GitHub, and they're available to use, so I threw in a little glossary for those of you who are unfamiliar with the terms. They're not defined in help files. Okay. So Describe is a container, okay, and I consider it the naming container. If you look up in the upper right-hand corner, there's going to be a little breadcrumb trail and that's a good thing to watch for. You'll see how these pieces interact with each other. Okay. So describe, I considered describe to be the naming container because when you run Invoke-Pester to run your tests, there are TestName and Tag parameters and they always refer to the test name and the tag on the Describe block, right. There are other containers in Pester and they have names. The names are always required and they are almost always arbitrary, but when you use TestName and Invoke-Pester, it only looks at the names of the Describe blocks, so it's my naming container. Oh, it creates a scope for both mocking and TestDrive and we'll talk about those in a little more detail, and again, the name is required and arbitrary. One more thing and this gets everybody, see the curly brace that's on the same line as the Describe keyword, that curly brace, regardless of your favorite curly brace style, is required to be on the same line as Pester because that ScriptBlock is the value of a positional parameter, right. So if you did put it on the next line, you'd have to use that awful back tick, right, hopefully without a space following it, right. Okay. But there's a wonderful error message and I think when you see it, remind yourself of the value of excellent error messages. There's an error message in Pester. If it can't find the ScriptBlock, it said did you put the curly brace on the next line. Okay. It's great. It's wonderful. The other thing is that if you omit the required name parameter, it assumes that this script, well PowerShell, so it's looking for a string followed by a ScriptBlock, these are all positional parameters, that's how it does the natural language, so if you happen to omit the name, PowerShell converts the ScriptBlock to a string and then it can't find the ScriptBlock. Okay. So if you omit the name, you'll get one of those errors that said did you put your curly brace on the next line, right, even if you didn't. Context is also a container. Look, keep your eye on the breadcrumbs. It's a container inside Describe. It's an optional container. You don't need to use this one, right. They added it for organization, but it's very important because context is the smallest container that creates a mocking scope. There are smaller containers, but they will not contain a mock in Pester 3.4.0. Okay. The name again is required and arbitrary. The next container is It. It is a required container inside either describe or context. It encloses your tests. It is the smallest test container, but it does not contain mocks. It does not create a mocking scope. It also has a name that's required and arbitrary. Should is not a container. Should is actually a function. It compares actual to expected values. You always pipe to it and it does that comparison based on the value of the B operator that follows it, so this is a very typical should format. And these are all the wonderful comparison operators. Okay. They change, so keep an eye on them. Not all of them are documented. I noticed last night, BeOfType, which is one that I use frequently is not documented, but I owe those guys some favors, so I'll probably throw that in. These are fantastic. Learn these and use them. They will help you in your tests. Oh, and then mock, which is my absolute favorite part of Pester. It lets you fake the input to the cmdlet that you're testing, right. It is absolutely invaluable. I heard some people say that they're using Pester, but they're not using mocking. This is the way that you make your code independent of your environment and independent of code that it interacts with. Okay, and what it does is it fakes the input to the command that you're testing. Okay, so the command that you mock, does not run. And instead, you provide the fake output so that when the command that you're testing runs, it gets the output that you specify instead of running that command. So you do not mock the command that you're testing, right, because it won't run. You mock the command that provides input to your test command. So in this case, you see my cursor? Good, okay. So in this case, we're testing new user, right, but we're mocking a helper function that it calls, Get-UserType. Okay, and we're going to mock it with admin so that when New-User creates its object, it gets admin. I could put anything in there and one of the things I'd want to test is all the valid values and at least one invalid value. Okay. Let's see, oh and in unit test, you mock all calls to other cmdlets, that's the isolation feature. Oh, we're doing pretty well on time. Good. Mock does not scope to It blocks. Okay. So the mock that's done the mock of Get-ChildItem in the first It block, also affects the output in the second one. The smallest mocking context is, oh sorry, the smallest mocking scope is context. Okay. Boy, did I learn that the hard way. And then TestDrive, which creates basically a virtual file system just for tests. This is something that the Pester team could've made us do as testers, but instead, they handled it themselves and it was a brilliant decision. Here's how it works. Whenever I answer a Describe block, I get a brand new test file system. That's just for my tests. I can write files in it, I can read files from it, I can change them without affecting the file system on my machine, and more importantly, without being dependent on the contents of my personal file system so that it works on my machine. Okay, and what's really going on is that it's creating a temporary drive in ENV temp, right. But so you refer to TestDrive by as TestDrive: like C: right, always with a colon or the TestDrive variable holds the fully-qualified path to the current TestDrive. Okay and you'll see me using this when I create paths. You answer a Describe block, you get a brand new TestDrive, you leave the Describe block, that TestDrive is gone, you open a new Describe block, you get a brand new one. Okay. So there's no pollution between tests. It was a brilliant move. Again, if you see these guys, pat them on the back. This is a huge facility. Okay. There's some scoping stuff and we can talk about that if we have some time. In module scope is an Uber container just for testing modules and what it does is it creates all of the test elements in module scope, which means that the test has access to all of the elements of the module including elements that are not exported, right, like variables and like those helper funs that you need to mock. Okay. So when you're testing a module, this is almost a required thing to use. This one has a required name, but the name is not arbitrary and it must match the actual module name. Okay. And then the final thing is the wonderful Invoke-Pester cmdlet and let's get to some code.

Testing with Pester

So here's the little module that I created. There is no code in here. This is empty. This is the one that I created from my help file and here is my first test for it. I'm going to go all the way up here. The rest of this talk is going to be looking at code, so I should probably watch the time. Come on, June. Okay. So let me show you what one of these things looks like. Pester, very reasonably, cannot handle multiple versions of your test module in the session at the same time. So when you're running, you can use Pester for version 2.0 and later, but when you're running on PowerShell 5, this is absolutely critical. And even if you are not running on PowerShell 5, you're users might be, right. So you need to get and remove the module, any, all versions of the modules from the session, so that's the first line. And then the second line is to import the version of the module that you actually want to use and to do this I always use the required version parameter. Your experience may vary. Other people might want to do this in a different way. And then I, because Import-Module returns a non-terminating error, I use ErrorAction Stop because if it can't find my module, I don't want it to run the test. Make sense? Okay. And if you are developing outside of PSModulePath, the alternative to all of this Get-Module and Import-Module is to use Import-Module with a full-qualified path to the module manifest. Right, that works too. But again, I use ErrorAction Stop just to make sure that, and your tests, test this. Your test will run and you don't even have your module or the version of your module that you want. Import-Module imports the first version of the module that it encounters, which is usually the first path in PSModulePath, right, it's a discoverability issue. I do a whole talk about this. Okay. Here's my InModuleScope. Let's get rid of this. Here's my InModuleScope keyword and it has the name of my module. Okay. Structure make sense? Cool. So the first thing I did, actually that's a lie, the first thing I did was to try to put all of this setup in my It blocks and that was just a total mess. So if you have setup to do, you've heard that tests are arrange, assert, and act, right. I do the arrange in a big function. Okay. And so basically what I'm doing here is I'm setting up the TestDrive and I'm setting up fake profiles in the TestDrive. And so here are fake profiles paths. You can see that there's no $ before PSHome because I'm fooling around in a TestDrive. I provided a way for me to grab groups of those files that I needed for testing and at its heart, I take this TestDrive path, I append one of those phony profile paths, and do a New-Item command on that TestDrive, again with ErrorAction Stop. Make sense? So that's what I'm doing and it's much easier to do that in a single function than to try to do that in my It blocks. So I'm going to go back to my help file for a second to the Get-Profile cmdlet that gets profiles and I'm actually going example by example. Okay. So my first example gets all profiles. It's pretty simple. So here are my tests for it and you can see that I even used the example name so that I can associate each example with my help file. It just gives me some guidance so that I know which sorts of tests to write. So here's my Describe block and I usually use the name of the command that I'm testing in the Describe block. That's just a style thing. Okay. Here's my Context block. And this is the little internal function that I just showed you that I'm running. I want to test all profiles, so I test All and it returns a string because I need to compare strings. One of my experiences was that my tests were breaking because my types did not match. Right. I would say this input is a file info object and I expect it to be this string. Well it fails, right, or vice versa, right. I'm giving you a string, I expect it to be this file info object. And one of the ways that I was able to get around constantly making these errors were to make the output type on the functions that I'm testing explicit by using the output type attribute so that I always know what the output type is. So in this case, I needed a string. So I'm testing Get-Profile and I'm mocking a helper function that does not exist at this point. I have no code. That file is empty, but I realize that if I want to mock the paths, I need to have a helper function or something that provides those paths to the function that I'm testing. Okay. So normally, when I test, I end up with more helper functions than I normally would if I wrote the code without using the test-driven development process, okay, but it makes the code very modular. It's actually very nice. So I'm mocking something that doesn't exist. Here's my Get-Profile call. And so the first part of it gets all profiles and the second part gets only profiles. Okay. And this is a pattern that I use a lot. I'm creating some fake stuff that's similar to those. I'm trying to make it hard for myself, and I make sure that they're not there by doing some counting. Okay and when I run this, I'm just going to run it in the console here, it will fail because I have no code. Okay. Yeah. It looks like your test may be requiring something that you're not wanting, that you're not specifying. You're required by this particular sorted order. Right, I sorted them. Okay. Yeah, they're sorted. See the Sort-Object on Get-Profile? Okay. Yeah. Yeah, good point. Thanks. Okay. So this failed because there was no code. So I'm going to close this and grab the next version. The test file is virtually the same. I'm going to put it over here. It helps me to have my code and test side by side. Okay and here is my little Get-ProfilePath helper function and only the code that I needed in Get-Profile to pass that test, which was a call to the Get-ProfilePath helper function and a Get item. Okay. So this time when I go back and run that test, it passes. Can you see that? I'm going to do that again. I'm going to restart the shell and run it again. Okay, so that's basically the process. I take my examples, I convert them into tests, I run the test, the test fails, I write the code. So let me show you what happens after a little while when it gets a little more complicated. So here's my test code for the Disable-Profile cmdlet. Let's take a look at the examples for just a sec. Okay. My first example gets the CurrentUser CurrentHost profile. That's really easy because it's not in the System32 directory. My second example is the tougher one and this is a decision, this is a design decision that I needed to make when I wrote my spec or my help file. What do I do if there's something that I want to disable that I can't disable because it's in system32, right. And so the design decision that I made was that it should disable what it can and write a non-terminating error if it can and this is the example that shows that. Okay and you'll see the test that show it as well. And then the last one just shows that if you have no profiles, there's no output and no errors. And here are the tests that I wrote. You'll get all this code, but here are the tests that I wrote to test that. This is the ones for Get-Profile. Let's close this. And here they are for Disable-Profile. Okay. Notice that when I'm testing Disable-Profile, I'm mocking Get-Profile, okay. Disable-Profile is going to call Get-Profile to get the profiles to disable and I never want a problem in Get-Profile to make my test for Disable-Profile fail. Okay. So I always want to mock any cmdlet that's in my module, okay. So the first one disables all the user profiles and then the second one turns out to be quite tricky. Okay. So in the first case, I have multiple contexts for this because I'm using multiple mocks, but the first test I tested with system32 profiles in a non-admin session and you'll see that I ran Disable-Profile, I converted my non-terminating error to a terminating error with ErrorAction Stop and I used the Should Throw operator to test it because it should generate an error. Now in the modular when I run it, it's a non-terminating error, but here, in order to use Should Throw, I just made it a terminating error. Okay? Good trick. Okay, then I tested the same system32 profiles in an admin session. It should not throw and it should disable them. And then in the third case, I used all profiles in a non-admin session and you'll see that some of them should error out and some of them should actually be disabled. And here's that last test case where it's no profile, so I'm mocking Get-Profile with null, and when I run Disable-Profile, my output should be null or empty and it doesn't write an error, even if I use ErrorAction Stop, it should not throw. And I'll run this and I promise you that it works, unless it's a demo. Okay. Any questions about the structure or content of these tests? Yeah. So you probably should not throw. Is that applied in the throw when and you hadn't sent that and it totally passed. Uh-huh. Well, so I would see the non-terminating errors, but unless I was watching the test run I wouldn't see it, right, because it's going to go to the error stream instead of the output stream. Right, so if I'm taking my test results and saving them somewhere in a file, I would miss that. Questions? Other questions? How are we doing on time? Pretty good.

Testing with Pester - Part 2

Okay, so some time pass and I continue to write these tests and I realized that I might because this is the files, the Windows file system, I needed to do integration tests in addition to my unit tests, and this is just because I didn't trust my own funny mocking of the file system, okay. So what I did was I took the test file that I've been working on and I just renamed it to .Unit.Tests, okay, and then just to make it easy for myself, when I wrote my Describe blocks, I changed it to Get-ProfileUnitTest and Get-ProfileIntegrationTest and I added a UnitTest or IntegrationTest tag so that I could run those separately. Okay, these are just structural. The code was the same. And then I wrote some integration tests and I want to show these to you, okay? I got rid of that great big function that creates that virtual world for me and here's that Get-ProfileTest, but its integration test, so I use the real Enable-Profile to enable the profiles. I have a Get-ExpectedProfile that goes and looks for profiles on the system. I'm actually running Get-Profile and Disable-Profile on real profiles in the real file system. Okay and this made me feel a little bit better. It did not follow the academic model, but it satisfied me that the things that I'm doing actually work in the real file system with admin permissions and without. And the other thing I did on these integration tests was that I put a #Requires -RunAsAdministrator at the top so that I could test the admin parts. Okay. I'm testing the non-admin parts in my unit test. Any questions about this or the structure? Yeah, Adam. The #Requires -RunAsAdministrator, does anything parse that or is that just a comment? Oh, I'm sorry. That's part of PowerShell. And you can put it on any script. Yeah, there are a couple of different #Requires. In my version chaos talk, I was talking about #Requires -Module, right, and it does, there's #Requires -Version for the PowerShellVersion. There's #Requires -Module for dependent modules and you can, this is a different talk, you can actually use a module specification object in your #Requires -Module to require a specific version of a specific module with a specific GUID. So yes, very cool stuff. So now I have integration tests as well as unit tests and I can, let me run them actually for you. I'm going to copy the file path and run as administrator. Okay. Never trust yourself to type when you're doing a demo at a major conference. Okay. And it should run them all. Have I forgotten anything? I don't think so. Okay. Look at all of those tests for the simple little cmdlets. Now one of the reasons that there are this many tests is that for each thing I did, I did an enumeration of the files of the file paths that it returned and then I did a count to make sure that I got all and only the ones that I expected and that's because PowerShell doesn't do a great job of comparing arrays with arrays, right. Anyone who's ever used Get-Unique, right, will recognize that and Pester, because it's PowerShell, doesn't do a great job of comparing arrays to arrays. But in version 4 of Pester, there's going to be new array comparison features and I should be able to cut the number of tests here in half, right, and just test, be able to compare arrays to arrays and that will be a huge change. But again, that's why I specified that I'm doing 3.4.0, Pester 3.4.0. Okay and fortunately everything passed. Okay and now what I'm going to do is I'm going to do, oh I'm going to do it on a different set. I'm going to grab this next set. Let me show you. We have a couple of extra minutes, so I'm actually going to show you something cool. I didn't think I would get to show this to you, so I'm a little jazzed. This is a great lesson and this is actually real life. So I'm going to do Invoke-Pester, whoops, do not type during major talks at conferences. Copy the folder path will do it. Okay. Great. And now I'm going to use the code coverage parameter, which is fantastic and it shows you code that did not get run during the tests, so it helps you write your tests because it tells you where you've missed code and it requires a file name and you always want to run it against your script file or modular file, not against your test file. You don't want to test the code coverage in your test file. You want to test the code coverage in your module file. So this actually happened to me. This is real life. Okay, I had my unit tests and my integration tests and I thought I was really covered. So let's see what happens. The output is at the very end. Okay. And it shows here that my code coverage was 97.5%, which for people like me is a failing grade. Okay, and what it showed me is that I'm missing a right warning, I'm missing what I interrupted to be a test for a warning that I put in, right, and I cheated. There's no example for that warning, but I put the warning in because I kept forgetting that when I change my profile in one session, because those profiles are loaded when the session starts, I need to remember to restart the session to make the profile changes effective, but I did not put that in my examples and I didn't test and I thought I got caught, okay, but I didn't. Let me show you what was actually happening. Note that this is on Line 172. Okay, and I'll go back to my code and I'll go to Line 1, whoops, delete, 172. You see what's going on here? I'll scroll up just a little bit. I did a cut and paste. No one ever does that. And I pasted my code outside of my function. Of course it wasn't running. Okay. But if I grab this code and paste it back inside the function and save, and I run that same test again, (Running) sadly, I get 100% and I don't deserve it, okay. Code coverage means the code ran. This parameter does not say test coverage. It's code coverage, right. So a code coverage of 100% means that during those tests every line of code ran. It does not mean that every line of code was tested. Okay, big lesson, important one. Does that then mean that the code coverage is really making sure that you end up mocking something and then never actually not mocking. That would be one way to look at it. Yeah, so JB very brilliantly suggests that what we do is use code coverage to make sure that we're mocking things so that it should show up in code coverage, to use it in a positive way, so I should see like a if I'm mocking Get-Profile and I'm running only my Disable-Profile tests, that Get-Mocking doesn't run. And I think the only thing I need to test that, the only thing that might happen is that if I only run my Disable-Profile test, it might not look at Get-Profile at all, and also, it might recognize that it mocked. Right, there's a great command called Assert-MockVerified, which means that even if the mock doesn't do anything obvious, it lets you know that it was called. Okay. So there's my little CodeCoverage slide in case we didn't get to do this demo.

Testing Modules and Conclusions

So when you test modules, as opposed to scripts, you use the InModuleScope container, which means that you can mock any function. It can't handle multiple versions, so make sure to use that Get-Module Remove-Module. Oh, if you just use Remove-Module without Get-Module and there are no modules in there, it throws a non-terminating error. Okay, but if you use Get-Module and then Remove-Module, it doesn't. Yeah, it's nice. And then write separate tests for all of your nested modules because they're not automatically tested, right, but you just give it the fully-qualified path to the module. So a couple of conclusions. It took a really long time to do this. It was my first one. I have since written more tests for things that I actually have in production code and it went much snippier. Okay. So it took a really long time, but so did my first decent script. Okay. It's a learning process and I think you really need to look at it in the long run, not in the short run, right, so you don't base how many times, how much time a test takes on somebody who's doing it for the first time. Right? That's a ridiculous measure of efficiency. So I think that in a year or so when people get really good at writing tests, we'll have better metrics for how efficient this process is. It was much too easy for me to assume when I hit a bug in my test that it was a Pester bug. This is a system in development and I did find a couple of Pester bugs. They were pretty minor. Most of the time they were my bugs, they were logic bugs. In this code, I have a for loop, right, and my for loop would not run. You can debug these files too. Just run the test files, run them through your debugger, and I could watch in my debugger, watch it skip that for loop. So I went up to Dave Wyatt and I said your code doesn't have a for loop. I had set my for loop up to run while the value of a certain array, right, the index was less than the value of a certain array and it was nothing in my array, okay. So really, don't assume that these things are Pester bugs. Take responsibility. I found them to be most valuable for regression testing. So in production, I have a system that runs fairly frequently and gathers some files and I change it a lot because my boss always wants me improving my code, right, and this is wonderful for catching unintended side effects. So normally, when you test a new feature, you test to make sure the feature works and this automatically tests to make sure that your new feature didn't break other features that your users depend on. The output type became really important to me. I have learned to use that output type attribute and to include it in my documentation and it makes my writing, mocking, and tests much simpler. It avoids a lot of mocking and test errors. Amen! Yes. The error should be explained in the help file. Okay. That's a really important thing that I didn't realize how important it was for me to document errors and not have my users try to figure out what errors they might encounter. Okay, and then the final thing is that every module should have at least one test file. This talk was reviewed by Dave Wyatt twice. He looked at all the tests and all the code and all of the slides and I owe a huge debt of gratitude to the people who went before me including Jakub Jares and Mike Robbins who wrote great articles about Pester. We're celebrating 10 years of PowerShell and these are the people on the PowerShell team who've been there all 10 years at Microsoft. And this was Real World Test-Driven Development with Pester, so thank you for coming.

Stupid DSC Tricks

Introduction, Dynamic Printer Mapping Example

Hello. My name is Don Jones. Welcome to Stupid DSC Tricks. So this is not kind of a standard session that you might have already gotten used to in your short time here at summit. We're not going to look at a lot of code, and we're not going to do a lot of demo, and we're also not going to do a lot of slides. We're hopefully, going to do a lot of talking and discussing. As a, I've been working with DSC for a couple years it's been out now, right, v4, you spend a lot of time talking to different people who kind of have the base idea of what DSC is supposed to do like from a business perspective. Alright, for example, one of the things you can tell people is look DSC is at the end of the day very similar to Group Policy, right? You're going to have this policy-based thing that tells your computers how to, what to be, and you're going to ship it out through some magically method and the computer will just do that stuff. So I mean super, super high-level, very similar. And so they start digging into it, well can I do this, can I do this, well no, I mean not really how it's designed. You've got to rethink your philosophy. So over the past month or so, Murawski, Dave Wyatt, and I have kind of been having this informal bounce back and forth Exchange about some of the ways we could get DSC to do stuff that maybe it wasn't specifically designed to do. Not that it's bad, but kind of hacking it. Like where could we really just like jam your fist in its mouth and make it do what you wanted it to whether it wanted to or not and that kind of spilled out. We have a private Slack channel for the folks that come to my DSC DevOps camp. It's about 20 folks who were there last year, and so these are folks who have their brains connected to PowerShell 24/7 and they're doing DSC and they're doing DevOps and they're running into the problems and they're trying to get creative about solving the problems and everything else, and so we start going back and we start kind of coming up with ideas of ways we can do different stuff that when you look at the DSC PowerPoint slide, you're like no, it could never do that, but maybe it could. So I thought this would kind of be a nice place because I know a lot of you---how many of you are using PowerShell? Oh, that's reassuring. Good, that did not happen the last time I did this. And how many of you are using DSC? Even just fussing with it in a pilot? Okay, good. So I thought this would be a good crowd to bounce some ideas off of and maybe have folks say well what about how could you do this and see if we can come up with some approaches, right. So that's kind of what we're doing. Oh, by the way, you guys have seen this slide before, right. Quick round of applause for all the folks who've… I'm really proud of the fact that---I don't know if you know, it's a little bit rare on a lot of product teams at Microsoft for people to stick around for a decade. Alright, it's an easy company to move around and you want to move onto something exciting and new, so it's pretty cool that there are so many people who've been around that long. So this is really meant to be kind of a brainstorming session, a think-outside-the-box session. Let's talk about some things that we'd love for DSC to be able to do for us as a business, right, forget the technology. Anything should be possible if we have enough money. So let's just think about what we want it to do and then get a little bit outside the envelope and think about how we could force it to do it. So I'll kind of kick things off with an example, which is this one, dynamic printer mapping. Now talk yourself through how DSC works, right. You sit on your computer, you, the author, you sit on your computer, and you type out a PowerShell script, a configuration script, right, and it can have lots of logic in it. You can have if blocks and loops and all kinds of crazy stuff in there, yeah, and you push run, and what's it produce? MOF. MOF. One or more MOFs, right, Management Object Format, MOF file. A MOF file is just text file. It's not code. Now you can have little snippets in there if you do a certain type of resource called the script resource, but generally speaking, a MOF is static. This MOF is what I want that computer to look like and I created that description completely out of context. I didn't run it on that computer. I'm running it at a totally different place in time. I'm just giving it to that computer and that computer then runs that, but the computer doesn't think about it. Right, the MOF says you need to be a domain controller. You need this IP address, you need to be running these services, you need these files copied over, don't ask questions, don't make decisions, don't think about it, I'm not interested in your opinion, just do it. Now what we tell people is in a server workload, that's kind of okay, right, because servers we tend to set them up, we want them one way, and then we try really hard to make them never change, right, that's why we have ITIL, right, stop all change. That's what that means. And but it doesn't apply so well to client scenarios because clients are, client computers, well who uses a client computer? Users, right, and they're squishy. They're people. They move around, they get up, they change jobs, they install Elf bowling, they uninstall this, there are changes. Well one of the neat things about Group Policy is you don't actually set up all the policy decisions up front. Some of the decisions get made by the client as it downloads it, right, what organizational unit are you in, what domain are you in, what site are you in, what user groups are you in, what WMI filter type things are going on? So it can be very, very kind of dynamic and customized and the Group Policy you get today might be slightly different from the one you get tomorrow. Well that's not what DSC really does. DSC just says be this always. Now obviously there's certain client's computer scenarios, you think kiosks or if you work at a bank and you've got those floors full of phone slaves, you want all those computers to be exactly the same all the time, sure, but most users are squishier than that. And so, one example is dynamic printer mapping. Now if I'm writing a DSC configuration script, I can certainly have a printer resource that says install such and such printer. Right, no problem. But if I tell you to install the printer for Building A and you run over to Building B for a week, how do I get you a printer over there? Right, it's not---there's no evaluation. The LCM, the Local Configuration Manager, as it's reading that MOF doesn't have the ability to say alright we're going to do printer mapping now, so it says map into Printer A, but gosh, I just know he's not over in that building right now, so I'm going to pick a different printer. It doesn't do that. There's no logic in the LCM, it's just following a set of instructions and calling a bunch of resources. So this is one of the first problems we started down because it's one of the first things that comes up in a discussion when you're trying to explain to someone why DSC and Group Policy are different, and we thought we could totally do this with DSC, completely. How? All we have to do is write a resource that dynamically maps printers based on some criteria. So in your configuration script, you say whatever the resource name is, map a printer, yes, map nearest printer, yes, and that's all that goes into your MOF. Your MOF just knows that it's supposed to map a printer. That's it. The MOF doesn't know where the printer is, doesn't know where it should map it to, it doesn't care about any of that, so when you compile that MOF, you create that text file, you slam it out to all your nodes, it runs a DSC resource, which is just PowerShell code, or can be, right, and that PowerShell code goes oh, okay, I'm supposed to map a printer, great. What's my IP address right now? Let me go see what Active Directory site I'm in. I can figure that out locally. I can actually go see a list of published printers in Active Directory and find one that's near me and I'm going to go ahead and map that. So the dynamism, in that case, lives on the client computer. It's not in the LCM. The LCM is still relatively dumb, but the LCM is calling on a smarter friend who can make that runtime decision. And we started thinking it was some of the ramifications of that, right, because you have that idea and then you put your third drink down and you start to think is this a good idea, and after three drinks, obviously it's a great idea. So the next morning we thought, alright, so what are some of the ramifications of this. Well how often does the LCM run a consistency check, by default? Fifteen minutes. Is it going to be bad for that thing to be constantly trying to remap a printer? Well, if you have it go look at what printer it should map and then just check to see if that one's already mapped, and if it is, well don't do anything, which is how resources are supposed to be written, right. Don't do anything until you test first, right, run test. Only run set if you have to. Well, maybe that wouldn't be too bad. And we actually---how many of you work for healthcare? Yep, there's always a couple. We actually thought, you know what, this would actually solve a couple of really cool scenarios because you get into a hospital situation and the doctor runs from room to room to room and he's logging on different profiles, but he wants his profile to be mapped to the printer that's in that room right then, so he's not printing out someone's personal information six rooms down, right, so we thought actually this whole really frequent dynamic remapping thing in certain scenarios could be kind of cool. But we stopped thinking about that because healthcare freaks us all out because blood, gross. So what do you think? Comments? Have anybody ever thought of going after it that way? One thing you're going to have to think about is unmapping a map for… Oh, and when do I unmap it? Yeah. We thought about that too. Partially, we weren't sure if we cared or not because I guess what it was is in that we figured if the test runs and you're mapped to the one you should be, don't do anything, but if you're not, then just go unmap whatever was there and then map whatever you should be right then. It kind of depends on the business situation, right. If you've got some HR person who's got a local printer you don't want to unmap, you'd have to have some way of accounting for that, yeah. Yeah, we kind of walked through that a little bit. What else? What if you're not on the network? What if you're not on the network? Well then you're not going to be printing, so it's not a problem. (Audience comment) Well yeah, it might. That, again, all gets down to your test, right, or your set. We had a little bit of argument about where we'd put those. I think we eventually decided maybe both. But your set should certainly say look are we even on a network here? I mean maybe we don't have the right printer, but is it even possible, and if not, forget it. That's more doing exactly the right thing is what you're doing, is you're now, what are the failure scenarios I might run into and let's account for those, so we're not just spewing errors into the log, but yeah, you do have to walk through that. What else? You might be able to develop some tests, I mean, I'm not on the network, I have no printer, I mean that's what the test would look at. Yeah and that was actually my argument is in the test, first check to see if I can even possibly get to a valid state, and if I'm not, just say true, so we don't even run the set. (Audience question) Yeah, so how do you determine the locality to a printer? That's your problem, right. And I say that with all love, but the fact is, we're all going to have a different set of business rules for how we do that and what we're talking about here is you're going to have to build a certain amount of infrastructure potentially to let you do that. Some people might have databases. Some people might have something they put into Active Directory. I've got plenty of customers who don't have all their client computers in Active Directory, and so they're going to have to have some other place to look it up. Maybe there's a SQL Server. Maybe you've got some back-end tool or something. So yeah, you're going to have to do all that, right, those are things. I don't call them challenges, they're just part of what you would have to solve as part of it. What if say your nearest printer is down. Now see but let me pause you real quick. Before we go down the rabbit hole of making this a printer mapping session, I want to point out that what you guys are thinking about are the right questions. We're not trying to solve the printer mapping module here today. What we're trying to do is make DSC do stuff that it is initially not designed to do. All of those questions are answerable. We could code an answer to all of those. It might mean having to have a directory of printers and who's allowed to use one or whatever, which actually gives you more control and flexibility than you probably have today. We can do those things, but the point is getting DSC to a point where it can let us do those things. So yeah, we do have to have all those things. What else we can get to is there like an alternative configuration or a second configuration? There is no alternative configuration. In fact, let's talk about that. ESPN, you and me, we had it yesterday too.

MOFs

Here's how the LCM works at a really high level. When it gets a MOF, right, so we're pushing them off to the LCM, right, either we're pushing it or the LCM has been programmed to pull it from a pull server, right. Those are the only two ways you can get them off there, right? No. No. That's a trick question. Trick question, there's three ways to get them off to the LCM. What's the first way? Give me one. Push. What's the other one, second? Pull. The third is injection. The third is called file copy. It's very advanced, haven't really had that technique for more than five or six decades. And if you're going to inject, here's what you need to know. In the folder where the configurations live, you've got pending.mof. It's a text file. Pending.mof is the one that you just got most recently and haven't done anything with potentially. Current.mof means you are currently running a consistency check and that's the MOF you're using. This is a really cool thing. Look, they're two separate files. Why are they two separate files? Because if the LCM has one of them open, you can't write a new one to it, right, you guys have run into this before with every file type ever on a file server. Right, so it's not like this is word collaboration. So when it starts to run a consistency check, it makes a file copy of pending.mof, names it current.mof, so it can have one exclusive to itself without having an exclusive lock on pending.mof. So while a consistency check is running, you can overwrite pending.mof. You can also do it when there isn't a consistency check running. You can anytime you can do it. We've also got previous.mof and that's the one I ran last time. That's kind of a last known good type of recovery situation. This is why if you're in DSC 5 and you're really, really messing around with like how many of you like to just kick the LCM and make it run right now and not wait 15 minutes? Right. You know what, you're going to live for at least 20-30 more years, let it take its time. Right, you're not going to die that soon. But that's why if you screw up a config and you get the LCM into kind of a ah state, you can use Remove-DscConfiguration. There's a command to remove the previous config and you have switches, which one do you want to remove? Previous, current, pending, right, so you can remove all of them if you want to. And that's why it's important to do that if you need to kind of get it out of its own head. But so we've got these ones we can mess with. And so this takes us to a really, really kind of important capability. We could, for example, tell me if you think this is crazy. Let's say I write a custom resource and we're going to call it crazy resource. I ship that out to all my machines, yes. I write a configuration script and all it does, the only thing in my configuration script is run crazy resource, crazy resource parameter yes. That's it. Okay, I run that configuration script. What gets produced? The MOF. What's in the MOF? Just one thing, right. It's a text file and it says run crazy resource. I push that out to my computer. It goes into pending.mof, right. LCM sometime later, ah, time to run a consistency check. Fifteen minutes has expired. I'm going to copy pending.mof to current.mof. I'm going to open it up. At this point, is there anything stopping me from doing something to pending.mof. Nope. So it runs crazy resource. Crazy resource dynamically generates a whole brand new MOF and sticks it in pending.mof. Who said, ugh? I didn't say these were smart DSC tricks. Look at the title of the session. And at the very end of that dynamically-produced pending.mof, it recalls itself. So the last thing in there depending on everything else is call crazy resource again. And so it runs that. That'll take a minute, two, whatever it takes. LCM waits 13, 14, 15 minutes, okay, pending.mof copying it over to current.mof and wow, look at all this stuff that's in here and all those decisions were just made 15 minutes ago and they were made locally on that machine. You could obviously do a hybrid approach, right. That's kind of an extreme example to lay it out. You could certainly ship out a MOF that had a lot of your base configuration stuff and then called crazy resource to just dynamically add-in printer mappings or whatever dynamic more local intelligence things. You could evaluate the WMI repository, you can make decisions, you've got the full PowerShell scripting language at your disposal to create a new MOF that will run the next time. Crazy resource could say, okay, look here's the base MOF. I'm going to copy that off here to make my own little copy and I'll use that as my starting point and I'll add dynamic stuff to it as I go. That's crazy. It's called crazy resource.

Downsides of MOFs

Now downsides. One, I'm not actually sure if you can do this in version 5 right now because v5 introduced this new encryption, so the MOF files actually sit on the drive encrypted, and I'm not sure if it would be feasible for your local crazy resource to get that same certificate and encrypt them. It should be. I just haven't really dug into that. It should be possible because they're not encrypted on the pull server, right. They come across the pull server in clear text. Now they do that across an HTTPS channel because none of us are dumb enough to run an HTTP pull server, right? Right. Right. So they come across clear text through an SSL channel. The LCM is what's encrypting them to begin with. Therefore, that certificate has to be somewhere on the machine we can get to it; therefore, crazy resource should be able to A, decrypt it because the LCM has to be able to decrypt it, but crazy resource should definitely be able to create a dynamic MOF, encrypt it, and then stick it in pending MOF, and then the LCM will copy it to current, and then read it. Make sense? Seem plausible? Are there tools? Are there any tools. Wow. Wow. Are there any tools for making MOFs? Yeah, configuration scripts. That's not a flip answer. Right, because if crazy resource is a configuration script, then PowerShell knows how to run configuration scripts and make MOFs. Or if crazy resource has the ability to .source a configuration script to run it, it will produce a MOF. Right, .source this file, run it, blah, MOF occurs. Right now, that's the only reliable tool we have for making a MOF. Another downside is troubleshooting. It's going suck. It's complicated. It's going to suck. It's going to be complicated. There's a lot of moving pieces and here's the downside. Anytime you've got logic running at the furthest point from you, it's harder. So if you were to do this, you'd really, really have to be disciplined about logging, really disciplined about logging, ideally, into the event log because you can get to that remotely fairly easily. Worst case, dumping stuff into a file that's in a well-known location so you can get to it, you're going to have to log the snot out of stuff. How many of you are used to producing like nuts verbose logs in your scripts. Yeah, the rest of you need to get to a point where you can raise your hands on that question because it's going to need a lot of logging to do this. Now if you modularize that, if crazy resource is doing five different things, mapping a printer, doing whatever crazy stuff it's doing, if you modularize that really well, it should be easy to test those functions in standalone in a variety of situations, run them through Pester tests, make sure they work before you deploy them. Test your code. Okay. A couple folks looked surprised when I said that, so I just want to make sure. Logging is going to be really, really important. The application of that MOF, right, so we created the dynamic MOF, now the LCMs going to run. Ideally you want your resources, whatever resources are being called by that dynamic MOF whether it's a Microsoft resource or one you've written your own, you want it to be spewing a lot of log data too, like to the debug log or something like that, to the point where if you're using one of the Microsoft ones and you don't feel it logs enough, you're need to walk your butt up to GitHub, fork the project, add a ton of logging, and then do a request for them to pull it back in. Wow. You have two lines of logging for every line of code. Yep, that's okay. That's just what I feel we need. So I mean a lot of logging is going to have to be necessary here. You guys don't look scared enough.

Use Cases for DSC

What are some use cases, some things that you wish you could do with DSC? Before, because I have another tangent I want to go down, but before we do that, what are some things that you think you want to do with DSC that on the surface doesn't appear you could do because of where the logic lives. Well I want to get from having pets to having cattle. Pets, that's actually my next tangent. But right now, I've got pets, so how do I get the DSC to make my pet into cattle? Hold that thought. Yeah. I want to get the output of one resource as input to another resource. You want to get the output of one resource as input to another resource. Yeah, I want my pool to not have algae, but that I think is a more fundamental design issue. Hang on. I don't want you to wear your arm out. We'll get there. We've talked about that because that's a really serious need. It would be best for the product to address that, but since it doesn't, we decided to hack around with it. How comfortable are you modifying every resource you use? Fine. Sure, good, brave man. I like you. We're going to drink tonight. It won't go well. Our idea was to take the same basic hash table that is in the configuration script that essentially goes into the MOF that becomes the input to both your test, and your site, and your app, right, and persist that in just a known location that every resource can then choose to read in. We've even, talk to me and you might be able to talk me into getting access to our private Slack channel because we've even started hacking around with some tools to kind of abstract that, right, so you can just include this module and then have a persist function and whatever the opposite verb is function to add to your resources so you can do that. One of the reasons we felt that would be better than a memory-based solution is because it almost acts a little bit more like workflow in that if something happens midway through and there's a reboot or whatever, right, the whole idea behind DSC is it should be able to just pick right back up again. Well, that would give you some state information about where you were because the LCM will know, but you won't. Yeah. Yeah, we've been hacking that around. You. So the package resource and binaries like the canyon virus, where it's a gig in size. Every time a consistency check runs whether I need the version installed or not, it's going to download that thing again long enough to look at it and say oh no I've got this looking, forget it. Yeah, so short version of question, package manager sucks because it has to go get the file in order for it to determine if that's open and when the file is huge, it just takes a long time and hold things up and I think it really we can boil that down to the current package resource sucks. And that is really not the fault of the people who wrote the package resource. It's the fault of the people who wrote Windows Installer and InstallShield and all this other garbage. So Windows itself just does not have a great way of doing that. I think the go forward solution there and we're just like on the precipice of this starting to become a thing is PowerShell Package Manager, which if you're using the right repository that supports checksums and everything else would let us author a new package resource that could check things that way. It would be nice if package manager would look at the product Id and say it's installed. Well lots of things would be nice, but it doesn't. We can write our own. I don't think writing a better package manager is at all outside the scope of what DSC does and let's take that as a philosophical point. I'm not actually picking on you, well a little bit, but not really. If you're looking at one of the Microsoft provided resources for DSC, other than the file one, which is binary, right, so if you're looking at those and you're like yeah this kind of sucks. It'd be nice if it did this instead. You need to get your butt on GitHub, fork the project, and fix it instead of complaining, right, because there's certain bits of the product that only Microsoft can do, right, like we can't fix the LCM. We can't fix the pull server code. We don't have access to that. The things we can't fix are the things they need to focus on and the rest of the stuff, we need to do for ourselves. And that's just how it is. It'd be nice if they gave us everything, but then they could do our jobs and we wouldn't need our jobs. So that's where we are right now. So the package, there's actually a lot of people who are hacking away to make better versions of that. Get involved in one of those projects. Okay. The current one relies too much on the Windows Installer database and anything not hooked up to Windows Installer sucks and then the Windows Installer validation process sucks. Anybody ever query 132 product on a Server 2003 machine and then lose your job? Why is the server slow for six hours a day? Virus, I don't know. I'll run a scan. That's my log in script story. So anymore? Any things that I'd love it if DSC could do this, but it just doesn't seem possible the way it's architected. Because that's why we're here. We're here to hack the architecture. Okay, so you're completely satisfied with everything DSC does and you feel the architecture is completely approachable and finished. That's fantastic. We'll pass that onto the team. They'll appreciate that. Ordering like composites can't be dependsOn, right? Ordering and composites. Right, you can't do a dependsOn. I would argue that you probably don't want it to because the amount of logic that would go into tearing those apart and going down that rabbit hole would make the thing impossible. I'll make a design argument, which is---so do you guys know the whole reason that--- you guys know what dependsOn is, right? You need to jiggle your heads. Okay. The whole argument in favor of doing dependsOn is in case the LCM ever becomes multithreaded. If the LCM is doing 10 things at once, it needs to know which ones not to do yet and that's what dependsOn is designed to document. My argument because that becomes a problem for composites, I have an argument against dependsOn, which is that it makes your configuration document harder to read. Unless you actually list everything voluntarily in dependsOn order, it becomes hard for a human being to parse that. Alright, so that's going to happen and that's going to happen, and that's going to happen, and oh, oh no, that's going to happen, then that's going to happen because this has to. It gets harder just to mentally grasp it. Make file. Kind of like a make file. My argument and this is a design thing and this is something Microsoft would have to address. My argument is that a not doing dependsOn and instead doing a sequential, which is like what Chef and Puppet and those guys do and occasionally giving us a keyword called coalesce meaning here's 10 things, do them first in whatever order you want to and then stop until all of those things are done and then continue with phase 2. That would have been semantically I think a lot easier to read, a lot easier to implement, and avoided that whole problem because then sequence becomes important. So we can talk about that with them tomorrow because they'll be here. Yes, we can suggest that. Jump up and down and scream. Anything else architectural? I want to do dynamically generated configurations. I want to do dynamically generate config, yeah. That, right. Locally. Yeah. Yeah, I'll give you two places I think we could do and I think we can do this almost nearly today-ish in terms of dynamically-generated configurations. I think we can do locally dynamically-generated configurations on the node today. You're going to have to write your own resource to do that, but that's what we're talking about here. I think we are nearly to the point where we could also dynamically generate them on the pull server. Now that pull server protocol is documented, we just don't have any code to look at so that we can make our own smarter pull server. I think we will get that. I mean, they've been pretty clear that their goal is to open source at least sample code where you can build your own and at that point, we could have our own pull server taking state data from right then and dynamically generating a MOF to hand to the node. I think we're close to that. We can do everything now except the actual pull server because we don't know what the code looks like and none of us wants to just reverse engineer the protocol. I think we're nearly there. Let's talk about cattles and pets. The cattle versus pet thing is something that's going to require 10% operating system design change and I don't think these are major. I think it's just a very evolutionary step and 90% us changing our processes. How many of you, for example, assign static IP addresses to your servers? Why? Do you not have DHCP? Did you not know about DHCP? Are you afraid of DHCP? Are you superstitious about DHCP? How many of you just think DHCP is freaking voodoo? Who thinks it's science? Yeah, so let's take that for a second. Vendor requirement is bull. That is the 90% us changing our processes. Right. You can't say I wish I could do X, but there are stupid people in the world. I know that. We're all surrounded by them on the highway every single day, but that's not a reason to, you can't say I need to re-engineer the entire universe because of stupid people. No, that's an HR problem. You need to bury the stupid people in the desert. That's why I live in Vegas. If you've got stupid people, you can't have nice things, period. So we're not talking about the situation where you just politically can't have nice things because you've got the option to go find a different job. So let's talk about the technology things, right. There is no reason to not have your servers getting their IP addresses from a DHCP server or a DHCP-like entity. Right, you get on SCVMM, for example, and you've got like IP address pools and things like that aren't DHCP, but they're vaguely similar, right. There's no reason that a configuration, a DSC configuration MOF needs to be pushing a static IP address. They should be set for DHCP. And if you want to as you're kind of MetaConfiguration be configuring reservations and stuff like that, God be with you, that's fine, but cattle don't have names and an IP address is a name, and so you should not be assigning names. You should be doing that at a metalevel. That's problem number one. Problem number two, machine name. Right, that's the other unique piece of information we have to set, well ish. Windows anymore makes up a name for itself when you first install it. Let it keep it. Who cares what the cattle thinks its name is. I don't care. As part of the configuration, rather than assigning a name because your configuration is often going to be role-based, right. You're configuration's telling a machine to be a certain way. Part of who it should be is registering the necessary CNAME records in DNS. That's what you talk to it by. Right, if you're in this application, well then register that name and that's what we're all going to call you. The only couple of, and this is where we get to the 10% that has to be an OS design change, the only couple of weirdnesses you get there is let's say PowerShell remoting becomes involved. Can I easily remote to a machine via a CNAME? Yeah, not as easily as knowing its name because I need to know its canonical Active Directory name if it's in Active Directory, so I can get a Kerberos ticket for it. It won't work with a CNAME. But you can certainly work around that, right. If, for example, you were doing the right thing and that machine was requesting a certificate for itself with its CNAME, then that certificate could be used on an HTTPS listener, which is what you should be doing anyway because none of us are dumb enough to be doing remoting over HTTP, right? Right. Home Depot, guys, Target. It's going to happen. So if the machine knows its role, then it knows what CNAME to set up for itself. And yeah, are you going to have to then come up with a back-end process to periodically scour or scavenge those? Possibly. You could certainly have things like expirations on them because the LCM is going to run periodically and make sure that it's re-registered. So you just look to see the last time it was just every hour, right, the LCM doesn't have to run every 15 minutes. You know that, right. It can do it less frequently. Every hour, go update the update record on your CNAME so that I know it's recent and if anything becomes too old, I'll have another process that scavenges those. Pretty much exactly like DHCP works. So you've got your CNAME record there. That means you can go to our PKI. How many of you have PKI? The rest of you need to catch up. We all need PKI. Go to the PKI and get a certificate in that name and now you can hook that up to an HTTPS remoting listener, and so now I can remote easily into the machine using its CNAME. I never need to know its natural machine name. You think Microsoft knows the machine names of all the crap they have running in Azure? I doubt it. They sure as heck don't have an Excel spreadsheet somewhere tracking all that garbage, which is what you use, I know. How many of you use Excel to track your static IP addresses or servers. Yeah, of course you do. He's the only brave one who admits it. The rest of you are lying. I know. So when you start thinking about the things that are unique, most of them are only unique because of the way we're accustomed to doing things, not because there's a tremendous technological hurdle. Is it inconvenient to change? Well no. It's only inconvenient if you're holding onto your old processes. If you just let go, then it's actually not hard at all. Static IP addresses for servers, I mean, move on. Let it go. Let it go. I'm not singing, don't worry. We only do that in the closing session and it's a song about beer.

Use Cases for DSC - Part 2

What else? Who else is moving toward a servers are cattle, not pets, kind of environment and are using DSC to help with that in some ways? Yeah? (Audience comment) Good. DSC and Chef, yeah. And it works? You're standing, so I'm assuming that there hasn't been a blood bath. Not yet. Not yet. It's possible. You just have to decide that you're going to change your processes. And you know what, if there's a theme to this whole discussion, it's forget what you know and think about fresh, right. That's what this whole thing is. Forget what DSCs architecture is as you've been told. What could you do with it if you just stepped outside of the lines a little bit? Forget about well you've got to have a machine name. Do you really? Right. So just step outside those lines a bit. What are some other architectural things that you wish DSC did that you just don't think it can do? Oh, that's right. Everyone's completely happy with the product and doesn't want any changes to it. I forgot. (Audience comment) Ah, that is actually really good. Dave Wyatt and I have an argument about this and I'm afraid he's right, the bastard, because I wanted to make, so the question was what about one DSC resource calling another DSC resource and you might think why would you do that? Well here's a really good example. Let's say I wanted to write a dynamic evaluation resource. Okay, so in my configuration script, it would say pick a basic resource that you like. Oh, that's good. File. So let's say we only want a copy of this file if this thing in WMI is set to this value, right, something like that. So my idea was we'll write a resource and let's query this WMI class, compare it to this value, and if it's true, run the file resource with tab to indent these settings. If it's not true, tab to indent, run the file resource with these settings instead, right. And so we kind of went down this and I said so look and there are two answers. Answer one, if you are using function-based resources, you can totally do this because they're just modules. So you're if then resource, if we can call it that, can we call it that if then, your if then resource can just go load the module into its scope and call the test in a set and it can just manually pretend that it's the LCM and run those things. It's a little harder with class-based resources because if a class-based resource attempts to instantiate another one, you actually get an error saying that there's a configuration, a consistency check already in progress, like the LCM gets involved and stomps on you. So we haven't figured out a work-a-round for that yet. But for function-based ones, you can have one resource call another. Class ones, I still think there has to be a way to step out of the scope and get into it, but we haven't gotten it to do what we want quite yet. (Audience question) Sure. Change the INI file, restart the server. We still have INI files? That's awesome. I love INI files. What else? Anybody tried to do anything crazy with DSC or are you just doing like file, package, network, firewall. Yeah, just the basics. Nah, go home and get crazy with it. Go nuts with it. Try weird stuff. Set yourself up a little lab. What do you need like three machines, client, server, pull server. Do crazy stuff because crazy stuff is how we break it and crazy stuff is how we break push the edges. And until we try to break it, nobody can fix it, so we have to try and break it first and let's find out where it's not doing what we think it needs to do so that we can tell the team because that's really what they want to hear, right, they know they're just guessing as to what we need based on conversations they've had, but they need more conversations and we've got to try and break stuff, we've got to try and push the edges of it and find out you know what this model would work really well except for this one thing it won't let me do. Can you take that off? Make a case for it. Reporting's tooling. Yeah, go ahead. One thing I'd like to see is something to deploy the DSC in an existing machine so you generate configuration like based out of this because… Sure, point DSC at a machine and have it just generate a config for how that machine is set up. Two responses, response number one, that's tooling, right, that's not so we got to make sure we're differentiating between platform and tooling. DSC is not tooling. DSC is platform. So what you're after is a tool because that's not something you're going to do every five minutes. You're going do it as a provisioning thing. We'd also like tools to do reporting for God's sake, that'd be nice. We'd like tools to be able to keep track of what config was supposed to go to what machine. That'd be great. All that will be called system center expensive something, something, something, something I'm sure, one day. So we'll diverge to tooling for just a second. I don't think you do want that. I think people think they want that and I hear that a lot, but let's go down that rabbit hole a little bit because when you point that thing at that machine, is it just going to enumerate every single registry key, file, service, process ever? Or is it supposed to compare like a base OS with? It gets really tough and Windows has got 632 quad zillion management points and it doesn't know which one you care about. Now what I would say is that it's probably legit to ask for a tool like the packaging tools used to do where it can watch. Yes. Right. Okay, I've got everything up. Here's my base image. Begin watching. Yeah, but except people make that. Right, that's what, oh heck, they used to be a sponsor of us, dev power people with stuff, ScriptRock. UpGuard now. Oh, they did change their name. No one knows why. Yeah, so there are people who do that. You're going to have to pay for it, but you know if you want nice things, you have to pay for them. I'm not sure I would ever expect Microsoft to produce that tool just because they've declined to do so in so many different iterations, the exception being App-V. Right, which is part of how App-V, that's how you App-Vify an app as they watch it while you install it. But they bought that from someone else, so it's hard to tell. But yeah, those tools exist. Now what they don't currently do, what UpGuard does not do is make a MOF out of that, but I think that's just a matter of enough customers saying you know what we'd really like. How about installing MSU files? Installing? MSU files. Installing MSU, you mean patches? I don't know. I go back and forth on that one. I don't view DSC as a patch deployment mechanism. But what if you need a specific patch? Oh, you just need one like on a short term thing? Short of just because that system for your business case needs this patch in place. Yeah. And you want to make sure that it's on there, but not maybe necessarily run the entire Windows update. I'm not sure. I go back and forth. I'm not saying you're wrong. I'm just going to imply it. (Audience comment) And part of me says that DSC can't be the one tool you use to maintain your entire infrastructure and that the best use of DSC for a software deployment situation would be to use DSC to make sure that you're software deployment solution is installed and active and running and then use your software deployment solution to push files and crap. And the reason I say that is because software deployment is such a thing. You have to track status because you want to be able to query it. You have to have distribution points. You have to worry about WAN links when you're pulling software and I just think that starts to get outside of basic config. I think your basic config is make sure that this other solution is configured so that it will do its job. That's kind of where I sit. We're in this weird space with software deployment right now. My suspicion is that as Microsoft moves more and more toward a cloud command and control in tune style stuff with maybe on-prem distribution points, you'll start to see that becoming easier. I think right now it's great to say just use DSC to configure your patch management and software deployment solution, except we really don't have a great one. We've got Windows update and SCCM and that's it. Can't we call it sick 'em anymore? So yeah, that's kind of one of the reasons I'm not really looking for the package resource to be all that awesome. To a point, like to the Yum RPM level sure, but I don't really want to see DSC become a whole software deployment solution because we already have so many of them and they all suck. So I'd rather get one of the ones we have working, but that's something fun to talk about. Let's drink and talk about that because it's more interesting. Yeah. So the squishy metaphor like where users and DevOps… That's not a metaphor. People are squishy. Try it, poke him. Squishy. So after the point there, I can see that argument there, but for the service where it's maybe there's just the one application they want and really the particular latest version of… No, no, I think you and I are on the same page there. I think as you get into a server and your goal is to provision a server and you need this management agent and you need this and this and this, you need to get it to the point where it can participate in the rest of the infrastructure. I think that's a legitimate use for DSC being able to deploy packages contingent on A, us getting something that really works well like PowerShell Package Manager hopefully will for us, right. We need a Yum RPM Package Manager, not just giant file full of Windows Installer files and it'd be really nice if we could just burn Windows Installer to the ground and never look at it again. That'd be great because it's the source of most problems because people build crappy packages. Not that Windows Installer is bad, it's what people do with it, right, guns don't kill people, developers kill people. So it's basically that. So I think there is definitely---this almost gets into the DevOps-ish world, right, do we have, no we don't have time. Well sometime when we're drinking, get me to tell you my whole DevOps story and that kind of fits in it well. One more question all the way in the back. He was first. (Audience question) In doing who? Networking. Networking. Love it. Switch. Oh, you mean like using DSC to manage switches and stuff? Yeah, so that's pretty much your job to pressure your network vendor because Microsoft has already done all the work they need to do in that space. They've created a reference OMI stack, right. OMI is basically a super lightweight WS-MAN stack and a super lightweight CIM repository that doesn't require a repository. It's all in memory, so it's designed to run small footprint, small processor, it's designed to run in embedded devices like switches and there's at least one company, Dell, that's already jumping on this bandwagon and you can shove them off into one of their switches and it'll do it. So that's happening. You've just got to make your vendor jump on board. Who's your vendor? Cisco? It will be Cisco and Azure. Yep, have fun. Good luck with that. (Audience comment) Cisco and some of their stuff is, yeah, and they'll roll it across generations. The fact is that Microsoft with DSC did the most intelligent thing I've seen the company do in a long time and they didn't invent a lot of new crap. They just adopted standards that we're already out there, and so for other companies to jump on doesn't mean they have to bow down to Redmond, they're just doing what the rest of the industry had already decided, but not actually implement it. Microsoft was the first to put a big implementation out there and got the ball rolling, so that's already happening. So look, cool. Guys, we're at the end. Thanks very much and enjoy the dinner on tonight. Thank you. And let's keep talking and having ideas. Yeah.

Accelerated Toolmaking: Copying PowerShell Commands

Introduction

So we're supposed to spend roughly two hours talking about a topic that I've called Accelerated Toolmaking: Copying PowerShell Commands. I do a lot of scripting and I have built my own set of tools to help me Build tools faster. I'm assuming most of you have written a PowerShell scripter tool. Is that correct? Okay. And it can be kind of a tedious process. We're always looking for shortcuts and other ways to do that. So that's kind of the premise for my talk today is what can you do to kind of speed up this process and I'll go into this in a bit more detail. Again, for those of you just here, you've seen this slide many times. It's kind of amazing that I've known a number of these people for 10 years. That's really kind of hard for me to get my head around sometimes. Alright, so those of you who don't know me and there are people who don't. I was at a bar last night and someone recognized Adam Bertram and then he turned to me and he said who are you. I loved it. That's perfectly fine and made Adam's night. So, I am a grizzled IT Pro veteran. Not quite as grizzled as Richard Siddoway or Mark here, but I'm up there. Well you know if someone's got to be the old alpha IT pros and I've been around, I've seen a lot of things, and I try to bring all of that experience to the work that I do and the books that I write, the classes that I teach, the sessions, the blog articles, all of that. I'm a long time PowerShell MVP, oh cloud and data center now, but I don't know, I'm still always going to call myself a PowerShell MVP. The whole cloud and data center, that's just too much to say. I am a PowerShell teacher, author, I've written a few books, I do some consulting and project work, so some of you know me and have seen me, I've been around a bit. I do a lot of my writing at petri.com, I do courses for Pluralsight, and I am pretty active on Twitter. I've got a slide at the end that has all my contact information. Alright, so getting into this here. So what's the problem? Well, you have some sort of PowerShell scripting or toolmaking requirement. Someone has come to you and said hey we need a tool or a script to do X. And you look around and go oh well you know we have this one PowerShell command and it does almost everything that we need it to do. Almost. If we just had like maybe one more parameter or maybe if it didn't have a particular parameter or if I could just combine this with another command, it would be perfect. Now I could, me, I'm talking in your stead, say I'm a PowerShell expert, yeah, I could sit and I would know what to type, but often you're building stuff for other people. You don't want them to have to rely on knowing what to type, how to plug stuff together. You want to build braindead simple tools, right. So how do we get there? You may also need a tool or script or something for delegation. You're building a server. You may have a constrained endpoint. You want to put something on there that you're going to delegate to the help desk, or junior IT admin, or an end user or perhaps you're even using JEA. And again, you want to have some sort of tool in place that is based on something that is there now, but more customized to do exactly what you need to have done. Now instead of reinventing the wheel, just build a better wheel. Now fortunately, I'm going to show you some gimmicks, or tricks, or hacks, or tools that I have built to help speed up this process of building a better wheel because like you, the last thing I want to do is open up Visual Code, or ISE, or PowerShell Studio, whatever tool you're using, #Requires S version 4.0 and type everything out. I'm assuming most of you use things like templates and have other snippets and all these other tools to help speed up the process. Well what I want to talk about today is some other ways that we can speed up this process of building tools. So I have this little kind of a copy-a-command concept and this idea of copying a command, you can go down two paths and we're going to look at both paths. I've got lots of demonstrations of this is the command I started with, this is what I ended up with, and we'll walk through it and look, and so you can see the changes that I made. Everything I'll show you, I will share at some point. Follow me on Twitter because that's probably where I'll announce where to go to get the download link. So we can create a proxy version. A proxy version is a way of creating kind of a copy of the command, but you actually are still running, when you get to running it, the actual command and you'll see that. Anyone here ever build a proxy function? A few. Okay. Was it easy to do? In some ways it's kind of easy. I'm going to try to show you how to speed up that process. I mean, it actually is easy because it's really almost a two-step process. You run this command and then Get-Foo will be whatever command you want and that will create an object that has command Metadata, basically, the parameters and the ScriptBlocks and all the stuff that defines that command. You then create a new proxy function. I think I've got more code examples here. And then you edit, take away what you don't need, add in what you do need, and when you're done, you have a new command. Your proxy command can have a totally new name or you can reuse the name. Personally, I always come up with new names. I'm not. The only reason you would create a proxy command with the same name is if you were going to be doing some delegation probably with JEA. For the most part though, the tools that I build I'm building to help me do my work and help other people. I don't really do a lot with delegation right now. Maybe once JEA really matures in v5, I'll move into that more and look into that, but and you'll see all these in action in a moment. The other approach I take is to create what I call a wrapper function and probably many of you have built things like this. It's a function Get-Foo that at the very heart of it, then call some other PowerShell command. Those can be kind of tedious to write because you have to generate the help. How many of you just love writing comment-based help? Yeah. So hire June and she'll write your comment. Use Visual Studio to right-click your function and click Generate in PowerShell. You have a question or you like writing help? I like writing help. Okay. So we've got a few people who like writing help. You'll see that I don't write help. I copy it. And but you also then assume multiple same parameters for your underlying function or an underlying command that you are calling, but you have to type, do a lot of typing, or manual copying and pasting. And when I build my wrapper functions, I leverage splatting because I'm going to assume that the parameters for my wrapper function are mostly the same parameters for the underlying command, which makes it really easy to just splat PSBoundParameters right to that command, so it makes my code really simple to look at and I'm all about having really simple-looking code. Alright, so that's really all the slides I have, so I'm going to go into PowerShell, walk through lots of demos. We'll come back to the slides, ask questions, give you a few references,

Creating a Proxy Script

and let's look at some magic here. Now even though I'm running PowerShell 5 on Windows 10, everything I'm doing should run on Windows or PowerShell 3 even or later. I tend to put a requirement of at least version 4 because I think everyone should be at least at version 4 at this point for anything running on your desktop. Yeah, there's nothing that I'm doing that is version 5 specific. So if you're not at v5 yet, you don't have to worry about anything. So let's kind of walk through all of this process. And let's just run Get-Command. It's alright. So you all know Get-Command retrieves a command info object about whatever command you specify. It can be either a cmdlet name or an alias or a function, in this case, I just am going to use the full cmdlet name. So we're going to start off by kind of looking at this proxy approach. So I'm going to get the command Metadata for $C and that $cmd, that object now, is pretty rich. There's a lot of actual useful stuff here that I think a lot of people skip. Now I'm going to go through manually through a lot of these steps and then I'll show you my cheat, my automation tool, which will speed up the entire process. How different is the output from the command's metadata than just what you do when you've looked at all the properties up in the service? Oh, back up at the command default. It's the same information, but this object, this automation command metadata, has some additional methods. Thank you. Which we're going to take advantage of here. Right, so it's more than just the properties. There are some methods to this object that we will invoke to make the magic happen. So like any object, of course, you can drill down, so you can look at the parameters to get a hash table. This will be the same type. You can get the same type of information by doing Get-Command and looking up the parameters. It's still there. We're just looking at it at a different way. And I'm showing you all of this so that you can understand where, when I get to copying and pasting, where the data is coming from. Okay, so it's coming from objects like this. And the, no actually, I'm going to take that back. I may have misspoken. Let's pipe $cmd, Get-Member -Force. So actually the methods we're going to use are for a different class. I misspoke. I'm an IT pro veteran. Not a developer veteran. So we're going to take advantage of the System.Management.Automation.Proxy class and that has a static method called GetParamBlock and the parameter we're going to specify is our command metadata. So this is going to be another way of parsing that hash table of parameter information. Let's see what that looks like. Yet, you know, and if you have a laptop, you can follow along with some of this. I'm not going to go really fast. So now we can see. Question. Is this all going to work for homemade of cmdlets and modules, and well if I hook one up, to get that same sort of result? Yeah. It should. If you have. I assumed it did, but I wanted to make sure. I think the only thing I would make sure of, if you wanted to copy one of your functions, is I would make sure that you are using a cmdlet binding attribute. All the advanced functions. Right. Yeah, so but if you have an advanced function, but if the Get-Command doesn't really care, it's going to still find the same information. Yeah and Get-Command works on simple functions also. Right. I don't know, but the command metadata will work unless you're using that cmdlet. I always use cmdlet binding attribute in my functions. Anyway. So there is the parameter information. That's the parameter block. Now there are some things there, we're going to copy this later and eventually you'll see how I fix this, I'm not a big fan, for example, of the way that they layout the parameter names. So instead of being $name, it's $ and then in curly braces and then name. It works fine and I'm sure there's a legitimate .NET reason for them to do that. I know one reason is if you have a parameter name with a space you have to do that, but you don't build parameter names with spaces, right? If you do. Then you get exactly what you deserve. Yeah, yeah. But you can see I have all of the parameter attributes and the aliases, that's the type, so all the information is right there. We can also use this class, and again, looking at the command, to get the different blocks of the command. Now every cmdlet and advanced function is going to have three blocks, right, your begin process and end ScriptBlock. So I can use this magic here and say what's in the begin ScriptBlock? Remember, what we're looking at here is what is then going to end up in a proxy function. Once we get to this in an ISE tab, I'll go through it in a bit more detail. But there is the begin block. We then have the process block, which is really just running the command. We'll come back to this in a moment. And of course, the end block. We're just ending the pipeline. What's also nice is that we can get the help for the command. We're going to again use this proxy command object, this class, and there was a method called GetHelpComments and the parameter has to be the help content. So I'm just having kind of nested Get-Help command for Get-Service and you could certainly use variables and when you see my acceleration tool that's what I'm doing. What this will do is, well let's select that, run that, this gives you all of the help, basically, in a text format. You see where this is going because I can now start getting the pieces I want and in essence copying and pasting them into a new command. So if I'm. Yeah. Does this work with the commands with .NET? No. No, this is only going to work for a cmdlet or a function. This won't work with any native .NET class like the math class, for example. For something like that, you could look to classes in v5 and build something new around that. If we have time, I have something that we can at least get a peek at to give you an idea of what that might look like, but that's kind of we'll see how quickly we get through this. Yeah, everything I'm showing you assumes that the command you are copying is just that, it's a command. It's not a .NET class. So then if we wanted to, we could go ahead and create the actual proxy command. This is the second line that I had in my slide and I'm just, I'm actually, I should change that because actually I am using v5. I have some clipboard cmdlets now. (Audience Comment) Yeah or you could just pipe it to clip, number of ways you could do it. So now, if I come here do New, paste it in. So there's the proxy command that basically took, I wanted you to see the different little pieces because eventually I'm going to break those out. But if you just create the whole proxy command like I did there, then you get all of that. So now all you have to do is just start whittling away or adding in what you need. Okay. You okay June? Well I, so I've been writing proxy functions for years and I did not know this and I've just been copying all this stuff. Yeah, thank you. Wow. Okay, okay. It's the only way I know to create a proxy function. Now granted though, it's not necessarily documented very well. Well not a lot of people know about it.

Refining Proxy Script Parameters

So there's some things here that you would probably want to get rid of or fix. For example, this is all just right now a script. So let's say I was building, this is for a Get-Service, right, so I'm going to define my new function. Let's call it Get-Myservice. Alright, so let's look at some things we would have to fix manually and you're so welcome to kind of go through mam because maybe this will be mostly what you need. Because this is going to be my version of Get-Service, I don't really want the help link for online to go to Microsoft. You can, I suppose, but I'm going to delete that. I'm going to delete because I really don't need remoting capability, but I do like that it gives me, if the command has parameter sets that it concludes all of that parameter set information for me, including the default. That's good. I may go and add additional parameters or parameter sets, but at least what I'm starting with is good. I can then come here and I'm a big, and this is just a personal preference, I would prefer this to look like $Name without the curly braces. So you could go through and modify all of the parameters. If I'm building this and let's say, you know what, I don't want to have the required services parameter, I can just delete it. If I want to add a parameter, I can go through and add a parameter. When we get to some of my before and after, you'll see how I do that. I'm just going to kind of give you a feel for how this proxy function works. The main part is here in the begin block. So this line here is basically going to build a wrapped command that's going to call the underlying command. So the assumption is that wherever you're going to run this, whatever that underlying command is, it exists on the machine where you're running this, right? So we're going to take that wrapped command and this is basically just kind of a ScriptBlock that we're going to build here with $ScriptCmd. This line here. This ScriptBlock, it's really quite simple. All it's saying is take the underlying command, which is the wrapped cmd, invoke it, and then splat whatever comes through PSBoundParameters, which is just a hash table. You get that every time you have a parameter defined when they run the command. So often what I will do, just for debugging purposes, is I might have here in the begin block, I add things like write-verbose, (Typing) it has to be a string, so then when I run my cmdlet verbose, I can see what the PSBoundParameters were. It helps me when I'm trying to debug to figure out okay how come this isn't working because I want to be able to see what is going through. PSBoundParameters, are those just named parameters? Yes. They are named parameters. That's where sometimes it can be a little tricky. But that's why I do that, so I can see exactly what is coming through because if I set some defaults sometimes that can work, sometimes not, so I need to, I want to be able to see what is coming through the command, what's going to get passed to the command. And then, hold on just a moment before I jump the train of thought here, and then I will also if I need to, if I let's say I've added a new parameter and it's called $Test. I don't know what it's going to do, but I'm going to add some. What I will need to do though does Get-Service know anything about the Test parameter? No. So if I, but if I pass $Test into my new proxy command, it will fail because Get-Service says I don't know what -Test is because remember all I'm doing is splatting. Now I can still use that parameter and you'll see this in my, again my before and after shots, but what I tend to do then, well actually in my tend to do, you have to do is remove that parameter. There are things you can do to test it and if it exits and then remove it, I just get this to, I simply just do that. If it's there, because when you remove it, you get either true or false according to the pipeline, so I just do that. I just say remove it. I don't really care if I get a message or not, just make sure it's gone. I can still use $Test in my code later and what I typically will do is modify the wrapped command. So instead of using wrap command PSBoundParameters, I'm probably going to expand that or do whatever it is that I want to do. Most everything else will stay the same because at that point, all I'm going to do is just pass the parameters to the underlying command. Now there were some questions over here. You in front. You mentioned when you debug, you sometimes put out PSBoundParameters, but if you're actually using the debugger, PSBoundParameters still doesn't show up to that? Well I do it with a verbose statement. So I just run the command with -verbose and I can see it. I'm not, to be honest, I'm not a big, I rarely use -debug because I'm doing stuff with write-verbose. I put all my verbose messaging in from the very beginning, so I can see exactly what is going on. And one of the reasons I do that is if I build a tool for someone and they report I'm having a problem, it's not working, I can say start a transcript, run my command with -verbose, save the transcript, send it to me. Then I can see exactly what's going on. And because one thing you have to remember when you're building PowerShell scripts and tools is you always got to think about who is going to be running this. Is it going to be you? Is it going to be someone who barely can type two letters without making a typo? How will they use it? Will they assume to take something from one command and pipe it to your command? So you have to take those things into account. Do I need, and I've done this in some of my commands, well I'll have some verbose information at the very beginning that will also write out the computer name, the operating system, the user account, other kind of meta command information, so that helps me figure out, okay, why isn't this working for you. Oh, well you're running PowerShell v3 and for whatever reason this isn't working on version 3 or whatever. So you include as much verbose information as you want that can help you if you have to support or troubleshoot why your command is not working. And there was another question there. Yeah, for the PSBoundParameters.Remove, do you have to use the string of variable name? That is correct. PSBoundParameters is just a hash table, so you can use the dot method to test there were some methods to test if keys exist or not, I just remove it. And conversely, you can also, if you are splatting building another hash table, you can just add that. But in this case, for the proxy function, I need to remove anything that I add because the underlying command will have no concept of what that is. I can still use $Test, that variable, that parameter anywhere else in my code. It doesn't get rid of the parameter, it just removes it from PSBoundParameters. Oh and then the last thing with the proxy command, by default, and this is one of the, for me, one of the downsides, of creating a simple proxy command with the steps I just showed you is all I get is forwarded help. Ah, that's not going to help me a whole lot. Now I've got to go back and copy and paste and get the help and paste it in. That's too much work for me. I'm lazy. I don't want to have to work very hard. But that's what the kind of the layout of a proxy function. Before we get to my solution, which simplifies and takes us to the next level, any other questions over at least what this looks like? We'll get some examples where you can see the before and after and we'll run them, so you can kind of follow through. This is one of those things where I think the more you see it, the more it sinks in and you go oh now I get it, I'm seeing where you're going with that. So we're all good?

Tool Accelerators - Get-CommandMetadata

Alright, so let's come here and look at my accelerator tools. F8. Oop, get the right line. So the first script I have here is a function called Get-CommandMetadata. It kind of automates a lot of the steps that I just walked through. I actually have two commands we're going to look at. This is the first one. The second one takes this to a next level, but I want to show you this just so because this may be all you need for some of your needs. So what this is going to do, and we'll just skip the help here, you have to specify the name of a PowerShell command, you can specify the name of your new command if you want, and you can also specify if you want to include dynamic parameters. Alright, so one thing I didn't talk about in my previous examples, a number of cmdlets have what are called dynamic parameters, which means you don't necessarily see them unless some certain condition is met. It can make it tricky to try to identify those things when you're building a proxy command or doing it in my copying command process I'm going to show you. So I have a switch there that will say if there happened to be dynamic parameters, go ahead and include them as well. I almost always when I run this include that because I don't know for a fact which command might have a dynamic parameter or not. Now I do know through painful trial and error that a lot of the Active Directory cmdlets or pretty much all of the parameters are dynamic for whatever reason that escapes me, the developers on the AD team decided everything will be dynamic parameter. So when I was building some proxy commands for some Active Directory tools, that's how I realized, oh I have to include coverage for dynamic parameters and I'll show you how that works. Alright, so I'm kind of stepping through. If I do include an alias for the command, then I just resolve that command name because I need the actual underlying command in order to get all the command information, not the alias. And you'll notice here for example, talking about my write-verbose, and when I run this, I'll run it with verbose, so you can see I have a lot of verbose messages, so you can see if you were running this what every step is doing. The other advantage, kind of a sidebar here, with the verbose messages, documentation, right. The message, as you're reading through the script you can see oh that's what this next section is going to do. So if you build this all in as you're writing your script, when you're done, you've got your script documented and you have useful feedback information for debugging and troubleshooting. Alright, so we here on Line 65, we get that command Metadata. We saw that command before. I then create the NewName. If you specify NewName, I'll do that, otherwise I'll just use the existing name. Now this is kind of a tricky part. I had to add this from the Active Directory stuff. It's very possible to have a command that when you get the command Metadata, there are no parameters. That's what happens with the Active Directory cmdlets. But there are parameters there, right? But they're all dynamic. So I have some logic here that says if you do GetParamBlock and you find no parameters, let's assume that there probably are some and let's automatically, even though you didn't specify it, let's go ahead and get dynamic parameters. So I kind of take advantage and so you don't have to remember to do it, I force you to do it. Otherwise, you wouldn't see anything for it, at least the AD cmdlets. So if you, once you do that, if you get those parameters, even though nothing shows, you can use the Get-Enumerator and there is a property that you can eventually see of, it's a Boolean, of IsDynamic. So if enumerate the properties, or I'm sorry, enumerate the parameters and I detect that some are dynamic, I just add them to the hash table of my parameters property. So it's my kind of work-a-round to look at what's dynamic and then manually, unless I kind of copy them, up to the parent parameters hash table. That way I can get all of the parameters. I get the command help. Now I do this for you, no charge. Know the command help if I'm copying say Get-Service, all the examples will all say Get-Service. Well my command is going to be called Get-Myservice and I may want to use some of those same examples or some of the other text in the description. So I use a the replace operator and just replace Get-Service with my NewName. That seems pretty simple. Remember I'm lazy. I don't want to have to do a lot of hunting and replacing. So let's do that. I also set the HelpUri to null. I then create the command metadata. In this section here, okay hold on, it's regular expressions. Don't freak out. I'm not going to try to teach you that pattern, just know that it works. Huh? Yeah, I'm basically replacing the curly braces. I wrote this number years ago. I might be able to refine that regex, but it works, so I'm just going to let it be. So yeah, so I'm cleaning up because I also want the name to move up because I like having the string and then the name of the variable or the name of the parameter. So that's my little magic to make that happen. I also use a regex to find where that forwarded help, those forwarded help links are because I don't want them, because I'm going to copy and paste the actual help. So I use regex to get rid of that and replace that. Basically, wherever it finds the forwarded links, it just replaces it with null, basically deletes it. And then, at this part here, building a here string, this is the new command and all I'm doing with the here string is putting in all the information that I pulled from the proxy command and the parameters and just plugging it in and that is saved to a variable. If you happen to run this in the PowerShell ISE, which I always do, then I create a new tab and it inserts the text, otherwise, it just writes it to the pipeline and even create an alias for my command. So let's not source that and I'm going to make a copy of the, because I run Hyper-V, so I'm going to make a copy of the Get-VM cmdlet. And I don't think it has any dynamic parameters, so I'm going to go ahead and do dynamic, oh and let's, I said I was going to do this, let's turn on the -verbose and hopefully this will all work. Alright, so there's all the verbose output so I can see if something had failed somewhere along the way I would know at what point I had to go back and look. That's pretty simple. So here is the final command. I'm going to, notice, see I called it Get-VM, notice in the help, I changed all the help. I'm just going to hide the help for now. Oh, I take that back. My little magic isn't perfect. It does get rid of the curly braces, but it doesn't move it up. So, that's not that bad. I just have to come here, hit Delete a couple times, clean this up the way I want it to be. Those are all the parameters. And then I have the commands, the proxy command. And so I've got the help, it's all cleaned up, it has my names, all I have to do is again whittle away what I don't want and add what I do want.

Tool Accelerators - Copy-Command and Wrapper

So let's, let me see my next demo. I don't want to get too far ahead of myself here. We're going to come back. I'm going to do another version of this so you can see again the before and after. So that's Get-CommandMetadata, which is designed to create basically a rapid way of creating a proxy function that you can then modify. I realize that I want to go more that. I may want a wrapper function where I don't want the proxy, I'm probably still going to call the underlying command, but for whatever reason, I want to build a wrapper around it, but again, I don't want to have to try to find all the parameters and retype them and the help and all of that. So I built a second version of my script and I'm going to close some of these things here. This tool actually is on GitHub. So my GitHub repo is jdhitsolutions. There's also a link on my blog or ping me on Twitter if you can't find it. So let's see this is this command. We'll just hide the help. I also realize in after I finished that first function that even though it was getting rid of forwarded help, well there may be some instances where I do want to use forwarded help because I'm still going to create a new version of Get-Service. It's going to override and I'm still going to call it Get-Service and I want to show the forwarded help. I want to be able to include that as an option. So I have a parameter there with switch, so I go ahead and use it if it's there, so basically don't delete it. The default behavior for this command is to create a wrapper function or I can use that as proxy and it's going to do the same, a lot of the same code that I just showed you is in this command. The big differences here, this forwarded help, is that as I go through, again I'm building a here string, and if I'm specifying a proxy command, then it will go in and add the proxy pieces, otherwise, it just uses the different commands that I showed you with that proxy object, the proxy command class, to get the different pieces that I want and then manually paste them in. The difference being, so I'm entering the parameters, the big difference is when I define here on Line 220 that process, that is what will show up in the eventual command that I'm creating. So I can either create the proxy version of it or I can just build a wrapper that's going to run the command and then splat PSBoundParameters, at least as a starting point. You'll see some of my examples again before and after where I go a little bit further than that. And even in my copied commands, you'll notice I'm inserting write-verbose commands, so you have no choice, but to use them. Alright, so let's see how this works in action. Alright, let's clear that. So I'm going to run Get-Command, again with Get-VM called Get-MyVM -IncludeDynamic -AsProxy. This is basically, this will give me nope, no, no, no, no, no don't run the script. Now I need to do. (Typing) For those of you whoever do presentations like I'm doing where you're walking through a script, and I never do it, put a return keyword at the very beginning of your script file that you're walking through. So if you do it like I did and accidentally hit F5, it'll hit return and then stop. It won't run through all of my demo. I busted many a demo by not putting that in. Alright, so we are going to create basically recreate that proxy version of Get-VM. A few differences, I've inserted a little header metadata so it shows who created this and some copy of what command, who created it, when they created it, I know, I just personally I find that useful. I put in my standard disclaimer. I'm using the help. Help has all been modified. Oh, this version I fixed my issues with the parameters, so now I've got everything in line with the way that I want it to be. I have all of the parameter sets and the names. And now I have the Begin, Process, and End ScriptBlocks just like we had before with a proxy command. I can go through and modify this as necessary. So what I did, let's just, so this is the before. So you can see I have all the parameters that I would have running Get-VM and this is the after. So I'm going to reach in the oven, pull out the finished cake, which in, make sure I'm on the right line here, yeah. So in this case, I went through and cleaned up the help a little bit. It's always easier to delete stuff and then just copy and paste in my new examples. Oh this copy command script, that function, will also detect if the command that you are copying comes from a module, it will make that a requirement. So it detected that this was not from one of the standard out-of-the-box Microsoft.PowerShell modules and requires Hyper-V, so it automatically inserts that for you. Remember lazy. I don't want to have to think too much. So the reason that I wrote this particular command is I use Hyper-V all the time and I'm running Get-VM, but I want to know what VMs are running. I'm tired of having to type Get-VM, type it to Where-Object, where the State equals, so I just want a version of Get-VM that has a parameter, I can't remember what I called it now and I can't see that, called state, State= Running, so I've made that the default. You'll also notice and this once you go to add stuff, yeah that's where you've got to earn your paycheck, so I did a little work and I discovered the class name for that state is that, is an Enum. The advantage is, instead of making that a string or anything, by going that route and identifying the enumeration, I get IntelliSense. Okay. How cool is that? So that is a new parameter that Get-VM doesn't know about, so here's how we fix that. So I have some verbose messages. I remove that state parameter, right, because Get-VM is not going to know that from PSBoundParameters and then I, this is a really simple version that's why I'm showing it to you, I then modify the script command, basically say go ahead and run the initial command, but then filter it to where object State= Running. So I don't have to rely on having to type that all out. I can give this someone else and they want to say find me all the VMs that are paused, they can do that without having to understand how to pipe to where and know what to type because we've got the IntelliSense it's all braindead simple. That command that wrapped that script command then gets thrown to the pipeline. Everything else in Get-VM works just fine the way I expect it to be. Now what I don't know, oh let's see if that even works. (Working) Well let's do this here first just to show you. So get help. So all the parameters and stuff are copied over from Get-VM and you can see my state parameter and because I had that enum, PowerShell automatically expands all of that and shows me all the possible options. Kind of handy. And in my description I say that it's a copy. (Typing) Now what I don't know is if I can see on the network, let's find out here, there's my verbose, oh good it worked. Because I'm going, because this virtual machine I'm running is trying to connect to the Hyper-V host, which is my laptop itself and when you present and the way I have to stuff it up so I have a switch to make sure the networking all works and good it worked. So I didn't have to do anything and now I have all running VMs, -State. So very easily, I just created a copy of Get-VM, made my little tweaks, tweaked the help, done. It didn't take nearly as long as if I had started with an empty ps1 file and manually had to type all that stuff out. Questions over and this is kind of a one-way ways you would use a proxy function. I'm still calling the underlying command, but I have tweaked it in a way, in this case, I've added a parameter. Yeah. (Audience Comment) So let's say like Get-ADUser, do you always want it to be -prompt department title in edge or something, but so you're wrapping or you're doing it, now you're setting the whole value of the parameter that already exists. How would you go about wrapping it? He could use PS default parameter values and then you don't have to deal with… Yeah, no he's saying if I'm building a new command and I want to set a, well yeah, it should override that or you can just explicitly in your command because you can still splat something and specify additional parameters. It's not an either/or situation. What about these other ones that you sort have been provided because properties have showed they have to set other things and you're also setting that override to that. What, are you talking about the properties on the underlying as a user object? (Audience comment) Right. And so you said that your wrapper showed this specific value, but it's like calling it also tries to set that. What happens? It depends on how you've written your command. Hold on to that thought. I have some Active Directory examples we'll get to later. Maybe you'll see something there that will answer that. (Audience comment) They shouldn't be able to override anything unless they open up the script file and modify it. All they're going to do is see your command. Right, I don't see how that's the way to write that to make it work that way. Well you would just modify the help and assign, for example, here what I could do, let's go back to my I need, even though this is default, I set a default to the computer name, let's say I know that my, let's see if this will work. I know that my Hyper-V host is Chi-p50. Let's reload that. Invalid class. Oh, that may be an issue. Let's put in, so we're going off the off-script right now, but that's fine. That's how we learn things. I can't type and talk at the same time. (Typing) I also hope that I'm running the right version of this. (Typing) It's not passing through the computer name, which this is what someone was asking earlier. So let's because I didn't specify, so the PSBoundParameters, so what I might have to do then is and I don't want to totally delete, I don't want to totally ruin my command here, but maybe I will. We're going to do this. I'm going to, let's just comment out this parameter just because I don't want to have to modify my original command too much because this is kind of what you were talking about with the properties, right? I want for my command computer name to always be ChicagoP50. So I don't want to even make that an option for them, so I remove that from the parameter. So now what I can do here is I could add it, so let's do that. We'll just do PSBoundParameters Add Computername, right? And I should see that because I'm adding that before I display it. Let's clear this here. There we go. I'm not quite sure. Oh, because I have some additional code in there That's still relying on the Computername, so that's, but so that's what you could do. You would just remove from your version of Get-ADUser and then just hardcode in whatever other commands you want. Would another way of doing that be to examine the collection that you could use the value of the properties. Sure, I mean if you want you could leave properties in there and if it doesn't have, yeah, then you could add something. So an example is make sure that those three always give up regardless what else they have in the process. Right, a couple different ways that you could do that. Alright, let's move on here because I'm amazed an hour has gone by already. Or this is another version of Get-MyVM. Parameters and stuff are all the same, except this is now a wrapper function. So it's not a proxy, so basically all I'm doing is saying take whatever parameters come through PSBoundParameters and they're splatted in the process block and that's it. It took like no time to create an entire working function. Can you go back into the line you were…? Show me which line? The line that you select to amortize that. Oh, to create this? So I just ran my command, my copy command function, so that's when I can get VM, the NewName, include dynamic just in case, done. (Audience comment) Yeah, the proxy, if you do it as proxy, then it creates a proxy command version with a step-able pipeline and the wrap stuff. (Audience Comment) Right, the first command, Get-CommandMetadata that I wrote just does proxy functions only. This version Copy-Command because that's what it does. It allows me to copy an existing command and I can either create a proxy version of it or a wrapper version of it, which kind of brings me to my next question, can talk about which is better. Why would I want to use a proxy function and maybe why would I want to use a wrapper? Anyone? Yeah, well what you're doing if you don't use the step-able thing, I think you actually can go through the inside command multiple times, but the behavior's not always going to be the same. Okay. Which may or may not be, depending on what you need accomplished, that's something. And that's why I want to have this quick little discussion here as to because there may or may not, actually the only way you're going to know which is the right version is to probably do both and test. It may also depend on how much you need to add or delete. Do you need to put in stuff, I have to ensure that these certain things are met. How much more difficult is that? Proxy function I find a little harder to troubleshoot because everything's kind of nested in those step-able pipelines versus a wrapper function. I mean if you look at the wrapper versions that we just created in Untitled7, and my mouse, this is really simple. So there are a number of things that you have to, again, and also think about who's going to use the command. Are you going to be piping stuff to it? Will they need access to other commands or they may be part of like the Get-VM stuff, I don't know. So you have to kind of make those decisions, but I have a tool now that you can use to go both ways and you can decide and it does not take very long to create something totally new. So here is the finished version of Get-MyVM2. This is the wrapper version. So in this case, I hardcoded in the computer name, I removed the state because Get-VM doesn't understand that, and I'm not sure I tested this when I created this just the other day, so let's test it. The default behavior should be able to display all the running virtual machines on ChicagoP50. And again, I didn't put in my, oh let's do this with verbose because I bet I'm having that same issue with the PSBoundParameters. Oh, not clear screen verbose, that's kind of silly. (Audience comment) Yeah. Okay, so yeah that's why you've got to test everything and I modified it, but I never went back and tested it. So I need to go through and do the same things I did before. I'll probably just get rid of computer, well no because I still want to give them the option, but I want it to default, so I need to do some other magic, so I'll tweak this before I publish it or share it with all my demo stuff. You can see how I solve that. Because in this particular case, I want them to be able to specify something else in addition, but have it default to the current one, so I'll have to go back and look at that. Alright.

Tool Accelerators - Get-CimInstance

Okay, I'm going to quickly go through this. Because of variation on the copy and commands would be to create commands based on CIM classes. So instead of mucking around with CDXML, unless you just really hate yourself, create either proxy or wrapper functions using Get-CimInstance for whatever class you need. Now the nice thing about CimInstance and the reason I like the wrapper is because I then can very quickly either use computer name or CIM session. I don't have to try to generate the, write the parameters for that to happen. So I have some tools. So you could just, again, you could use the same idea of using copy command for Get-CimInstance and create a proxy function, let's call, I have a call Get-CimOS because the goal will be to create, so you use Get-CimInstance, Win32 operating system and return a preset collection of parameters, of properties, sorry. So the final version of that, that's the command, again going into the oven, is this. We'll just minimize the help. So I didn't have to write all that code for the parameters for Computername and CIM session. I have my verbose output, in this case, I'm adding particular parameters for the class name and the name space and I'm then splatting that and I'm also doing something a little extra where I'm inserting a type name for the output because at the end of my script here, I have some additional code that I wrote to update the type data and format data, so this is all in one. You could build this in a module. I have, just for the sake of demonstration, all in one scripts. So if I run that, that loads function, adds all that type information, and let's see if this will work now. Gets. So the default should be for the local computer name, you can do ComputerName. Nope, (Typing) nope, core01, not 0 that's why. And if I were to just show you, if I pipe that to Get-Member, you'll see that it's my custom type that added all that information. I could create CIM sessions and pipe them to Get-CimOS, so I wrote it in basically to mimic Get-CimInstance, but returns the variation I want, so I have a tool that I can give the help desk. Oh, if you need to find an operating system about the computer, run this command, give the computer name, you're done. So Get-CimInstance was kind of useful. And I also have oh this version, I also created a version that is a wrapper. There's really, and it does the same thing, there's really no benefit that I can tell one way or the other, at least in this particular case. Only that one might be easier to understand or troubleshoot if you're looking at code or kind of new to PowerShell. And I'm also going to share with you because I'm feeling like that kind of guy and we have the time,

Tool Accelerators - CIMScriptMaker

I have another tool accelerator that I call my CIMScriptMaker and this is going to use a lot of the same ideas about creating a wrapper function, except in this case, it is specifically for Get-CimInstance. And I'm just going to run it and show you how this works. Well you'll see the results. You can look through the code and discover how it works. (Clicking) And this is just, it only takes one parameter, the computer name, because you may want to build a command that's going to query a remote computer that has CIM classes that don't exist on your computer. What this script is going to do and this just kind of cheats and uses up GridView as an object picker, is it going to fail. Yeah. I think I have, yeah, yeah. First thing that the script does is enumerates all of the namespaces on your computer or whatever computer you specify, so I'm going to pick Root\CIMV2. It then enumerates all the classes within that namespace and I just use that GridView because that was the easiest GUI tool to an object picker. I could go through and do WinForms and WP app with that. I'm lazy, right. And I can find the class that I want, so let's do Win32, no let's do service. (Searching) I could filter, but I'm committed at this point. I have an option here if you want to test the class, which will basically enumerate or get an instance back. Most of the time, I'm going to assume that you know what the class is ahead of time, so I'm not going to test it. I can filter. So let's say I want to build it a command that's going to filter on the, oh let's see, for WMI it's the status, no is it StartName, no, the state. Where's state? Oh, here nice thing about our GridView, there we go. Click on the column headings. So I'm going to filter on state and by default, it's going to be equal and then this is just going to pop up a little VB script style and running. I can then select the properties I want to display. Let's just do description, Name, Status, StartMode, PathName, StartName, I don't know. And then it creates a new command. It saves to the clipboard and let's then come here and paste it in, generates a quick little help, puts in all of the parameters. Now I still may need to test this and tweak this a little bit, but all of that grunt work is done. I just had to go through some basically wizard to pick what I wanted, remember lazy. Now yeah, I mean, it took me some time to build that first tool, but now you don't have to do it, so win for you. So this is another way. Basically I built a wrapper function. I'm still running Get-CimInstance, but everything else that I want to do, it's all done for me. I didn't have to create any of this code really from scratch as you saw. Alright, questions before I get into the last part here and we look at some other before and after tools. No, we're all good? I was just curious. Did you ever build anything to wrap thread line executables? I have done things to take the output, yeah that's a little trickier because you're not really wrapping, yeah, I've done some stuff for like Netstat to build a command that would run Netstat and then parse the output, so you could do that. I don't have any cheats like these because every output would be different. And hopefully, there are PowerShell versions or tools to do what you might have been using a command line tool for. Not always, but. Alright, so here is,

Tool Accelerators - Get-MyADUser

let's run, take a look at Get-MyADUser. So I wrote this function. This is a, let's just hide the help here, and this was created using my copy command for Get-ADUser and it automatically got all of the default parameters because from being dynamic. Most of these should look the same, except I made a few changes, one, I wanted to be able to search for users in multiple OUs. The default for Get-ADUser, the search base only takes a single string. I wanted multiple locations, so I modified if you can see there and put in take an array. Why not? Who's in charge here, you know? And I also wanted the ability to exclude user accounts by some pattern. And you'll see I'll do a demo here, so you can see how that works. Everything else is pretty much copied over from Get-ADUser. What I had to do was remove the parameters that Get-ADUser doesn't know about like exclude. Now it knows about search base, but what I ended up doing here, now this is not a proxy function, this is a wrapper, so here's a reason and one of the reasons I did this was because notice I have a foreach that's basically looping through the collection of OUs. That would be really hard to do in that step-able pipeline thing trying to piece that out. This is much easier, at least for me. (Audience question) Actually, I think I wrote this someone posted something in a forum and they were trying to search for users in multiple locations. Why wouldn't they just not specify the script base? Because they didn't want to search the entire Active Directory OU, so they've got 100,000 users, but all they care about, the user could be in one of three OUs, which limits them to maybe only searching a 1,000 user accounts, so they wanted a way to fine tune and restrict where they searched. Does this work meaningfully different than simply putting my script bases in strings and then iterating in a foreach block and calling Get-ADUser? You could do that too. Exactly. But that requires you to have more knowledge of how PowerShell were to work. This runs faster than something like that or if its… I don't know if it necessarily runs faster, but it's easier to type, to run the command because I just have to run Get-MyADUser and specify the parameters. So all I'm doing is hiding all of, yes, what you could do manually. There's not necessarily a performance gain in terms of getting the end result. It's just a matter of ease of use for whoever might need to use this tool. So actually let's see if this is going to work. So let's dot source that. Okay, so I had, now this does require the Active Directory module. Now if I look at help for Get-ADUser, the original command, you'll notice original parameter for exclude, right. Nothing up my fancy sleeves here. But my version, my copy version, does have and I added help for it, which you should be doing, and you can also, I'm going to compare some parameters. So I'm going to get the parameters for Get-ADUser and my Get-MyADUser. It's another way if you want to prove what's different between two things. Alright, so exclude is the only difference. Although, I know for a fact that search base is different. So search base in the original is just a string and in my version you can see it's a collection of strings. That's just kind of to prove things. So I'm going to build a hash table. I want to filter on, I'm going to find all user accounts where the department is equal to engineering. I want to search in either of the employees or the research engineering locations and that's it. And I want to get the title and department back. So let's, this is not using the exclude piece. If I were to do this and splat that to Get-ADUser that fails because the search base says hey you're trying to tell me to do more than I'm designed to do. Fine. Be that way. I'm in control. There's the result. So I searched using Get-ADUser and I wrote my own little code very quickly that, when I made the copy of the command, the only code I had come up with was really just that little bit in the process block. All the other boring mundane stuff, which you have to have anyway, I copied over. So and that ran pretty quickly considering I have like 5,000 plus user accounts in my domain, and if you want to see how the exclude works, so I want to find all users in the domain, except, oh so the exclude is actually for a particular container, I want to find all users that have a name like Joseph, but skip anything that's in a testing OU because Get-ADUser has no way to do that and that's actually probably a more typical use case. I want to find something, but I want to ignore, there's no easy way to do that with Get-ADUser. It would take a long complicated command, so but I put that in. Another before and after.

Tool Accelerators - Get-MyHistory and Tee-MyObject

I wrote a, I copied Get-History and came up with a new version I call Get-MyHistory and what I have done is added parameters to specify a regex or unique because by default, Get-History will show me everything and I had two use cases, one I want to do Get-History, but I want to filter out all the duplicates, so just show me the unique ones. Yes, I know. You could do that manually typing out the long command, but I'm just going to screw that up more than likely or my other use case was I know I wrote a command and it had this pattern, so I wrote a parameter and added some code into the wrapper to use the regular expressions to get the history where the pattern matches the command line. Again, I don't want to have to try to remember how to do all of this manually at the console or rely on someone else to know what to type. I want to build a tool that they can use using copy command, again, generating all the boring mundane stuff then I just had to write the little fun pieces like regex. Oh, I also wrote another version called Show-History (scrolling) that will, I'm not quite sure what that does. Well you can play with that. Yeah. Well we can look at it. I was just going to demo get Get-MyHistory. So I can find all of the commands that have a pattern and I wanted to know show me all the lines where I defined a variable. There we go. Now, yeah you have to know what the regex pattern is, but that was really easy to type to run the command and I built a better wheel. I did another so I'm not going to do the copy command. I'll just go right to the final object, the final result. F8. So I built another. This was before version 5 came out. How many of you have ever used Tee-Object? That's a pretty hand cmdlet that I don't think a lot of people take advantage of. With Tee-Object, you can run it and get the command at the console and you can save it to either a variable or to a file. I'm not satisfied. I said I need it to also go to the clipboard because I do a lot of writing and often I need to run the cmdlet and I need to copy the results to a clipboard, so I can paste it into my document that I'm working on, and I don't want to have to go extra steps to run the command and then copy it. I want to be able to see it, so I wrote a version of called Tee-MyObject that adds a parameter to specify the clipboard as an option. So I just copied Tee-Object and went through and added a parameter. I defined a new parameter set and I also ended up specifying the width through some trial and error. If I do the clipboard, I also, in this case, again remember I wrote this before version 5, so I'm not using any of the v5 clipboard cmdlets, so I'm using the actual Windows form, some .NET stuff to do the clipboard copy. You could modify. What I found gives you some other regular expression stuff. When you copy stuff to the clipboard, I was getting a lot of empty space at the end, which screwed up when I paste it into a document, so I've got some regex to clean that up. If I added all that, again this is just a wrapper, but now if I run this and let's just do, oh let's do this, get-myvm Tee-MyObject -Clipboard. (Running) There we go. So it still, so I wasn't satisfied with the wheel that Tee-Object gave me, so it was close, it was so close. All they had to do was add one more parameter and I would be happy. Well Microsoft is not going to make that change for me, so I did it. A lot of the stuff that I write either is a result of something that I need for either training, or because of writing, or something that comes across in a forum and a lot of people would probably have that issue and I can use it then as a teachable moment. So like the Active Directory example came out of a form problem said ah, that's kind of an interesting, let me see what I can come up with and now I have something, you'll have something you can actually use. Uh oh, they're having fun next door. Questions, that was kind of the end of my official demos. Let's go back to the slides just so I can be totally efficient here or at least cover all the bases. We can always come back to this. So hopefully you saw a little magic there. Questions over anything that I showed you.

Q&A

Question. Well, I'll just have to show you. I was curious if you've ever seen this. Oh, you'd be surprised. So one of the things that I tried to do a while ago is try to duplicate the behavior of the dir command you see in .exe in PowerShell then a bunch of people who have absolutely not used PowerShell solely because they're used to dir and all the weird-ass ways you could use it in a cdn.exe and of course that was just like an unending... Is it because they wanted to use a slash instead of the dash? They wanted a slash. They wanted to put a space between the slash between dir/. Yeah, there's no way you can to, PowerShell's parameters are going to expect a dash, and so there's no way around that. But I haven't seen anything that comes close like those people had. I don't know. Has anyone else seen anything like that? One way to do it is to open up PowerShell and start with the cmd. (Audience comments) Yeah, I thought that I had written some other directory. I know I have a number of commands, scripts, and functions to get better results for the dir command, not necessarily as a complete replacement, although I thought I had something. I'll look. It was a case where we were finding ourselves having to write this in the scripts, intern developers and both things were conceived DWDSC and PowerShell. They just need to get over it. Sometimes is easier to fix the person. Easy to say, harder to do. Other questions, yeah. This is mainly more of a comment than a question. What you're doing here is really exciting, by the way. It fits into the general category of code generation, right, to automatically generate code. One of the knocks against code generation has always been that you've made this new copy of a bunch of code and then you tweak it here, you tweak it there, but now you're sort of stuck. So if the original mechanism by which you generated that copy changes like you showed how you could, erases were made, so you're now stuck and you have to reapply all that stuff. Yeah, if you build a command that's built on say a Microsoft cmdlet and Microsoft and the next version of PowerShell introduces a breaking change, yeah, you may have to go back and absolutely. The only note is that the more of these automatic tweaks you can build into your original script and the less you have to tweak afterwards, the better off you are to have to rerun this thing later. And that would be a situation where using a proxy command is maybe a better approach because you're not relying so much on the command. You're going to just send it the parameters. Your changes go in modifying that script CMD ScriptBlock. So if you are, if that is a concern, then a proxy approach would probably be the better way to go. But certainly. And I write stuff like this primarily because a lot of IT pros don't like to type, they're not very good at it, they know they should write a script, but you guys are all crunched for time. You want, and this is from people that post in a forum, I need a script to do X. Well we're not going to write a script for you, but let me give you something that will help speed up the process. So I'm always trying to find shortcuts and ways to take the pain away and the grunt work so that you will put in comment-based help because you don't have to do anything. It's just going to copy. You just have to then edit what's already there and editing is a whole lot easier than typing it from scratch, right? So I'm assuming everyone here's basically IT pros and you're crunched for time, you needed this yesterday, and I don't have time to sit and write a complete thing from scratch, I don't, I'm still learning. You still need to know PowerShell and understand some of the things, but you could run my command to generate the new version of your wheel and hopefully then have enough information or if you get stuck then you post in the forum and say I've got this version of my, I'm building this script that basically is wrapped around Get-ADUser and I'm running into a problem. How can I get around that? So then you just focus on the little pieces that you want to change and not worry about the rest of the stuff that you need. There's another question over here. Somebody just stretching their arm. Here. When you brought the dynamic parameters over, any logic associated with those dynamic parameters was maintained there? Oh, that's a good question. Because if you've got something like Get-ADUser, it doesn't have any parameters associated with that. No, that's a good point. The and I'll have to test that, for example, the dir command has dynamic parameters for a file and directory. If you're in the file system, you get those parameters, but not if you're in say the registry. So if I were to build a well let's just ah, how much time do we have here? I'll work on that. The include dynamic parameters is at least getting the name. It's not actually building a true dynamic parameter, which a true dynamic parameter has logic, has some sort of code to say if this condition is met, then bring this along. The main reason I had the include dynamic was to get around the Active Directory issues because for whatever reason those cmdlets are, and I don't know why, well because I know what they're doing, but I don't know what logic is saying make this dynamic because it's not dynamic in the way like I'm in the file system, so make this parameter available. They've decided that all of their parameters were just going to be defined as dynamic, which is a little bit different. But you're right. I don't know if my copy command, I know it doesn't make a true dynamic parameter, it just gets the name, identifies it. So you could still make it dynamic if you need to. You'd have to go back and add that piece back in. That's a very good point. Other questions? Is any of this useful or magical? As I said I do all my writing and pretty much at petri or on my blog. PowerShell in Depth, 2nd Edition. Oh we know, I guess people brought books, which is kind of a good thing because this is one of the few times that Don, and Richard, and myself are all in the same time zone and the same location. So if you've brought books for people to sign, I meant to tweet that out, do it because this is the time. And this is you know other people should've told them like bring your Mark books and he'll sign. I see some books down here. So this is a great time. So PowerShell Deep Dives, I've going to give a plug here for this book. This is also published by Manning. I was the editor, which I will never do again, a variety of PowerShell MVPs, Microsoft people, people very active in the PowerShell community. This book grew out of some of the original PowerShell deep dives in summits. This is a book that is basically this conference, but in a book. So the chapters are all written by someone active in PowerShell. They're all little niche topics. You will not find that information anywhere else and what made this kind of special is no one got paid for this. All the proceeds for this book go to charity, so you should buy two copies. If you go to Manning and order and buy the book there on the Manning website, you can get the, I believe, you get all the eBook versions for free. So don't go to Amazon if you want the eBook versions. Anyway, so a plug for that because it's for charity and you will stuff about PowerShell that you won't learn anywhere else. And then my contact information. If you need me, I'll be here the rest of the day and all day tomorrow. That's where I blog. You're welcome to email me. I have some business cards if you need that sort of thing. I'm pretty active on Twitter. You can kind of find me on Google+ every other day or something. On Twitter, let me also mention this, Adam Bertram and I on the first Friday of every month host, "host" kind of a PowerShell chat on Twitter. That's first Friday of every month 1pm Eastern time to 2pm, for about an hour. So we just get online. We use the PS Tweet chat hashtag and we just kind of answer questions or chat and find out what people are working on. So if you want to hand out or even just kind of lurk and see what people are working first Friday of every month. I try to send out some tweets before that to remind people, but something else to think about. Alright. Thank you very much.

Domain-Specific Languages in PowerShell: A Brief History and an Ongoing Experiment

Introduction: What Is a Domain Specific Language?

I'm really glad you guys came. I'm glad there's a size crowd because this is a topic that I'm particularly passionate about. I do a lot of, I'm very curious when it comes to PowerShell and programming languages in general, and so I do a lot of deep dives when I start thinking about crazy ideas, I wonder how this works, I wonder if I could do that, and I go down the rabbit hole, and I disappear for a few months or sometimes a few years, and come back later on with something neat. And so this is one of those neat things because I've gone down the rabbit hole a few times. And I've talked about this topic at some other events in the past where afterwards I feel like I flew a plane over their head around 50,000 feet, but those weren't PowerShell-specific events, so it wasn't necessarily the right target audience for this kind of presentation, so I'm really glad to bring it here. I hope you brought your thinking caps. When I'm doing content at any session, it doesn't really matter if it's PowerShell related or not, I take questions at any time and I'll try to do my best to answer those questions during the session, and if I can't, I'll push it until the end or maybe do a sidebar outside of the session and figure stuff out. So feel free to bring questions up at any time as I'm going through this content and yeah, hopefully you guys have fun and it'll make you think a little bit. So domain-specific languages are interesting to me because they offer a lot of value above and beyond what you get from native PowerShell. And so I want to talk about I guess a bit of a history of DSLs in PowerShell, which is relatively short, and a bit of an ongoing experiment. First, for those of you who don't know me, who am I. My name's Kirk Munro, and back in well pre 2006, I used to work on MAPE and LDAP layer programming inside of a product called Message _____ Quest and then I went to an Exchange 12 conference here in Redmond where they started talking about this thing called Monad and kind of got bit by the bug because all of a sudden I could make guesses and be right 80-90% of the time and that never happens in MAPE or LDAP. So that really kind of peaked my interest and ever since then I haven't looked back. I've had my career totally focused on PowerShell for going on 10 years now. I grabbed the nickname years ago, Poshoholic. If you want to contact me for anything, that's my Twitter handle, poshoholic@gmail.com, poshoholic@hotmail.com, poshoholic.com, anywhere that I'm around on the web, that's typically the nickname that I use aside from a few places. So on GitHub, you'll just look up Kirk Munro and just be careful of the spelling of Munro because it's not one of the more common ones, and you'll find my content upon GitHub up there, but that's kind of how you can reach out to me. So I mentioned we're going to talk about domain-specific languages and I'm going to do a couple of different demos showing you the kind of things you can do and I need to bring all this with a bit of a caveat. So this is experimental because there is no official public domain-specific language support in PowerShell today in version 5. They started doing stuff under the covers with some public .NET interfaces in version 4. That's where it first started to coming to light. Some domain-specific language work was done prior to version 4 just using functions and what you could do with PowerShell commands, I've done that, but experimental is key on this because it's not a fully-defined process. So I wanted to bring this here to you to get you thinking about it and to start discussions, maybe make some relationships that continue beyond this event that you guys can talk to me about stuff or collaborate on different things on GitHub and what not and see where we can take it because I think it's really interesting and very compelling with what you can do with it. So even though it's experimental, it's worthwhile bringing to the table to talk about now. So what is a domain-specific language? Excuse me. A domain-specific language, I mean this is just from Wikipedia, it's specialized, so it's domain specific and it maybe an entire language. You might have a domain-specific language that is off on its own and the only thing you can do is what is defined for that domain, but it may also be embedded within another language in the case of PowerShell because PowerShell you have cmdlets, functions, aliases, and all of that goodness, but then you can also embed these little mini languages that you can intersperse with regular PowerShell and that give you some benefits above and beyond what you get from normal PowerShell commands, so that's what I'm going to be talking about is that kind of a integrated language. Yeah, so they're not general purposed. They're very focused on a particular task. Why are they important? They're important because well these four reasons, so simplicity. Domain-specific languages can take something that would be very complicated to do normally and it might require a whole lot of code otherwise and boil it down into a very simple presentation so that you can digest it and consume it and do things with it. Familiarity is key because you're coming from often with a domain-specific language, you're working with a certain set of concepts and features and keywords and different things that you have in mind from what the area of topic that's being covered by the domain-specific language is, and so you can bring those concepts in and use them in a familiar manner just so you can make intelligent guesses and do the right thing. Power because you can, typically, they obscure a lot, a lot of code behind the scenes with just a very small package and structure. Structure is an interesting one and it doesn't apply to every domain-specific language, but it applies to a lot. When you're dealing with structure, well key domain-specific languages in PowerShell today, Desired State Configuration, you're dealing with structure that defines your network infrastructure and how it's laid out, and so it applies very well. The concept of domain-specific language applies very well to DSC and so DSC is a domain-specific language. And any place where you're dealing with structured output, so certain documents have structure, or like I mentioned infrastructure itself, domain-specific language is worth considering for those solutions and you'll see some of the reasons why as I go on through the talk. How can they be implemented? They can be implemented in a whole bunch of different ways. So I started tinkering with domain-specific languages back in PowerShell v1, I think, about just using functions and aliases and cmdlets and defining things on the fly and doing a lot of metadata programming, but they also could be implemented using native functionality in PowerShell 4 or later, and so the key piece to that is System.Management.Automation.Language.DynamicKeyword. Dynamic keyword is a class that has a bunch of static methods and that's where all the magic is intended to happen and that was added for the purpose of DSC, but technically, when they added it, they didn't really talk about it a whole lot because it was only half baked and they didn't really want anybody going into it and figuring it out and I ignored all that, went into it, and figured it out and figured out what was missing and added stuff, and so I've done a lot of tinkering around with dynamic keyword and my stuff is still in an in-progress state, so I'm going to show some things that are not published on PowerShell gallery, but are only GitHub right now because it's still evolving and until I get it to the point where I think yeah, okay, I can push it there more broadly, then I'll push it on the gallery, but for now, it's still is a work in progress and experimental, as I mentioned before. Questions so far? No.

My First Domain Specific Language

The first demo I'm going to go through is the first domain-specific language that I wrote in PowerShell and that was back in my Power GUI days. So PowerGUI is a PowerShell editor from years gone by and it was created by Quest and back when I said I started working on PowerShell, I was fortunate enough to hear about this team that had Quest working on this product, so I changed from mapping LDAP stuff to doing PowerShell by joining the PowerGUI team and did development and then evangelism, and architecture, and eventually product management before they put it on the shelf because it was free and not making money for them and they didn't like that. And so during that time, and there were two versions of it, there was PowerGUI and there was the VESI for Virtualization EcoShell and they were both the same thing, one was just a skin version of the other. How many people, by the way, know PowerGUI in the room? Great! Okay. I ask it sometimes and I see like two hands and stuff, so I'm thinking oh really it's that old, but no, this is cool that you guys still know about it. So yeah, so back in those days when I used to work on power packs and adding PowerGUI for those of you who don't know, one of the features it had was a management shell-like user interface that was driven by PowerShell, and so you could add nodes and put your data in the middle and have actions for the data on the right-hand side much like you see in MMC and modern management tools and with that I did some tinkering with some domain-specific language work and I'll show you what that looks like. So the first thing I'm going to do is load this ps1 file and just to show you how old this is, I haven't gone and brought this forth into the world of modules yet. This is just a ps1 script file that I dot-sourced because it contains a lot of functions and it doesn't use a lot of the modern even PowerShell v2 features just because it's that old, but I still keep it kicking around. I intend to bring it forward with this DSL work that I do, it's just not quite there yet. But it's interesting to show that you can do this using the old PowerShell techniques of doing things. So when I load it, it adds all these huge amount of functions and it's all about Visio, right. So it's fun when you're working with Visio to do automatic generation of documents based on something and the most common example that I used to do back in the day was based on your infrastructure, specifically on your virtualization infrastructure. So I could take this and I could automatically generate infrastructure of well VMware at the time, that's what my focus was when I started doing this work, and it would show your host and then all the virtual machines all lined up in this diagram that's drawn on the screen for you while you run the command and so it's kind of I guess sexy in terms of technology just in terms of seeing the UI show up on the screen while you do it and it was fun to do. But if you look at these, this is not domain-specific because these are just PowerShell commands, verb-noun, so you could go through and create Visio documents using these just like you could any PowerShell set of commands today and that was useful and I did it this way intentionally and actually this is probably how I would create a lot of DSLs because this allows me to do one-off things by having the verb-noun structure, but if I'm doing something entirely from scratch and I'm not just taking a Visio document that already exists and going in and tweaking something on it, that's where the DSV comes in. So let me show you an example of what that looks like. So, yeah, let me show you this file. I mean you guys can look at this file and I'll share all this content. I'll send it to Richard and he'll make it available to you. So this file contains the Visio commands. By the way, just on the experimental line, these commands, I haven't gone through and tested this a whole lot with PowerShell v5 and Visio 2016 just because it hasn't been a high priority for me, but I still have I've got it working for Visio 2016 and PowerShell 5, so if you run this stuff and you find bugs in it, that's not a surprise. Don't think well why is he releasing this stuff if it's in this state. Technically, I'm not really releasing it. I'm sharing it as a set of examples and kind of things you guys can do with it, and so take that caveat and feel free to experiment and take a look, but don't necessarily build your production on it and have your manager come upset when it's not working. So an example of how this might look in the DSL. If you notice, let me flip back for a second, the key piece, the entry point into the DSL is right at the very bottom of this set of commands so that alias of Visio to the Use-Visio command, that lets me jump into this kind of magic. So look at the top example here. You can see the structure of the document being defined in the language itself, and so this is where domain-specific comes to play, right, because I'm working with Visio, so the domain is Visio, and anywhere where I'm dealing with Visio elements, those are the key points that I'm going to be talking about in my structured domain-specific language. So I can create a new, I can open up Visio with a Visio command and then I can define some defaults, the default background and the default page theme, and by the way the default background doesn't work in 2016 right now. It'll just generate a warning. And then I can go and I can create a new document. So then you can see I go into this document with a ScriptBlock, and in that document, I create a page, and then inside the page, I set the background and the diagram style, and start tinkering with the properties and all of this using something totally different from verb-noun that lets me kind of really just look at the points using terminology that I'm already familiar with inside of Visio. But at the same time, I can intersperse regular PowerShell because it's just PowerShell. So all I'm doing here is running PowerShell, even though I have this domain-specific language and even though these commands that show up like DefaultPageBackground and page and all those things are defined on the fly as I run this upper Visio command so that I can, all it's doing really is taking the aliases to that huge list of cmdlets that I had, right. So I'm just, as I go inside of Visio, the first thing I do is define a bunch of aliases to the things that are relevant at that point in the hierarchy and then as I go into document, the same thing, and so I kind of control scope and visibility just by doing some alias turning things on and off. And then inside of that, I can intersperse PowerShell. So here I have some script that's just going to work with my file system. I'm not really doing anything like large scale infrastructure, but I can. This is a simple example where I wanted to show you how this works. So I'm going to take that and I'll run it. And if the demo gods are nice to me, it'll work. And by the way, the reason why I'm running this inside of the outside PowerShell session and not in ISE is again, the whole experimental thing. I can throw this away and continue with the rest of my content without worrying about it being mucked up. So I'll just copy and paste that in and why did that paste like that. Hang on. Right-click-based. Better. So now it's running and you see Visio opened up in the background and let me see if I can quick go to it. Where'd it go? There. So that was kind of jumping in and progress, but that's the kind of magic that happens because it's doing a lot of auto layout on the fly, right, so you can picture this being larger, going across a whole lot of infrastructure, maybe going to a huge set of Hyper-V hosts that are discovered through SEVMM or going into Azure and bring in virtual machines or anything related to just figuring out what you have for your infrastructure and doing things that you have to do to create documents that kind of manage that and show that to management who doesn't get PowerShell and doesn't know how to go and ask those kind of questions and giving it to you in a way that's automatic. And so you run this kind of a command and it just goes and does the PowerShell work to interrogate the system and then generates the document on the fly and that's the magic of a DSL. And this is again just using the old school PowerShell functions and commands. And if you look at another DSL that's out there, Pester. Pester doesn't use, as far as I'm aware, the internet keyword stuff. They're just using functions and commands inside and aliases and what not to make the magic happen. So this allows you to do that without getting into what I think will become the go-to way of doing domain-specific languages going forward. Cool? Could a provider be considered of the main system language? A provider could be, but it's generalized. So go back to the definition, I mean domain-specific, because, it's kind of both, right, because a provider is specific to the way that you access the different data silos, but yet generalized because it covers so many different silos and anybody can create a provider for any source. So yeah, it was a great question and I was thinking about talking about providers, but I kind of hesitated because it's a bit of both, I guess, because you have Get-Command, Get-Child, sorry, Get-Item, Get-ChildItem, all those common sets of core commands, there's like 30 of them, and they work against providers, and so that's specific to the provider model, but there's no domain-specificity to the--- specificity, thank you---to the internal silo that you're working with. So if you're working with SQL Server, the only parts related to a domain they might see are the labels like tables for the tables container. You might see some of that show up in the structure, but it's not quite the same thing. Oh, and also, going back to the same Visio example, so here's another example and this is just showing backing off a little bit from the domain-specific language itself how I could call directly into the commands to do the same thing and I could technically write this using the cmdlets, but I like the declarative nature of the structure for things so that when I'm drawing out in my head the diagram of the content, I can sort of draw it on the page just using this terminology. Useful? Yeah, very. Okay, good. So back. Now I've just got a whole series of demos, so I'm going to jump back and forth between slides and demos. So that was my first language that I did domains for, and so as I mentioned, that was back in the PowerGUI days and this is fun. My son, this long, long time ago back when he was a kid because he knew I was really into PowerGUI and so he made that for me one day when I was away on a trip, which is kind of fun. Kids are great, right, because you just learn so much from how they do things differently than how we do things. So my son is into automation, but not at all using PowerShell. His kind of automation is in Minecraft where he creates these huge farms to generate these blocks that he uses throughout the rest of his world's Minecraft and he's on YouTube, and he's got his channel, and he's got like a 100 videos posted, and so his automation, I don't understand at all because I look at those blocks and the red stone and how that all connects and I wonder how is he wrap his head around that because I don't want to read text, but anyway, it's just fun to see how automation applies in different ways. Oh, and I guess it's not fair if I don't mention my daughter. So my daughter, she's more managerial, so her automation is more making sure I'm up at 6:00 in the morning because she wants me to automate making her lunch for her for that day for school.

System.Management.Automation.Language.DynamicKeyword

So another demo. So let's talk about the---hum? (Audience comment) Yeah, so the class, dynamic keyword. So let me jump over to the second one. So the dynamic keyword class is a public class and it's controlled by a bunch of static members, so I'm just going to run these one at a time. So it's available in PowerShell v4 and later. In PowerShell's version 4, you're not going to see all of these because there's a couple new ones that are added. I'll get to that in a second. But you can get the members, you can use Get-Member-Static to see what's available on it and then let me just jump back. So by default, in PowerShell there are no keywords. If you want to see what keywords are available in the system, you just run Get-Keyword, but by default there are none. If you ran that from inside of DSC, you would see stuff because it uses keywords, but otherwise, you don't see any and the configuration keyword is technology not a dynamic one so that the entry point doesn't show up. Now so all the management of keywords is done using these static methods and these are the ones that are available in version 4, so AddKeyword to define a new keyword, and by the way, dynamic keyword has a constructor that takes no parameters and then you just set a bunch of properties on it. (Audience comment) Yeah, yeah, so you can just do hash table creation, yeah, so that's really easy. So there's AddKeyword, there's ContainsKeyword to see what keywords are currently on the system, Get to either look up all of them with no parameters or a string to get, excuse me, a specific keyword, and then of course, removing them by name. Now I didn't add this to my slides or to my script, but I should add it. I'm just going to create a new one, so k. I'll do it without the hash table just so you can see the properties. So there's a keyword, and so on keywords, it's got a whole bunch of different properties that are dealing with how do you or Get-Metadata and how do you define that and make languages and what not, so I mean there's a meta statement, there's parameters for the keyword, there's some parsing stuff in there, and to be honest, because this stuff is not documented, I certainly haven't figured it all out. All of my work has been through spelunking and trial and error and this is kind of like a hidden goodie in PowerShell that it's public, but it's really a work in progress. I'm talking to some team members on the PowerShell team about ideas I have around how this could be done and I know that the way I am doing it right now is probably not the way that they recommend doing it, but it's based on what I figured out and got working, so you can kind of look in through this and figure out how it works. I'm not going to go through and explain these things because I probably would explain them wrong because of the fact that it's not documented and is based on when I figured them out. I can have conversations about that outside of this session pertaining to the code that I've written, which is public and on GitHub and I can show you and you can take a look at it, but I don't want to go into the specifics of those today because I think that's a bit of a black hole. So those are the ones that are used for management, and then of course, you could also clear the keyword list at any time just by calling reset and the new ones that were added in version 5 that I just kind of discovered the other day, are Push and Pop. And so one of the things I discovered when I was tinkering around with this in PowerShell v4 that was an interesting challenge is that internally there is a single dynamic keyword table that uses just a set of strings to identify the keywords. And so if you have multiple languages that have the same keyword in different context, that's not going to work because you have one internal table. So that was one of the challenges that I had to work around, so I created my own table and I'm constantly shifting keywords in and out of the table that is currently being used by the language in the work that I'm doing. So if you look through my code and you see some of that stuff, that's what that's there for. Since then, version 5, and I'll makeshift my work to version 5 because I've just tried to move forward with where this stuff is going. Typically, I'm the kind of guy who likes to do down level support as much as I can, but this is so half-baked that I'm eager to move it forward leverage what they do with it going forward. But push and pop, which I haven't tinkered around with yet, I can only guess and actually I looked at the code using Reflector, that what they do is one takes the current set of keywords and pushes it down a level so that you can start with a brand new set, which is important when you're going through different layers of script blocks inside of a language and then pop as you come back up those layers. For those who aren't developers, can you talk about like .NET Reflector and IoSpy. Sure. They're really easy to use and you don't need to be a developer to understand either to use them or to understand the output. Yeah, so it's a great point. So I mean, everything these days or so many things these days are going open source. You see all the news about different parts of different modules in PowerShell going source and different parts in .NET going open source and Xamarin announcements, and all this goodness, they're publishing the source codes, so you can just go and read it. But what about when you don't have that source code? So PowerShell itself is not open source. You can use tools like IoSpy or Reflector, Reflector is a commercial tool, IoSpy is not, and you can load DLLs inside of those tools and it's going, then that tool will internally inspect the contents of those DLLs, show you the classes, give it to you in a searchable UI, and you can browse through and just really dig through and figure stuff out and that's a huge learning tool or it's great for troubleshooting bugs, great for just figuring out maybe some undocumented features because documentation certainly is not always up-to-date with what commands can actually do, and so those are good tools. Thanks June for that comment. Those are good tools to use and I use Reflector every other day, if not every day just going in and digging around and figuring stuff out. And power comes from understanding those things. Now so that's a high level look at what dynamic keyword has to offer in terms of interface and there's a lot of functionality that I'm still working on, like I mentioned, but I'm happy to work on with people if people want to work on that with me.

Redefining How DSLs Are Created with LanguagePx

Now that I've gone through that class itself, I want to talk about how that class and how work I've done with that class allows me to redefine how DSLs are created. So I work on a whole bunch of modules and these days all the modules I work on I put the suffix of Px at the end, which Px kind of looks at a glance, maybe if I squint, like Rx, kind of like remedies or prescriptive solutions and that what these are, so a lot of the modules I create, actually all the modules I create, so you'll find if you do a search for StartPx on the PowerShell gallery, you'll find a lot of content from me and on GitHub some things are not on the gallery yet like LanguagePx, that's my solution. So I create this module, LanguagePx and I want to show you what that allows me to do. Who here knows about Doug Fink's ImportExcel module? Have you guys seen that? A good number of people. So ImportExcel is cool because ImportExcel is also about doc creation. You've got content and you want to push that into an Excel spreadsheet and maybe have some pivot tables and some charts associated with that and do an automated way, and so Doug's been investing time and effort in there and that's been quite popular and people really like it. And so, when he first started showing that content and showing the commands that are available, I immediately thought of the DSL work because of the work I've done in that space. So let me just load that module up and I'll show you what the command set looks like. So it's got a whole bunch of commands in there, stuff dealing with ranges and stuff dealing with conditional tests and Excel charts and plots and pie charts and all these different things. And that's the functionality that's available in module, but when he first released this, the key thing that he was showing was one particular command. He was showing the command Export-Excel and some demos that he threw up quick videos on Twitter. So Export-Excel allows you to take content and export it from PowerShell into an Excel document much like Export-CSV, you can export to a CSV file. And if you look at it, right now it's got a lot of parameters and more being added over time as he adds more and more features to this. And so that's interesting because it allows me to do what I want to do, but I really don't personally I don't love the dash this, dash that, so on, having to use a splatting to get the work done. With all these different parameters, I prefer thinking about Excel documents in structure, in the structure that I'm creating them in. So yeah, so here's an example showing you the pipeline approach to creating an Excel document, so I'm just going to call Get-Process and then I'm going to export that into an Excel document with pivot data showing the pivoting on the working set size, pivoting on rows, I'll have the name for the rows in my pivot table, it's going to include a chart, and so and I run this. It's going to take a second. While that's running, I'll close down my Visio because I don't need that anymore. So this is going out and just getting the process on my system and then using a .NET library that's inside of this module and oh hang on a second. There we go. That's interesting. I just updated this module like yesterday and I wonder if that's why. I should not have updated it before the demo. I didn't really update this one. I did update -module and it got all of them. So okay, so plan B. So I don't want to troubleshoot this right now. I mean I did a debugging session yesterday and that might have been appropriate for doing some of this content, going through these errors. So I'm not going to troubleshoot that just this second. I'm going to keep to talking about the structure and the language and how I think that I've been able to improve on it. Oh, you know what? I wonder if it's because I've got the document on my system. Where's my path? Mydocuments test.xls. (Typing) Okay. Let's try it one more time. That may have very well been why. Whoops. Try again. By the way, I don't know if you noticed when I just made that shift to there back to this. So PowerShell drives are crazy powerful and if you are finding yourself jumping back and forth in locations inside of PowerShell a lot, I recommend that you go into your profile and create PS drives because they allow things to be done much more easily than usual. So I have, there we go, so I have inside of PowerShell this documents drive for my documents, I have modules for my modules folder, all modules goes to ProgramFiles\WindowsPowershell\Modules and so on. And so I use these to jump around my file system a whole lot and feel free to email me or ask questions about that if you want to know how to set that up because it's a big timesaver. So there, that was why because I had the file already on disk. So it creates this document right, which is kind of interesting, so it's got a big table here showing a bunch of processes on my system. I mean the data is not really interesting because it's an ugly pie chart and it's not really that meaningful, but I wanted a simple example. So that works today and I'm not going to save it. And I want to make sure that doesn't exist anymore. So that kind of shows you the process working just inside of a regular PowerShell-like syntax and then you can switch, right? And so you can switch to splatting and so splatting is interesting because all of a sudden what you're dealing with large command sets, you can boil that up into this nice little hash table, so you see params and then I've got a bunch of values and if you don't know the way splatting works, when you see on the Line 26 right there, Get-Process | Export-Excel @params, @params tells PowerShell take that hash table and anything that are keys are parameters names and anything that are values are the values that I'm going to use for those parameters. And so it makes it nice to be able to take something long like what you see on Line 14 that runs off my screen and shorten it up in a format that you can see in a document, so I use splatting all the time for that reason. And so, anyway, this is the exact same command just moving to a splatting format, which I'm not going to run because you saw the output, but this is better because here I can see path and I can see some properties like show and auto size and so it's a bit more structural, but it's still not quite where I want it to go yet. And so, I've got this LanguagePx module, so let me load that and I'll show you want it contains. It doesn't contain a whole lot. It contains invoke keyword and so the invoke keyword piece is the command that gets invoked by aliases. So trying to explain this. When I create a DSL, all the commands in the DSL technically, they show up in PowerShell as aliases. Those aliases are all mapped to invoke keyword, which in turn looks and checks what was the alias name used to call me? Oh, that's the keyword you want to invoke, and so that's how that maps between the alias and the actual internal dynamic keyword that is going to be found and run. New-DomainSpecificLanguage allows me to define a domain-specific language because I didn't want to just create domain-specific languages, I wanted to create a domain-specific language that allows me to create domain-specific languages, so up one more level, which is really cool and powerful. And so, I've got this New-DomainSpecificLanguage command and then there's Register-DslKeywordEvent. So as a keyword is being run, there are two different types of events that can happen and those events are associated with script blocks through Register-DslKeywordEvent and I'll show you that, and then Remove if I want to take a domain-specific language back out again. Now here's an example of when I saw what Doug had done what it might look like to do the same thing using a domain-specific language, well technically using this domain-specific language to create a domain-specific language. So I can define with New-DomainSpecificLanguage, which is part of LanguagePx, a language called ExcelPx, and inside of that, I've got ExcelDocMagic and these are just arbitrary values I pick, right. So ExcelDocMagic is what it's going to be, it's the name that I gave for this language. It takes a name, that name filed is actually the name of the document that it's going to create, and then it's got some properties that are general document-specific properties like do I want to show it or not, do I want it to auto size or not, and then it's got the data, so PivotData. And inside of PivotData, I've got row properties and the aggregate value for the PivotData and whether or not I want to include a chart and which is kind of fun because when you look at this versus the other syntax, the command syntax, including a chart only applies if you're also including a pivot table and you learn that by looking at the documentation for Doug's commands for ExportExcel and you can figure okay well I can't do -IncludeChart unless I'm doing -IncludePivotTable, but a syntax like this makes that implicit because you have the structure because you can't define IncludeChart unless you're inside of the PivotData and if you're inside of PivotData, that's because you have a pivot table. So this allows you to make sure that the pieces are lined up the right way, especially when you're dealing with commands that have very large sets of parameters. This allows you to kind of wrap it up in a bundle and make it look better. And then so here's the event handler. So Register-DslKeywordEvent and so I'll show you this. It's actually really straightforward. So Line 51 down to the end of the curly brace on 62. So DSL name is ExcelPx and the keyword path is ExcelDocMagic. So this is me telling PowerShell that whenever ExcelDocMagic is invoked, I want it to fire this event and there's two different types of events that I have support for right now, OnInvoking and OnInvoked. So you might have some stuff that you want to have happen as the command's being invoked and you might have some stuff that you want to have happen after the entire thing's been invoked, which is important because if I scroll back up a little, I told you I was going to hoping you guys were going to bring your thinking caps. I get a feeling I'm making some heads hurt, but that's okay. So you see ExcelDocMagic at the top and then inside there's properties and pivot data and what not and there's no events down here below associated with those guys because that's just structure. I could create a language where individual keywords have things that happen, have code that executes behind the scenes each step along the way, but I might also create one where really all I'm defining a structure and then at the end, I want to take that structure, which is just a hash table of hash tables, I could just step through it and get information from it and then do things with it, so that's what this scenario is, OnInvoked. After the entire thing is done, this ExcelDocMagic command runs, I'm going to invoke this action and the action all I'm doing again is the splat table I showed you earlier? This is just building a new hash table to splat in to import Excel. Question. Within your structure, which level are the keywords? Is ExcelDocMagic the keyword? That's the first one, yes. Okay, but then properties and PivotData are also keywords? Correct. And so that's because of this nested structure, that's where, that's why I was happy to see that they push and pull or push and pop to the dynamic keyword object because you have to deal with the fact that you're dealing, the different scopes as you go down deeper and then come back up again, and so that's exactly how that is defined. I create internally a large table that has paths and then based on where you are in the path, I take keywords like properties and I say okay well properties is available now. So this comes with, sorry one more question, so PowerShell doesn't store that as ExcelDocMagic.properties. No. It just stores properties. Correct. Yep. And then ExcelDocMagic is actually a type. No, it's actually a keyword. It's not a type. It is a keyword. Yeah, so all the ExcelDocMagic and Properties and then Show and AutoSize, so the difference here, some of these are properties, so where you see I have, I'm following a syntax that looks like C#, right? So Boolean and then some values or strings and some values and so I support some very common, yet primitive data types for associating values in property bags, so property is defining a property bag, it will have .Show and .AutoSize on it, and if you look below where I have my hash table, Show = $_.Properties.Show. Right. I'm all good with that. Name follows ExcelDocMagic? Yes. So what I'm wondering is so all of that is the value of name? What is name? Oh, so if looked, go down a little further. So, let me. I'm stuck on name here. That's okay. So ExcelDocMagic, here's the execution of that syntax up top. So name is just my path to my xls file, so I'm just giving it an identifier and then internally I can refer to that and actually do stuff, so in my code. But when you go, can you keep up scrolling to that function and I'll tell you why I was, I might not be the only person. I might, but I might not be the only person. In all other cases when you have something like RowPropertyName, it's type is string, and string proceeds it on the line, right, so the pattern here is the name and then the value. Correct. So it's, sorry, type, the type or the type of the property and then the value. Yes. And ExcelDocMagic receives name, so I'm thinking that that's a type. Right. I get what you're saying. It is actually a definition of the keyword. Well it's actually a property of the keyword. So when you have a keyword, so this is again all new experimental stuff and so these are great points. So ExcelDocMagic is a keyword. It has a property called name, which is just arbitrarily a string because that's the way that I've figured out how to do things internally, and then inside of ExcelDocMagic, it creates two other keywords, properties and pivot data, and each of those keywords are just property bags that have values that are type associated with them, but the actually properties or parameters, they're almost like parameters, for keywords like ExcelDocMagic, those are just a string, they're just arbitrary, so don't think about having to type those. The way this works right now, I only use types in property bag. I could probably update this to have more of a bracket structure with types and values if that would work. I have to think about that. That's good feedback, but does that answer your question? Make sense? It does. Okay. I think some of the confusion might be that maybe we didn't realize that new domain-specific languages use LanguagePx, not part of the system automation language .NET. I mean, it's not built into these. No, it's part of LanguagePx and internally it implements and uses the dynamic language keyword, right? So that's how I'm doing all of my magic to make this happen, and so yeah down here I register the event handler to do work, which is just calling the same code I had up top and then the bottom you can see the syntax to actually go through and run that, so it could just call ExcelDocMagic. In name, I give it the name of the file that I want it to actually work with and then I just set my properties, and go, and it's done. Which I like the syntax. I mean, how many people see the value in a domain-specific language syntax versus just having verb-noun commands? Yeah? Okay. I mean, it's a preference thing. A lot of people think well yeah but you really get verb-noun, that makes more sense to me and they can use that, which is great and it still works especially if you wrap those in your DSL so that you can use either/or, which you can here, but I like the DSL syntax. I think it makes a lot more sense when you're dealing with structured information. It allows me to separate getting my values and getting my content for my structured document from the details describing the document itself. Sorry, one more question. Are these your, is where PowerShell is concerned, the things inside of the curly braces are ScriptBlocks, right? Yes. I mean, it's the same deal like Pester or… Yep. And you had a question. Yeah. I notice when you registered the DSL, you actually have Get-Process. Do you ever abstract that or would you create specific keywords for particular actions? Oh, yeah so I could. This is right now, you're right, this was a simple example, kind of a proof of concept that I threw this together when I saw Doug's work just kind of showing what you can do, but you're right, ideally, Get-Process should happen external to this because you don't want to bake your data into the DSL itself, so it's lockdown. You're right. So can you put parameters into the action ScriptBlock? Well yes, so you could have more than just name or I could have, I could modify the DSL itself so that inside of my, inside of PivotData that I define a data set and that the data set goes there, right, so inside of there I could have some PivotData and then define my data value and have that equal to Get-Property, so it's pulled outside of the DSL itself. Yeah, yeah, so there's different ways. It depends on how you structure it. I think the fundamental question there might have been can you have parameters in an event handler? So, no. Right now you can't. Yeah, your event handler is taking the data that you get from the structure of the DSL itself, so the way you would parameterize that is by having the DSL have some argument or some value that it can work with and then internally, your event handler is going to know to take any data source, any data type and do the job.

ScsmMpcPx - A More Complicated Example

So one of the things I do, I work for Provance Technologies. Provance Technologies does IT asset management and now we do full IT system management, and so within that work, we've done a lot of work with System Center and we've plugged into Service Manager and we have this whole solution of asset management inside of Service Manager and one of the things that we wanted to do was to add auto close functionality, which is a really straightforward feature that's just missing in a service manager, but I didn't want to just create a management pack the normal way because creating management packs involves XAML and XAML and compiling workflows and building DLLs and a whole lot of steps that are just really complicated. I wanted to make it so that somebody could create a management pack more easily, so I built this DSL using my DSL, my LanguagePx module called ManagementPackConfiguration and so it defines a management pack and then it sets some attributes on that management pack and then build options, admin settings, which creates a UI, and so this is quite extensive and you can look at it as an example. I'm not going to run it on my system because I have nothing related to Server Manager on this system right now, but I wanted to include it as an example showing how big you can get, right, I mean I showed you a small example and this is a much larger example of a DSL and the kind of things you can do with it, but it's pretty powerful and pretty cool and it's great fun to watch this happen because I see DLLs being compiled and built, and placed in the right structure, and then putting it into a management pack, and then into a management pack bundle, and code siding, and so many steps, again, but done in a simpler way, right. So it's about peeling back the onion and getting down to just defining the content. So on the resources slide, that last piece that I mentioned is from ScsmMpcPx, so if you go into GitHub, that's where you can take a look at that larger example. LanguagePx is on GitHub, that's the main module I was talking about, and of course, ImportExcel you can grab from the PowerShell gallery. And if you want to contact me for anything, there's my contact information, which I already mentioned at the beginning. So thanks and if you do see these people while you're here at this event, tip your hat to them because these guys have been on PowerShell for a heck of a long time. It's a great product, a great foundation, so give them credit for their work because it's fantastic. And that's it. Thank you.

Managing the Infrastructure Stack with PowerShell

Introduction

My name is Josh Atwell and I am here to talk to you about managing your infrastructure stack with PowerShell. Little disclaimer, I'm not going to show you a bunch of code. I'm not going to teach you how to use a cmdlet. I'm not going to show you clever little things that I've done with PowerShell. That's not entirely true. I'll show you a couple of little things. But what we're going to talk about today and this is something that I very much want you to be engaged with me, point out where I have gaps, and expand on this. This is a conversation starter and we're going to talk about a lot of the challenges and the capabilities of PowerShell when you look at the total infrastructure stack. Okay? If you're on Twitter, I highly encourage you to use the PowerShell Summit hashtag. You can mock me and heckle me all you want. That's fine. I'm accustomed to that. Throw it on Twitter and we'll have some fun with it and I just thought POSHit was just a fantastic hashtag for so many reasons, so I threw it up there. You can use it or not. I'm good either way. But as I said, I'm Josh Atwell. I'm a developer advocate at SolidFire, which is now part of NetApp. And we have the big thanks to people. How many people were using PowerShell right at the very beginning? Throw your hands up. Alright, how many started with version 2? And how many of you do not like raising your hands during presentations? Excellent. I like to _____ my audience and make sure that I don't count any of your lack of votes. You guys are boring. I'm just kidding. I'm not name calling here. So thanks to those folks. So a little bit about me and I have two hours, so I'm going to talk about myself as much as possible. I'm kidding there too. But there's some things that I think are really important before we get started so that we can level set on why this topic was something I submitted an I thought we should talk about. I started out my career working for a small civil engineering firm where I did desktop support and then server support and started virtualization. It's where I got started with PowerShell. My intent was to automate some tasks. It started with Exchange and I think a lot of people who got started early on, Exchange was that first integration that they worked with, right, I mean can I get an Amen? Right. That was when Microsoft really legitimized PowerShell as a tool that you should invest time and energy in and I did that. And VMware then released the vi toolkit and we're going to get into a lot of these integrations here in a minute, but as part of this narrative, it's important to understand that once the VI Toolkit became a thing, now there was this opportunity to do something much broader and much more impactful in the environment than simply managing my mailboxes and accounts and all that stuff, right. So that took me into CARQUEST general parts where I was responsible for basically rearchitecting the virtual environment and enabling the development team, right. How do I speed up their efforts? We have this really interesting thing we call today called DevOps, right. This was something, and Luc and I've had a conversation on this, DevOps isn't something new. It's something that we've kind of already done. It's just we have new tools and new methodologies to talk about it. I'll talk about that a little bit more in a little bit. Going into Cisco IT from General Parts, I went from managing hundreds of virtual machines to tens of thousands of virtual machines and in that process, there was an element where the manager says to me we need to automate, so we don't have to have extra headcount and we need you to do that, right. And when you start looking at an environment that size, you have to take things into consideration that you don't normally have to consider because you have time and we're going to talk about a lot of those things in this session about how you can leverage PowerShell for multiple layers of the data center, some of the challenges that are associated with that, and then how you address them. And then now I'm at SolidFire. We were recently acquired my NetApp and in my role as developer advocate, I still write a little bit of code from time to time, but I'm primarily focused on getting people together who develop and work with our integrations and work in the infrastructure stack that are trying to solve problems whether it's DevOps, whether it's just LeanOps and trying to improve your operations regardless of what they're doing and try to help make them more successful. So I share the things that they do, I elevate the conversation, and I try to get people engaged, which makes doing things like this fun, except for I'm like oh crap, I need to write some more code.

Why PowerShell the Stack?

So let's get into why we would PowerShell the stack, right. I almost feel like it was challenge that somebody has like can you get Salt N Pepa into a slide? Yes, I can. So we're going to posh it real good. Alright? And here's the thing, right, why would you use PowerShell across the whole stack? In your environments, let me just ask real quick, in your environments, do you have control over multiple layers of your ecosystem that you could automate with PowerShell? Yes. Right. That's becoming more of the norm. We hear a lot of conversations about silos. Frankly, I don't like silos. I mean, I like the fact that they hold corn and corn is a primary ingredient of bourbon and I'm a fan of that part of it, but when it comes down to organizational structures, I really prefer looking at it as disciplines, right, and when you break down that concept of there being some separation between you and a peer or how you look at your infrastructure, you now have opportunities to share and contribute with one another. And so I did this poll on Twitter and if you're on Twitter and you want to, go to my profile, it's pinned. You can go ahead and contribute and then I can pull that up at the very end to see how this number changes. But what I wanted to know is how many layers of your infrastructure stack do you manage with PowerShell currently? And I won't lie, I was a little bit surprised to have that many people claiming seven plus layers. Think about that. Seven layers of infrastructure, right. And we'll get into how I define that in just a minute, but seven components, seven layers of your infrastructure managing with PowerShell. You know ten years ago if you would have asked anybody that was getting started with PowerShell if we would be seeing that kind of result, I think that people would be a little bit bearish on that. Jeffrey Snover will probably be bullish on that, as he should be. But I also noted that the majority of people only have about three to four layers that they manage and I think the fundamental element of that is based on what they have access to, right, what they were able to automate and engage with PowerShell because this is what our environments look like, right, and I tried to be heterogeneous if you will and demonstrate there's more than just because I think most everybody here is VMware vSphere, we've got some Hyper-Vs, maybe a couple OpenStacks. Anybody running OpenStack in here? Okay. Have you used the OpenStack PowerShell? Okay, so it exists. Some folks at Rackspace built PowerShell modules for OpenStack, right. And when we look at our ecosystem now, we have a few things that are really prevalent that we have to identify. The fact that we have more layers. It used to be your applications were directly on physical hardware. Alright, and we still see some of that, but we now have a lot more layers, a lot more components and one thing that's really important there is that in that complexity, there's a lot of interdependency. Anybody carry a pager? Right and there's something that goes wrong. What's the first thing you have to do? You've got to figure out what was impacted. Right, you've got to figure out where it came down to. You're doing your root, you're trying to find out the root cause and what is going on and there's lots of layers that you have to consider before you can identify exactly where the issue is, well unless it's blatant like storage going down or a network just going poof and disappearing, right. But with these additional layers, you have that additional complexity, right. You have dependencies that we didn't have in the past, and so it's really important for us to take advantage of the capabilities from a PowerShell standpoint in order to deal with that. And then finally, another thing that I see in the work that I do from day to day is that our consumption models of our resources is changing fundamentally, right. We are very quickly moving away from this world where someone calls you and asks you to provision something, right. We're moving more to a world where people are enabled to do their own provisioning, they're enabled to make their own changes in their environment. We're delivering IT as a service, right. So what we're seeking now is opportunities to manage at scale and when I say scale, I don't necessarily mean Cisco IT scale, right. Your scale is completely relevant to what you are doing in your environment. So if you only have a 100 virtual machines or you've got 10,000 virtual machines, you still have issues with scale, right. You still have your constraints and the things that you're trying to do and what you're limited to varies, right. We all have our limitations and things that we have to work against. And then you look at where you have opportunities where you want to implement DevOps methodologies and tools and you're looking at like this complicated stack and you're saying how do I even get started, like what can I do? How do I start moving towards this and how can I leverage things that I already know? And we're going to talk about that today as well. And I've already touched on it, right. We want more IT as a service. We want to give people an opportunity to be self-empowered to do the things that they need and not being completely reliant on submitting a ticket for everything that they do.

Infrastructure Tools and Use Cases

Alright, so let's start with some of the tools. Well here's the thing, right, we're all PowerShellers and when we look at the things that we're trying to accomplish, a lot of the things that I mentioned earlier that we're seeking, the first thing that we think to do is can I do it with PowerShell. It's not always the best tool for it. I would argue that generally it's a fantastic tool for doing these things, but for us as PowerShellers, that is our native first response, like can I do it with PowerShell. And one thing that is fundamentally changed over the last five years is that in our infrastructure we have more APIs. More products are coming to market and we are hitting refresh cycles in our data centers where we're bringing these products in, where there is more extensibility, right, APIs are made available that will allow you to remotely manage and configure and take care of your infrastructure, and as such, we're seeing a lot more growth in PowerShell modules that support that. Now this is not all-inclusive, in fact, everybody take a minute, absorb it, and then we're going to have a sharing session where you can tell me the things I missed so that I can include it later. And to be fair, right, I work for SolidFire with NetApp, right, we're a storage company, but I'll include competitors on here because I think it's fantastic that PowerShell has become so pervasive in our data center that we're able to do so many things with it, right, and when you look at this, right, you've got your PowerShell, and then you got the Hypervisor stack, and of course, Hyper-VM is in there. I kind of consider that rolled into PowerShell. Right. You've got some server technology. I think Dell has a little bit of stuff, right. You've got storage or you have storage and then you have things from the community where people are producing things and the reason they're doing this is because there's an API. There is extensibility in our data center unlike what we've had in the past and it is allowing us to tackle these challenges and to jump on new things and deliver new capabilities in our data centers, and in this, we're going to talk about how these layout, right? So this is the stack that we talked about and when you look, as I mentioned, PowerShell is everywhere, right. Most of these layers can be controlled with PowerShell one way or the other. I found the dead zone and I don't know why I put that slide in there twice. I just thought you liked it and so there we go. It's a great slide. So community involvement. What did I miss? Just speak up. Did I miss one that you use? I have used Dell EqualLogic. Dell EqualLogic. Revel of Intentry. Revel of Intentry. Excellent. Okay, so I did pretty well then. Excellent. Nutanix. Nutanix. Yep, excellent. Dell. Dell. Yep. System Center. System Center. Pier Storage. Pier Storage, got it. That's it. Yep. Azure. Azure. That's right. Any others that you use? AWS. AWS. Excellent. Alright, so for those listening at home and people that are here, there is a lot that is possible, right, but there are also a lot of challenges. Well we'll start with use cases, right. What do we do with this big toolbox of PowerShell capabilities? I have a meming problem. It's a fun problem. First step is admitting it. Alright, so when you talk about PowerShell in the stack, see I worked really hard on this graphic, it's not really complicated, but I'm just really proud of it, so I just want it on every slide that I can, right. When you talk about PowerShell in the stack, there are a few low-hanging fruits, right. You've got reporting, right, everybody's reporting, and the interesting thing is when you talk about reporting is there's a lot of different things that you might be interested in seeing from a report and we'll talk about a couple of those in a second. Implementation is another. Being able to go out into your environment and configure net new resources, looking at DevOps, how do you reduce time in the value string. How are you able to improve the effectiveness of your development team and the consistency of your operations team leveraging PowerShell? Maintaining configuration, obviously desired state configuration is a great example of that. And then also we're going to touch on extending tools and these are tools that you already use that are able to take advantage of PowerShell in managing your data center stack. So the first example we'll look at is from a reporting standpoint and from an implementation and configuration standpoint. If you were running a data center today, there is a high likelihood that you're running multiple VLANs. There's a high likelihood that you're running jumbo frames and if you or any, and I see some heads nodding, so if you're in an environment where you're doing this, has anybody ever seen a good situation from a VLAN not being consistent up and down the stack and being exposed. It never ends well and it's a bear to find sometimes, like where is this traffic dropping? Why am I not being able to get access to this application? Same with MTU, right, when you're looking at jumbo frames for storage or for whatever the application is, if it's not configured consistently everywhere, you've got problems and the challenging thing is depending on the architecture and the infrastructure that you have, you may not have PowerShell up and down that, but look at how many layers are involved, right. If you were dealing with a VLAN and it needs to be a specific VLAN for an application, that's a lot of layers to have to check and you have to deal with that with troubleshooting. And at the end of this talk, I'm going to share an idea that I've had on how to help improve this. So another great example storage. Anybody have storage in their environment? If you have virtualization, you have storage. Storage is fundamentally critical in virtualized environments. It is one of the key things that enable virtualization in the first place. The ability to have a shared resource that's accessible to multiple hosts, and when you look at that, there are some significant complexities around identifying, especially if you're dealing with performance and capacity issues, which let's be honest, those are the storage issues that we deal with regularly, right, to be able to identify that this application has got a problem and where is that problem? Like where in the stack do we identify that? And when you're looking at your entire infrastructure stack and you're trying to get gain visibility in what's going on, understanding the configuration, understanding the relationships that the different layers of your infrastructure have and leveraging PowerShell to do it, the most important thing that you can understand is that right there, that relationship, right. What is that piece of information that connects one layer to the next? And for most people when they're looking at managing their infrastructure as a whole, this is one of the biggest challenges that they have. How do I identify what correlates that data store with this volume, right, or that VMDK to that Datastore, that Datastore to that volume. Alright, what is that? And then this, I used SolidFire for this one because I already wrote this code and I didn't have to write new code. But what we're looking at here is I need to understand what the volume is that's supporting that Datastore and this is the Lun, like this is the physical storage that's being presented to the Hypervisor to be shared so that we can provide resources to that application. And in doing that, I have to identify what piece of information is important and available both on the PowerCLI side, the VMware side, and the SolidFire side that makes the connection, and in this case, it's the SCSI Id, right, the NAAdeviceID, and by going to PowerCLI, right, and we use the Get-ScsiLun by going into PowerCLI, we know the Datastore that we want, we collect, we find the CanonicalName, which incidentally matches the NAAID, right, for the SolidFire side, we get that, we do a little manipulation with splitting it, we get that part of the array, and we store that. And when I go to find out what the volume is, I'm doing a Where statement, right. So Where, I'm getting all of the volumes and saying where this value equals that. Now is there anything in this that just throws up a red flag of inefficiency it? I would get them all. Exactly. Right. Why? Because it's easy. That's why. Right, but and then also with some implementations, and this is something that I really wanted to point out. Anyway, I mean this is I guess you would consider a negative on the SolidFire side, right. We don't have a strong filtering mechanism. Why? Because the API was not designed to do that, right. The API just considers it sufficient. Like it's okay if you pull it all to get that, right. That's not neither good nor bad, but when you're thinking about a large environment, think about what it would take to do a report when you have 100 Datastores and how much time that would take, okay. So another use case, and it's in the book, just for you Luc. Okay, so when I was looking at contributing to that book, I was trying to think from a VMware perspective, why would I care about DSC? Because at the time, the work that Luc Dekens has done around DSC and VMware did not exist. There were no resources. There were no capabilities, and it wasn't at the time you ask the question, well what would you want DSC to do. Right. Still very much in that exploratory phase. So I have this little bit of a conundrum. Alright, so how do I demonstrate the value of DSC that would be useful to someone who manages the VMware environment. And so how many was it that manage VMware environments? You don't count. You said you don't raise your hand at sessions. Okay, so what I came up with is this concept of the DMZ, right, now that's not the concept I created. The demilitarized zone, this is where its front-end, right, it's behind a firewall, so it doesn't touch your regular infrastructure environment, it's virtualized, right, so it's sitting on ESXi, it's managed by Virtual Center Server, these are VMs, they're on a VLAN that is dedicated to the DMZ, so they have isolation, right, maybe not air gap, but anyway it's still isolation, right. And you don't want to open a bunch of ports because every time you open a port, that's an opportunity for penetration from the outside, right. You want your firewall as locked down as possible. So how the hell do you use DSC in this environment? Let's just say we've got hundreds of machines sitting there servicing a web application that is critical to your business and your businesses success. How do you deal with that? How do you implement DSC? Well, I'm mean you can put your pull server out there, but that's a bad idea, right. You talk about honey pot. Somebody gets a hold of that, you're toast. Right. They can configure whatever they want, so you don't want to do that. So this hurt my head. It hurt my head a lot. Like how would I accomplish this? Okay, so we have the DSC pull server, it's sitting inside of our network, but we don't want to open up the ports, so how do you get it to communicate? Anybody have a guess? Except for Luc. Invoke VM script. Invoke VM script. That is a great way of doing that. So yeah, you leverage the VMware tools. Right. For those who aren't aware, PowerCLI and VMware vSphere, you know with the VMware tools that are on a virtual machine, you can actually remotely execute through the VMware tools. It goes through the ESXi host directly into the guest and it leverages the VMware tools to do things. Alright. And so what I did and this is all available on GitHub, right, if you want to see the full code. I just pulled snippets to describe, right. What I ended up having to do is we create the MOF file inside of the network, we copy that MOF file using the, what was it, Copy-VMGuestFile, right, we copy that file into that and then we use Invoke-VMScript to execute DSC and to execute on the MOF file, right to get the configuration. Now recognizing that this doesn't allow that LCM to be able to reach out and say hey am I still good, right. This definitely you're pushing it out to the application's system, but it does enable that capability, right. You are able to go in, configure that, and get it out there without having to open up all the ports. So the key thing here is you have to use multiple different layers of PowerShell to accomplish this and I will point out that the original idea and the concept was I would just put the code from building the MOF file and everything and do Invoke-VMScript. Well you're only allowed to send one line of code with Invoke-VMScript, right. You can have it call a script file and have it do other things, but you're only allowed to send one line of code and that just doesn't work with DSC. Alright, it's not going to happen. Okay. So this was an example of how to leverage PowerShell in an environment against multiple different layers in order to solve a problem. Alright, and it was helpful because I'd already written a code, so I didn't have to write a whole bunch of code.

PowerActions

I don't want to teach you guys cmdlets, alright. Like you guys know cmdlets, you know how to use Get-Help right. So the next thing that I want to make sure that you're familiar with and introduce and I thought I had a picture on this, but apparently I don't. So I upload up a vin, I'll get connected, and show you how PowerActions work. So PowerActions is a VMware fling and if you go to labs.vmware.com/flings, search for PowerActions, what PowerActions allows you to do is you install it in your vSphere WebClient, and once installed, it provides a PowerShell console in the vSphere WebClient. Now it's powered and backed by a server, a Windows Server that's running PowerShell, but what it does is it enables you to execute PowerShell directly through the VStore WebClient and the screenshot that I thought was here was a beautiful representation of this, so we'll get that in a minute. But the other thing it allows you to do is it allows you to create scripts and files, right, that you can share, your create functions that you can share within the VStore WebClient that will allow you to be able to execute against something. Use case for this would be, I don't know about you guys, but when I go into a VStore environment and I see all the storage and there's Datastore, Datastore (1), Datastore (2), Datastore (3), Datastore (4), Datastore (5) because every time you install ESXi, it just calls the local disk Datastore. I'm a little OCD. This bothers me. Right. So there's a PowerAction I've created and put out where you right-click on the Datastore and you say set the local name, and it goes out, it pulls ESXi hostname, and renames the Datastore to that name-local. Alright, so now you have logical information. Now that's just an example. You can also do things such as being able to select a network or select a Datastore and be able to query and say what MTU is set up and down the stack, right. It'll check the switch, it'll check the port, it'll check like Cisco UCS and the networking there and you can have that send a report. So if you were in that troubleshooting situation and you remember we had that full layer, that stack that we had to deal with, this gives you an opportunity to go in and take care of that and get that information and have it embedded directly in the VStore WebClient, which is a tool that most of us are already using anyway. Alright, so you're not have to jump around. Everybody going to go check that out? It's not supported, so be mindful where you put it. It's not officially supported, but it's pretty cool.

Core Challenges

Alright, I was kidding. We're not going to talk about cmdlets. I was just kidding. We're going to talk about some challenges. So here's some core challenges that you have to deal with. Yeah, I got problems. So a lot of the core challenges that you have to deal with are how to manage the different modules and tools, and the snap-ins that are made available. The relationship mapping, which I've touched on, I want to get into a lot more detail about, and then dealing with problems that are associated with scale. So we're going to talk about like as you go to implement some of this stuff, these are some of the pitfalls that you're going to run into. So we'll start with multiple providers. Now there are two types of ways that companies will provide PowerShell capability. Right. Traditionally, it's been PowerShell snap-ins. Well because that was the only way they could do it, right, that was the only real option. Fortunately, most providers have moved over to delivering a module. Now it's still developed in Visual Studio, still developed with C# .NET, it's compiled and it's provided out to you, so it's kind of locked. You can't get into it, but it is a module, right, that you can load. And the thing that's really important to remember there is that with a module it can be transported, right, it's transferrable. You can put that on any system, any system that runs PowerShell that needs to perform a task, right, so if you have multiple layers of the stack, you can extract these modules, put them in a central location, or add them as I describe here to the PS module path so that when the session loads, that module is available, right, and you can then leverage it along with others at the same time. So if you're looking at managing VMware in Cisco UCS, and SolidFire, or Pure or whatever, you can have all of those in the same spot. Well this is really, really handy because it's transportable, you can put that on any system and you can now do a level of reporting, right, an execution that you wouldn't have been able to do otherwise. Because if you're having to deal with multiple tools in Windows, yeah everybody likes to talk about the single pane in class. I think PowerShell's like the ultimate single pane, right. It's the single pane of kick ass. I'm quoting that. It's going on a t-shirt. PowerShell single pane of kick ass, right. So I'll show you an example of that, right, so you have your storage and you're wanting to query for that information like we talked about, and for this one, like with the VLAN MTU, your common piece of information is the MAC address, the MAC address that goes from the Cisco UCS port that's being delivered to the ESXi host, you go out and get that, and as I show it here, and I'd thought I had shown the full cmdlet, but you can go in and that information is accessible, right, that's neat, it's straightforward, but you end up doing what I had mentioned here, you collect it all, you filter, and then you grab the piece that you really want. This is wildly inefficient, right. So the concept I wanted to share today, and I've talked way faster than I thought I would.

Application DNA - Applying Relationship Maps

the concept I wanted to share today is something that I've been working on. I'm very much interested in your thoughts on this as well. And it's what I call the Application DNA. And when you look in an application and you think about all the different relationships that are associated with it, the fact that there are multiple layers that must be delivered consistently and accurately in order to have that application actually function. In order for you to execute change, you have to understand this and let's just start with the reporting standpoint. If you're reporting on this, man that is a bear. I want to understand all of the virtual machines, actually you can't even say that, right. I want to understand all the virtual disks that are associated with the data store that's associated with the LUN that's associated with this file or this storage platform or this, right, because in today's world, as everything moves to policy-based, it could be anywhere. So the concept that I've worked on and I won't pretend like I've solved it, right, it's kind of why I wanted to have lots of time here so that we can talk about it, is when you're trying to get that information and trying to do those things, when I was at Cisco, we had a need for that kind of report. We wanted to understand all the applications that were sitting on VMAC storage because it's expensive and we wanted to make sure that nobody was on there that wasn't supposed to be on there. Are you kidding me? Right, like that's a significant undertaking, right, because not only did I need to identify what virtual machines were there, I needed to understand what the applications were and I needed to understand who the application belonged to, right, and I would run this report and it would take about 22 hours. I was using Get-View, I was doing filtering, it just took time because you're still stuck doing a lot of this, right. So what I ended up having to do, and this was one of the big challenges of scale, is you end up having to break it out into multiple systems, so each Virtual Center Server then had its own PowerShell Server, and that PowerShell server was specifically dedicated for executing things against that Virtual Center Server. I created some trusts in there, but now I have to manage a dozen PowerShell Servers. And of course, we did the one ring to rule them all. We have one PowerShell that would execute against all the other PowerShell Servers. That sucked, but I was able to get it done in 6 hours instead of 22 hours. And what dawned on me because like here's, yeah, what dawned on me is that there has to be a better way of doing this because not only did I get into that, but when there was an opportunity that I needed to execute something or change something, maybe change a configuration, maybe I needed to move all those virtual machines that weren't supposed to be there. I have to deal with multiple layers of that infrastructure, and then guess what I end up having to do, I have to do the same queries over again. That's just dumb, right, because it takes up time, it takes up memory and that's time that the business would like back, especially when you start thinking about DevOps organizations and you're trying to implement things in a fast reliable manner and you're trying to shave seconds sometimes in your value string. Why? Because the faster that you can get through test and release, the quicker you get value for your efforts. Okay? So the concept with the DNA is that what if in the background, we kind of collect these relationship things. We find some efficiencies in how we collect, but in the background we collect this information and store it and I've played with multiple types. They're all ugly, so I'm very much interested in feedback on how to accomplish this, but I've kind of settled on JSON because JSON is just really fast and you can collect the information into an object, pipe it out to a JSON file and it's very easy to import it and it be object-based. And so the way this would work is you would go out and you'd collect that information just like I had said before, but you do it for everything. Like this is just a single one, but you would do it for everything and you would get these mappings. You pipe it to a JSON file, and when you start your session, you put in your profile to load the contents of that file and you have this collection happen several times a day, but here's the thing, most the time it's stuff doesn't change. The relationship maps do not frequently change. The only time they really change is when you have to perform maintenance event, right. And so, being able to collect this information and store it into a file and then absorb it into an object, you can then reference that object instead of doing that cross-platform query. You get away from doing the Get-AllVolumes and find the one that just does the SCSI with the SCSI Id. What you do instead is you just find the SCSI Id that's in that JSON or in that object that you imported and you have all the information for that entire application stack that's associated with that. Does that make sense? Am I off my rocker? If I am, please tell me. Right, because this has felt a bit like a leap and it's a little challenging,

Why App DNA?

but what ends up happening is like why would you do this? Well you're going to get quicker reporting. You already have those relationship maps in place so that six hours can get down to more like 30 minutes. Getting that information faster, being able to have quicker time to resolution. So if you have an issue, you already have that relationship map. And I know there's like some change management databases out there that have similar types of information. There's products like Zenoss that do a similar type of thing from an operation standpoint, but that's not necessarily helpful in a PowerShell script standpoint when you're trying to automate. That's helpful from an operation standpoint and be able to get visibility, but. PowerShell's free. And PowerShell's free. That's a really excellent point. Like the cost and the barrier to entry of PowerShell is phenomenal. It's the best value in IT. And then from here, you also can do faster implementation. So let's say that you do need to make that type of change. You're able to go in and execute and find the information and find those relationships and execute those cmdlets regardless of where you are in the stack without having to do the query and the work that I'm doing, and again it's still early, if it's changed and it's not accurate, it'll throw an error. Just handle the error. Then you do your query. If in the catch, there is an error, do the query and see if it works that time and update. And then you can just update the object, right? Improving the value stream. From a DevOps perspective, being able to reduce time because I'm not kidding myself, right. In some respects, this is only going to shave a little bit of time on things that we do on a day-to-day basis, but when you start looking at things that you're doing frequently like provisioning out environments and destroying them and moving your software development lifecycle, seconds make a big difference, right. So yesterday during the talk about dynamic parameter sets, I started thinking and I brainstormed with several of you about the ramifications of things that you could do like when you're going to deploy knowing that there's certain Datastores that would be preferred, certain capabilities, if you know it needs to be on a VLAN and you want to keep it isolated to a specific server, since you already have the data collected, you could prepopulate in your functions the, what do they call it again, it started with C, help me out here folks. The dynamic parameters. Anyway. So it will prepopulate options so that when you go to choose, you can see the ones that need up, so you can have all the networks listed with the VLAN associated or have the storage with the percent free so that you're identifying something that has more resources available, making it easier for the user to be able to do this automation without having to do all these queries and lookups and understand everything that's happening in the background. And then, when you look at the extensibility into other tools like vRealize Orchestrator, for instance, or PowerActions, like I mentioned, or Microsoft Orchestrator, whatever, System Center, Service Now, anything that can execute a script being able to speed that implementation has tremendous value because there's a truism I've learned and you've probably seen this in your environments as well. When you're delivering IT services to people, they will fight against your processes if they feel like they can get it faster another way. They will spend budget stupidly often to get something faster. And so, what we found was if you wanted people to fall in line with your process and things like that, you make it super-fast and easy for the preferred way and painfully difficult the other way. Take cost completely out it. And so when you start demonstrating that not only are you delivering a service, but you can deliver it faster and more succinctly efficiently, right, very consistently, you'll find that your adoption to those efforts will go way up. Same goes for when you're sharing scripts. So when you look at implementing something with PowerActions and vRealize Orchestrator and sharing code with others, I think most of us would tend to deliver a function. Because I know in the environments I've worked in that I would create a script and I would tell them, and I think we've all done this, I think we all still do from time to time, but at the very top you have some documentation there and you then you say fill out these four fields that are associated with your environment for running this script. And then go and dot source it, run it, and things like that. Well as you look to increase adoption, you turn those things into functions and not only do you make it to where they are filling out information, you could then extend the parameter set or the dynamic parameters, you can extend that, pulling from that data, and you're now giving them an opportunity to do it faster with less effort, and more importantly, they don't have to call you asking you how to use it.

App DNA - Growth Areas

So when I think about the growth areas in this, I especially look at DSC and PowerShell classes. Being able to create resources and implement DSC and be able to leverage those relationship maps in order to ensure consistency in the environment. Or the other way around. DSC is setting the configuration and you just pulled the information from the DSC MOF files. Because when you still have to go out and do things and identify what's happening and where it's going, DSC isn't going to do everything at this point. We're going to get to a place where DSC is capable of managing multiple layers of the stack, but I was only able to find Cisco UCS that had a DSC resource outside of what Microsoft was delivering. Are there any others that you know of that Microsoft has not delivered? Yeah, so that's kind of what I thought too. And I also think from a growth standpoint, as we continue to see people move away from snap-ins and go more towards modules, it gives you an opportunity, as I mentioned earlier, to kind of extend this out a great deal, right, to be able to do more in the environment just directly from whatever system that you want. You don't have this cumbersome installation process and managing all of those. You just simply make sure that the module is loaded.

Q&A

Alright, so this is kind of near the end of the talk, but I'm going to log into a lab here in just a minute assuming the internet connection and we can kind of walk through some of these different integrations, kind of talk about this a little bit if you want to. If you don't, then we don't have to. It's totally up to you. But any questions thus far? Because you've been way too quiet. (Audience question) Yeah, so I'm still trying to sort out. I wish I had a whiteboard because whiteboards and me get along really, really well. So the question was how do you do updates? What's the plan for updating the JSON or updating the information because there is this opportunity for the information to become stale if something changes in the environment and so what I'm working on right now primarily is identifying the situations where a change would occur. In a typical environment, what type of things would make a relationship map change? And some of those would be like a VMotion, like a virtual machine moves over, you add additional resources that are tied to something else, you add disk, you add new network, adding things like that. Removing things obviously would make a change. So from a data integrity standpoint, it's really what we're coming to, right. Is this data going to be stale? Is it still going to be useful? I've thus far found that the number of changes are relatively small and I think if we look at what information we're collecting because like let's go back to when you think about what information that you need to collect all the way up the stack, I'm not sure how much change we really would expect and it's one of those I'm kind of unsure about how I want to deal with that because I've also looked at maybe using SQL Server and just create our relational database of all this stuff because I treat, at present, I'm treating network, storage, compute separately and pulling and collecting that information separately because they are relatively tied to each other, and identifying where you have things that cross across those three stacks. Because at some point, you just kind of have to draw the line of how you're going to be able to collect the information and when I'm doing this, I'm really focused on these, right, those points, so I'm building functions that collect those points. And so the intent is that once I build these functions and have all those relationships mapped out, you could then just pick the ones that matter to you, that are interesting, put it in there, and run those individually. And so, they can run on their own in the background at whatever frequency you want and update that data as you go. So this is why I really wanted to leave time to talk about this because I don't pretend that I have all the answers and I certainly don't pretend that I know how to solve this problem, but I do know that especially in environments where there's scale, this is a problem and it's a problem a lot of us are going to face as we start getting more and more layers of PowerShell. Right now, in my poll, they said 33% of the people polled of I think there was 40 votes, were managing 7 plus layers or items in their stack. I'm sure somebody in here is doing that, right. Anybody would classify themselves as seven plus layers? One, one, going twice, two. Okay. Let's go back to the. Alright, so when we look at this, alright so how many would say one to two? Three to four? Five to six? And then we already established a couple people doing seven or more. So what's going to happen to everybody that's only doing a couple? Here's what's going to happen over the next few years. One, you're going to refresh cycles on things in your infrastructure. You're going to buy new things. And as PowerShellers and people who have influenced their organization to automate and use PowerShell and get value, you're going to have influence in that buying decision. You should. You're going to look for things that have PowerShell integration as a capability. It's going to become valuable for you. And in doing so, you're going to then say alright, I've got a new tool, I want to go in, I'm going to PowerShell this bad boy, and then you're going to run into all these things about figuring out like how does this connect to that? How do I get this to happen faster? And that's one of the things I found challenging because I put this poll out because I'm just like is this a problem that a lot of people are facing yet and I feel like there should be, but I don't hear a whole lot about it. So I'm either way off the reservation with this, but I think what we're going to see is that we're going to see this problem creep up more and more because that increase in extensibility is going to expose all of you to more opportunities to automate because the other side effect of greater extensibility is that consumption model change that I talked about earlier. Alright, the way that we're consuming the resources that we buy today is very different than the way we did it 5 years ago and it's crazy different than what we were doing 10 years ago and I suspect it's going to be crazy different 5 years from now. And I get the question frequently they're like well Josh you talk about PowerShell and everything and all this extensibility, isn't there just going to be other tools that's going to do all this work and we just won't script anymore? Does anybody in this room believe that in five years we won't need to script anymore? Alright. You got one. It's good to have one. There's always one. I'm usually the one. So we're not going to see this major drop off of this need to script and execute in the environment. You're still going to have to do tasks that even if you have this very robust management plan that can just execute all these things, there's just some things it can't do. Because if it did do all of that, nobody would want to use the tool. It would be too cumbersome, too complex. You'd look at it and you're like there's just too much here and we see that with vSphere and System Center sometimes, right. There's things that you wish were there and there's things that you just wish would go away because you don't ever need them or use them. So you can't, nobody should ever expect that there's going to be one tool that's going to change all of it. What is going to happen though is that as those tools grow and you start implementing them in your environment, they are going to have hooks where you can execute PowerShell, especially once it goes to Linux, if it ever does. I hope it does. That would be fun. I would love to have PowerShell on my Mac because then I would know how to use my Mac. And so, as you are looking at your environments and the things you're doing, you're going to start seeing that you have more accessibility, more capability, you know those silos. I wouldn't say they're disappearing, but the focus is much more on how can we help one another. How can we leverage PowerShell to do something on this side of the stack and I think we're going to see that those relationship mappings are going to be critically important. So here in about the next two weeks, I will have on my GitHub repo, I'm going to have the App DNA repo where I'm going to start putting these functions and I would really love and appreciate anybody that wanted to come in and contribute to that and engage in this conversation some more because I think it's, unless SAPIEN makes a tool for this or somebody, I think it's just something that we're going to continue to see pop up on a regular basis. You said you had the time down from 22 hours. So I was just wondering, do you use…? I didn't at the time. It wasn't an option. Yeah, that was four years ago. Yeah, so and that's the other thing, right, some of this, we're getting a lot new capabilities and tools to help solve some of these challenges and improve the way that we're executing. I haven't used them because I don't have a big environment anymore. So has anybody leverage, are you doing that? (Audience comment) Right, and so what he was saying is leveraging workflows to go out and collect information from different places into a single location to execute creating a virtual machine and getting that time down. So yeah, absolutely something you can do. You have a question? (Audience question) So to repeat what he said, like in their environment on the Linux side, complete automation up and down, on the Windows side, nothing. So why do you think that is? I think it's mostly, started with the silos. So silos. (Audience comment) Right, exactly. Well I guess, so what he was saying is just educating people and making it a priority to do the automation. I mean, think that's fundamental. I mean this is it right? Let's just show everybody this. Let's just posh it real good. You've probably seen the image that's got the two people that are pushing a cart that's got square wheels, right, and there's two people right behind them that's got round wheels and it said the guy's pushing like no we don't have time for that right now. Just show them that and be like and so when you think about that and this is how do you make that change, like how do you elevate that conversation, get them started talking about it. I primarily just say quick wins. Little bits here and there, right, and that's like when I look at that DNA stuff, I really want it to be modular and extensible enough that you just grab the little bits that you need. You don't have to use every bit of it. You can grab little bits to do small parts. Maybe get that build time from 12 minutes to 10 minutes or down to 8 minutes. Every bit that you do helps and just kind of feeds because now you've bought yourself more time and now you you can start looking at more opportunities. Do you have a question? I kind of have like a comment actually. I'm a contractor. I work for a client on a big environment orchestration project and one of the reasons why I came to this was because I'm one of the seven plus and what I find is like whenever we provision a server, like you said, there's a whole lot of layers like when we say we want a new SQL Server, we send a call, we do things with SolarWinds or we put stuff in our automation database, we provision the VM, we configure all these silo on an OS level, but and there's probably like five or six different actual appliances that we put, insert new records into, or read records from, that sort of thing and what the good thing about the whole DevOps mentality is all that is essentially just an API. So we make an API call to SQL data that quote/unquote an API call to a SQL database, an API call to the IPM service, or an API call to after all that stuff and PowerShell allowed us to it's essentially like the glue that glued all of that stuff together that allowed us to just, you don't have to know what all these appliances do. We just have to make a quick API call, bam, bam, bam and PowerShell orchestrates the whole thing. And I highly recommend that and the funny thing is that when I think of what this concept like it's doing exactly that, but trying to store in a stateful manner where you're not constantly making the queries. I haven't been able to test it in a large enough environment to be able to say how much impact it's going to have. So I'm hoping later this year people will be able tell me. Go ahead. So you're talking about how the six-hour process to refresh that state data, and then you're considering well how often do I need to refresh it to make sure that it doesn't go stale even though those relationships don't change. My suggestion would be why not run that six-hour process every night and then you can have the scripts that you need now if something changes. If an unexpected change happens, wouldn't you want to know about it? Yeah. I think that makes sense. My intent would be a scheduled task that would go at some regular interval and then also adding little bits of code to do a query if that doesn't match up. because depending on what you're doing, it doesn't have to be and here's the thing, when you look at how applications are being built and developed and put out into the world, they're much more modular. The monolithic app is becoming less. They still exist. They're still going to exist for a long time, but it's much more going into little bits and chunks that do their little bit, right. What's that? Micro Services. Micro Services, exactly. And I think by enabling it with multiple functions that do very specific things and storing it into a central data set, you could probably keep it fairly real time, right. Or you just don't do those processes and only check when periodic. Yeah, like there's and I think that there is no right answer. There is no this is the way you have to do it because I think everybody's environment is going to be completely different because when you talk about the different use cases here, like he's talking about implementation, right, like that's the Day 0 activity. That is creating those relations like leveraging relationships and creating a new DNA, like a new app, like it's coming into the world with nothing just like a child. You have to give it everything. And so that is going to be one operation, but when you look at how do you turn and lifecycle it, how much does it change? Like I wasn't born with a beard. I got a beard right. Sometimes the beard my go away, not frequently because it gets itchy when it's not there. It just feels awkward. But when you start thinking about Day 0 versus Day 720 or 730, completely different needs, but the same information. So the refresh on that, not sure. We actually use a system just like this on one of our large clients and we use a SQL database to store and what wound up doing we developed kind of transactional model into that database, so now we can have training reports, so I can show you what's our trend on the end fields? Are we starting to fill up on workstations and how is that looking at the time? So it's, I think there's a lot of potential behind this and having a community-driven solution for this is a huge idea. Awesome. Well I'm looking forward to you contributing based on what you guys are doing. Absolutely. Alright, you haven't thrown things at me, so I feel confident that I wasn't absolutely crazy here. I appreciate that. Anything else? Alright, let me see if I can get into the environment now.

Provisioning Storage Function Demo

There we go. Alright. So this function is designed to go through and basically it will when I work with customers, a lot of them have used NFS or Fibre Channel and they're looking at size SCSI and they're like ugh, I've got to provision all of this storage into my environment. I'm like ah, it's a piece of cake. Here's a PowerShell function. That'll do it. But what I did here is I didn't want to just create a bunch of volumes and then tell the VMware administrator that they existed. So I wanted to do some things that were dynamic and I think it actually ran, so but so what we're doing is we go in and when you run the function, you identify that this is the cluster and give it a cluster name, how many volumes you want, the performance characteristics of it, is this for an existing tenant like an existing account, is this a net new account, and then how do you want those organized on the storage system. And then provisioning it all to the VStore cluster. And so, let me make sure I'm logged into VStore environment. I'm not used to everybody watching me type. Alright, and so what it does is when we go to set up, right, I'm collecting information as we go. I'm collecting cluster information, I'm collecting host information, I'm creating targets, I collect the IQNs from the storage system and I present them over to the VMware environment, alright, so there we go. I need to add the IQNs from the ESXi host to the SolidFire storage system so that it goes and trusts that it can connect to that and I'm leveraging multiple levels. So now I'm working on also doing some stuff with Cisco UCS where you can kind of configure like if they want a specific VLAN to connect to all the stores, things like that, and then go through and rescans and it connects. So here you're adding the Datastores, you do a scan, identify the Datastores exist. We're going to pull the volume name from the storage system, so we can make the Datastore match the volume name that is on the storage system so that it's easier for you to talk with your storage administrator about which system that you might need to change. And then it just adds the Datastore. You create a new Datastore. So you're dealing with multiple layers of the stack there and the bulk of the work in this is really identifying those relationships. Like when something breaks, it's because I didn't map something correctly, which is kind of what got me down this path. Let's see. Alright, and so now yeah, it's all disconnected. That's sad.

PowerActions Demo

It's not a good example when it's not working, is it. Alright, well let me show you the PowerAction stuff as well while we're talking about that. So this is PowerActions as I was describing earlier, so I'll go to Home screen real quick. Once you install it, it allows you to create these scripts that can be accessed directly through the VStore WebClient, and so I've got several of them that I see on a regular basis. Now I'm not a big fan of storage, but I work in storage company, so I talked to a lot of people about storage. So a few things that we run into are like changing queue depth and changing naming, changing performance characteristics, and in order to do that successfully without reaching across silos, if you will, going into a storage team, you have to make that relationship and to do that query. And currently, it works fine. It just takes longer to actually get all these. And since my other script didn't run, I can't actually show them in action, but I can show you how it works. Here's a PowerCLI Console that I mentioned. It's, well I'm not connected, but it's PowerShell like it just runs. Alright, let's go to this and so when you right-click, say Execute script, and it will tell you what's there. You select which one that you want to run and it will prompt you. I don't think I can run any of those because the storage is disconnected, but yeah, and what it allows you to do then is to be able to identify or execute things or report back and identify how this relates to other things like you could have a report that says give me all the snapshot information, give me all the VLANs that are being used that are associated with this host on the Cisco UCS or something like that, make sure that those two match up. Alright, so we've got some customers that are working on doing that. That make sense? I wish I could run it, but I can do the local Datastore. I guess my PowerShell Server is broken. I'm sorry. That's what I get for updating it to version 5 and not testing before the conference. Sorry about that.

Final Thoughts and Q&A

Any other questions? Thoughts? Funny stories? So Josh. I noticed on your third slide you had a whole VMware stack and a very popular vRealize orchestrator. Yeah. I'm curious. How do you use an orchestrator because we developed and raised PowerShell scripts that makes all these decisions for us and I feel silly for bringing an orchestrator because I've just taken all that logic out and I build it and I run _____ as something stupid like that. Yeah. So how do you use it? Or do you use it and use the dev? Yeah, so I use Orchestrator primarily if there's any decisions that need to make, if there's going to be any branch like if anything will change as a result of a condition or I need to do things like it will change due to a condition. It says that if this condition exists, go this way, if it doesn't, go that way. I tend to use Orchestrator for that. But it also really comes down to with Orchestrator what you have plug-ins for and what you have actions and workflows for and that's where you hit a point and you're like well time to go to PowerShell and then you can call from that. So I've tended towards Orchestrator primarily, and again, anyway I'll talk about the hammer like when you're hammering everything is a nail like I always start with PowerShell. I force myself to use Orchestrator and other tools. The biggest advantage and I didn't show it here, but right here is the same idea, yeah, so that's vRealize Orchestrator is also object aware in the VStore WebClient and it's supported like fully supported. So you can create the workflows right in that, so a lot of the stuff that I'm talking about, you can do with Orchestrator as well. This is PowerShell conference. But with PowerActions, I like it because it also allows me to do more things because Orchestrator doesn't cover everything. And that's the part that I'm struggling with is that I want to expose something simple to people. Yeah. I don't want it to let them... Yeah, and for me everything is if I can just give you an opportunity to run it and you not care how it's built on the back-end. I tend towards Orchestrator primarily because it's fully supported from VMware and it's not handing someone a PowerCLI script, but if you look at my GitHub repos, it's all PowerShell stuff, right. I haven't released the Orchestrator stuff yet. But we have some customers using it, I just don't, I help them, but I don't publish it. (Audience comment) Yep, so the primary requirements you have to have a PowerShell Server and when you do the install, you have to make some modifications to the vSphere Virtual Center Server appliance and there's some files that you have to make a slight modification to and there's that modification is just so it will show up in the UI. (Audience comment) I think it's 5.5 is the one with the newer VStore WebClient. So I've run it in 6, I had it running in 5.5. I don't think it's supported in 5.1 because I just don't think that the WebClient functions with it. And then from the PowerShell side, Server 2012, actually I can pull it up. So it's labs.vmware.com/flings and their site was down the other day for maintenance, so I'm hoping it's back up. It is. Good. (Audience comment) What's that? There we go. So look for PowerActions and it goes right in system requirements, 5.1 is included, so that 1 is included. Alright. Server 2003 or higher, .NET framework 4, and 4.5 and then all the PowerShell versions. And of course, whatever version that you have. And here's the thing. You have to have all the modules for the different layers in infrastructure onto that PowerShell server, otherwise, it can't run. (Audience comment) No. It'll be separate unless you were running the Windows version. If you're running the Windows version of the Virtual Center Server, it can all be native. I think VMware has made it sufficiently painful to run the Windows version. I think a lot of people have, they've improved the appliance sufficiently that a lot of people have moved over to that, at least that I'm seeing. Alright, well I guess I'll just close it out. If anybody wants to talk with me after, you'll get a little bit of time back. I hope that this kind of gave you some insight into how PowerShell's being leveraged across the stack. I also hope that when you start thinking about some of the challenges of taking advantage of those, this kind of gets you in that frame of mind. Like I said at the beginning, I don't pretend to have all the answers. I just simply know some of the questions that you need to be asking and wanted to feed some thoughts into how you might tackle those things. And so, once I get that repo out, I'll make sure that I hit the PowerShell summit hashtag when I deploy that out and I'll let everybody know and I'll look forward to your contributions. Thank you for your time and your feedback and your participation. I appreciate it. And despite what my wife says, I don't really feel the need to talk for long periods of time without people talking back, so thank you for that and I appreciate it.

Nano Server and Remote Management

Introduction: Customer Feedback and Our Server Journey

Alright, good morning. My name is Neema, and I'm a program manager on the PowerShell team. I work on PowerShell and DSC specifically and also Nano Server specifically on bringing all the cmdlets we know and love from full PowerShell into the CoreCLR and Nano Server. So today I'll be talking a little bit about Nano Server en masse, how we got here, where we're at, and what the path is forward, and then a little bit about remote management, how PowerShell integrates with Nano Server, and what we hope that you guys would do between now and the next time we see you with Nano Server and kind of spreading the word on manageability in Nano Server. So I'll talk a little bit about what we're hearing from customers, the journey we've gone through, like I mentioned, we'll focus on three areas within Nano Server manageability, looking at PowerShell Core briefly, and RSMT, the Remote Server Management Tools, take a look at some graphs and results we've seen, benefits of Nano Server, and then focus on Nano Server in 2016 with containers, and then next steps from there. And just a quick show of hands, how many people have had a chance to play around with Nano Server so far? Great. And how many people actively use Nano Server day to day? Okay. And anyone try to deploy a production workload to see if it works or not in Nano? Alright, good. So this is the perfect audience for this talk. Hopefully I can convert some of you to at least giving it a shot. So three big drivers from our customers that have kind of led us to thinking this is why we need to define Nano Server and it goes along the same tenants that brought us from Server Core to where we are now. First of all, reboots impact business. We know that the people we work with often come back and say to us, rebooting is a costly effort whether we're managing the systems, we're doing it for a client, or we're just experimenting, uptime is really critical. So if you're patching a component you never use, it's kind of a waste of a reboot cycle, and when the reboot happens, you want to get back into work and get back into work and get back to your workload ASAP, not fiddle around with having post restart configuration. Another big issue is server images are too big. Windows Server 2k12 R2 started to bloat up a little bit towards the end there and we saw that with the Server Core option people had a slightly more lightweight option. With Nano, the goal is to have the minimal, the minimal image, just the components you want so that when you're transferring the images or deploying them, you can do it as quickly as possible. When you're storing images, you can have per roll images customized to exactly what you want and nothing more. And third, the infrastructure requiring too many resources. So this is where we're talking about an OS taking up all these resources, presenting processes that are running in the background and really not giving you anything. A good example of this when we've had feedback a lot about an early Windows Server 2016 TP Build are things like the map service or the XBOX service. I don't want that stuff running. And while we can address some of that stuff, Nano Server really is the solution to having just what you want running. Also, by having a lower footprint, you can have higher VM density. So where on a piece of metal, you can maybe run 2 or 3 server VMs, now you're talking about 10, 20 Nano Server VMs. So this brings us to kind of our first tenant here, which is I want just the components I need and nothing more. And to show you that, we'll kind of step through a quick journey of how we got to where we are. So historically, Windows Server had this big footprint and you'd add on roles and features leading up to the server 2K3 timeframe. With Server 2008 and 2008 R2, we took a little bit of a different methodology where we provided Server Core as a GUI list option and full server that gave you all of the compatibility with previous server infrastructure. With 2012 R2, we've kind of split up the GUI part and provided a minimal server interface, and then on top of that, you load in the GUI Shell. And as we move towards 2016 TP5, some of you that have experimented with TP4 may notice that we call this a desktop experience and just server now. The default option for server is now going to be this desktop experience where a lot more of the Windows 10 client shell that you see will be looped back into the server product. So the Cloud journey now, these two big parts here, first of all, Azure, we want to minimize reboots here because for Azure tenants, having these things constantly reboot is just interrupting services. They have specific workloads that usually vesting towards some Azure resource group, which can be really costly, even more so than on the on-prem solution that we previously talked about and also the provisioning of a large host images has this big impact on network resources where a lot of network architects have to sit around saying how many VMs are going to get deployed on this architecture every day, every week because I need to allocate my network resources such that it can handle that throughput. And a lot of times, that's the most bandwidth intensive action is deploying and spinning up VM resources. The more recent step we've taken is with the Cloud Platform System and so Azure stack, Azure pack, the whole point here is that we are bringing in a cloud in a box running everything you need to get off the ground, but still there is some setup time involved, there's still some patches and reboots that cause interruptions to the entire thing, and like you see here, I mean, the reboot times are still, they're acceptable, they're a lot better than they have been historically, but we can get them better and that's really the primary driver behind Nano is that we can do better. We have to let go of some of the past to be able to do that.

Nano Server: Roles and Features, PowerShell Core

So the next statement, we need server configuration optimized for the cloud. How do we do that? So Nano headless 64-bit only and as a deployment option of Windows Server. We've refactored it from the ground up, we're focusing on this CloudOS infrastructure, and born-in-the-cloud applications, roles, servers that currently exist that are in use every day, we want to make those be first priority workloads on Nano Server. And so this is what the new picture looks like. So with roles and features, we kind of went along the zero-footprint model and even inside the company when teams are trying to bring components to Nano, they're saying well why don't I just bring everything I got and we tell them to take this opportunity and look at what they have and see whether or not it makes sense to bring it to headless Nano Server world and whether it is adequately manageable using PowerShell's from the other manageability tools we'll talk about. So all of the roles and going into TP5, this will change slightly, will live outside of Nano Server, so we're talking standalone packages that you can pull down and install, sound familiar to another operating system maybe. With that being said, the goal is to be able to pull these packages similar to how you add Windows features to Windows 10 right now on demand, the way you want them, when you want them, and to have the small amount of time between when you say install and when that role is running. So the initial pool of roles is there, so Hyper-V, storage, clustering, DNS and IIS have rolled out recently. In the latest builds you're going to see bigger, more improvements in the next release of Nano Server for those two, and of course, CoreCLR, .NET Core, and we're really focusing on trying to bring driver support to Nano Server. So all the drivers that are needed for networking storage, one by one, we're making sure that they're compatible and that there's a reasonable story to get those injected into your Nano image. Anti-malware, so Windows Defender is an optional package. We're working with the Windows Defender team closely to make sure that signatures are updated via Windows Update, so you'll have a fully-maintained anti-malware option on Nano Server still. It's not the wild west. And then, SC and inside agents are underway and we hope to roll those out alongside the next release. So PowerShell Core. So how many of you, I'm not going to ask how many of you have worked with PowerShell, how many of you have worked with PowerShell Core? Okay. And for those of you that have your hands up, keep them up, keep them up, how many are on IOT? Alright, okay. So a few people. So the whole point of core is to refactor everything to run on CoreCLR. That means that you'll have one common core between IOT and Nano and anything else we decide to do with .NET Core. Full PowerShell language compatibility is there. Remoting is there. So all those things that you expect for command invocation, PSSessions all work. Core engine components are certain to get more green-lit, things like DSC are starting to show up and they're fully-fledged functionality. Support for all cmdlet types. I heard some people in the room chattering about C# and scripts and CIM, so all that stuff works. And the intent here as we work within the company to bring cmdlets over is to make sure that people are bringing them over in the most future proof way possible. So if there's somebody writing underlying code, we're not just picking up cmdlets from full and bringing it over and we recommend when you do that, when you're working on your code too, don't just bring it over blindly, take a look at what benefits Core will give you and whether or not your functionality makes sense. And so, limited set of cmdlets initially, but we're adding cmdlets every day internally and those are reflected in technical preview builds that you guys see. And as I'm sure some of you have seen on the client with Windows 10 insider program, if you're on the fast string, you're getting updates to PowerShell whenever a new build drops, so we're hoping to have that same mechanism with .NET.

Remote Server Management Tools

So RSMT is one of the management options that I mentioned. So how many people have taken a look at RSMT? Show of hands. Alright. How many people know what RSMT is, have heard of it? Okay, so many more people. Great. So RSMT is a web-based portal to manage and it kind of includes replacements for all of those local-only tools that you've either had to already PN or set up a remote session to, so task manager, event viewer, device manager, control panel, even PowerShell, disk management, File Explorer, all that stuff is there, and the team is continuing to greenlight more and more functionality. So with that, I have a little video put together here that I'll walk us through of RSMT. Alright. So this video is publicly available and there's a link to it at the end, but I just kind of wanted to walk you guys through this. So RSMT as you can see in the nicely piece of portal that we've grown accustomed to on Azure takes the server properties and then what you'll see here is a blade that shows you all the details about that server. So you have the typical resource plan and IP info, you have blades for, or you have all the performance information listed, and all the tools. So here we'll step through kind of one by one to show you guys what you can see by RSMT, so OS info, computer id info, so computer name, whether it's part of a workgroup or domain, you can change those settings there as well without having to remote into the box or use any sort of remoting. You can work with users by adding users there, network information is there, adapter info. And then so if you look at each of these individual performance, you're actually getting a lot more functionality than anywhere else we've been able to provide this stuff remotely, so CPU information, network load info, memory use utility, usage, sorry, and disk metrics. Disk metrics are turned off by default for performance, but you can enable those and start getting disk metrics pulled in and there you go, so that's pulling off of the VM there. And then for the tools, these are some of the tools I mentioned. You can look at processes, you can search for processes, create process dumps and processes via this web UI. You can also run new processes, so if you hit that Run Process button, type in the process you want to run, hit OK, and it'll show back up there. You can add some more detail that isn't there by default, by using the call and selector similar to how you would in Task Manager. And similarly with services, you have the ability to start/stop/pause services, you can search for services, a really cool way to edit the registry, so no more having to remote in or start a remote registry session. You can go through here, search for whatever you need to change, whatever you need to add, gives you a nice UI to do that stuff in. Adding roles and features is coming. For now, this just displays what's currently enabled in remote PowerShell. So from a browser on whatever device you choose, you can remote PowerShell into whatever box, in this case, this VM that we've got setup here and everything that you would expect about discovery will also work. And you can see that it's following that same abuse of blade concept, like the team has worked really hard to make this UIP as discoverable as possible. Device Manager functionality works great. You can see devices. This is really good for debugging environments or somewhere where you're trying to troubleshoot what's going on with the system. Same principles that apply to maintaining systems in the cloud, kind of apply in the hybrid environment now. And lastly, event viewer, so you can navigate through the event logs, can manipulate the query. You mentioned File Explorer. Will that be available soon? Yeah, yeah. So for the time being, since we're all PowerShell folks, you can do that stuff from the PowerShell prompt, but there's two or three more tiles that are going to show up in the very near future. Is there anything that they can't do in PowerShell that the basic UI will allow you to, like when you enable disk metrics, can I turn that on via PowerShell? No. That has to be done through the UI because what it's actually doing is reaching out to the agent on that server. Firewall port that this used. Is this the standard PowerShell warning in our end? So it's actually using an agent that you installed that configures that stuff for you and then you have the ability to customize however you want it to connect, what port you want it to connect. Yeah, yeah. You can definitely lock it down. (Audience question) Sorry. Can you repeat questions? Can I repeat questions? Sure, sure, absolutely. Are you going to interact with that agent that installed via PowerShell? The question is are you able to interact with the agent via PowerShell. So no. There isn't much interactivity for the agent. It's a base. Is that planned? Yes, it is planned. Yeah. And when I say something is planned, I will caveat that it's saying it's coming in an upcoming release of Nano Server, alright. Is there an API exposed for this? Not currently, not currently. Yep. Will this be available for Core as well or just Nano? Will this be available for Server Core? It will be available for all flavors of Windows Server. Yep. The solution that you're trying in Azure, so is this interface available for on-prem Nano or just Azure…? So you can actually connect it to whatever you want in your hybrid environment locally, but it does run in Azure. So you'd go into Azure, connect your computer to it, and then you can manipulate, or you could connect to it even if it's a local machine. If it's off the internet right now, there is no way, but there is plans to try and get this gateway service to run locally. How can you connect with a small IPM? With IP address you can just define the same way we define that computer with the network it was in, you can just define over the internet. (Audience question) Sorry? What ports are you using? So I believe the RSMT team uses the default WINRM ports in addition to a couple of other. I can get those details for you after. So is this available only if you have Azure? It is only available with an Azure, yeah, with an Azure account right now. Yep. Will the IOP web base in the interface follow this at some point? Sorry, can you repeat the question? Will the IOT, the I-O-T management interface, will that follow along with this? Or the two submerge? Yeah, the question is whether the IOT management interface will look like this. I can't answer that question because I'm not part of the IOT team, but I can definitely get those details for you if you want to sync up with me after this session. Okay. Yes. Are there plans to manage any down rev operating systems? I can definitely reach out to the RSMT team for you and get you the latest details on that if you want to sync up with me after. Sure. Okay. Any other questions about RSMT? Yeah. I don't know if it's along the same lines, but do you see does this support the server manager going to replace with short one larger? The question is does this replace server manager? This is currently in addition to the management portfolio, right. To answer your question, server manager is still there in the most recent server installs. Yeah. Any other questions? Yeah. Are the clients just like RBAC just for the different toolsets? Sorry, one more time? Is there an RBAC, like the role-based access control for the different rules that will be available? You're talking about as to what you see here, like can you gray out certain functionality for a certain user? Correct. There are plans to put segregated controls over certain functionality into the RSMT portal, but I can't speak to when those will be in. Again, if you want to sync with me after, I can definitely give you more details. Yes. So access control, does it have, can I, for example, have my own servers and my own group, maybe someone outside of my organization access to manage a single server or do you do details on a single server? Yeah. It would depend on. Sorry, the question is if you can have some sort of access where within a group of servers you would have, give someone delegated access to one specific node, and yes, you could do that. It'd be dependent on your infrastructure and how you're visible to the internet, but there's actually one or two examples of that being done and if you want to sync with me after, I can share those with you as well. Yeah. Any other questions about RSMT or anything we've covered so far? Okay. Let's keep going.

Nano Server: Cloud Application Platform and Developer Experience

Okay, so snapping back to where we were here. The point here is to support applications that are born in the cloud. We're not trying to bring the legacy Win32 apps to Nano Server as the first tenant. It's to support a new generation of application and tool. A subset of the Win32 APIs exist, again, CoreCLR, PaaS, .NET and we want to have this kind of available as an OS everywhere, so you can install Nano as a host OS onto bare metal, you can install it as a guest OS on a VM, you can install it within Windows Server containers, Hyper-VM containers, run it in Azure, you can run it nested, you can do whatever you want. With other hypervisors we want it to kind of be a utility more than anything else. And future additions, as far as completion go, DSC will be feature complete in the next release of Nano server, so all the functionality that those of you that has played with it seems to be missing or not fully cooked kind of the similar way that we did with WMF previews, what was in previous TPs was experimental, what'll be in the next TP should be more lock down and every day, we're getting more and more resources that are working with Nano out-of-the-box. So we run into one issue when I try to join the domain. Are we allowed to be calling the Nano Server first and then try the domain join or will it be the same issue like when you need to join in like new create an image. Yeah, the question is right now with Nano Server you have to join the domain before you bring it up and the question is whether you will be able to online domain join. In a future release of Nano Server, you will be able to do online domain joining. Will it be like on the final version of the 2016 or will it even (inaudible)? I can't answer that question. It's going to be in an upcoming release. Okay, so dev experience. I heard some people in the front row talking about writing C# code and I love hearing that. So let's talk a little bit about what the developer experience is like. And before I do that, so I should really ask that who's worked with PowerShell Node? How many people have written C# code in the last 12 to 24 months? That half of the room. Okay. Was there a sign out front? Cool, so here's the point with being able to take what developer knowledge you have and take it to Nano Server. We don't want to redefine the developer experience around a lightweight product. We want to take the best developer experience and bring that part forward to Nano Server. So with that, apps need to remove references to what's there, sorry, what is not in Nano Server and 32-bit references, and developers can use Visual Studio 2015 Update 1 in the Windows SDK to target Nano Server today. This functionality has been around since TP3, but it's significantly better in the past few months. So first of all, the latest version of PowerShell WIN 10 or server 2016 TP, the latest WMF, if you're using a VM, you make sure Hyper-V is installed, connect to that Nano Server machine, In Visual Studio Update 1, you do a custom install, mark those 2 additional options, and then this link, which I will have available a couple of links at the end that will get you towards all this stuff. We'll get you the details, the files that you need to load into Visual Studio, the v6 file, and then you run that, follow the instructions to install it, import the module, and you're good to go, and start creating a Nano project. And I know people previously prior to the TP3 timeframe had started pulling DLLs off of their Nano Server VMs and trying to fuddle with it, but this is really a first class dev experience now and we look to refine it and get it to the point where it is just like developing for any other skew of Windows. Yeah. Does this just pertain to unmanaged code or does it also pertain to anything .NET? So the question is whether it pertains to unmanaged or managed code as well. It pertains to a subset of .NET because not everything is there, but the intent here is that as we go through this dev experience stuff, you would be able to explore a lot easier on your dev box what is and isn't available, as opposed to blindly targeting and finding the APIs in there. Okay, so it's more of like an API check versus allowing you to target .NET Core firmly right? As we work towards a more holistic dev experience, yeah, you'll be able to say specifically target .NET Core. The intent now is to kind of help you in exploring what it is and isn't there, and in addition to that, there's an API port tool that the .NET team runs and that's on GitHub. You can pull it down and say check my whatever you're working with, whatever file it is and see whether or not this has APIs that are not CoreCLR compliant, or .NET Core, or ASP.NET compliant. So project templates, full IntelliSense experience, your little error squiggles, full remote debugging experience. If you download that thing that was linked and run that PowerShell script and all the info is on the Nano Server blog. It's with the last blog post from 2015, if you're looking for it, and it kind of outlines the process. And then also for this half of the room, there is like a Hello World example in C# that you can run, execute on your Nano Server and I think it's like eight or nine steps should take you five minutes. Any questions about dev experience before we move on? Great. So reverse forwarders. Show of hands, who knows what this? Okay. Ten people. Alright. So as we know, a missing DLL will result in an app failing to run, even if everything else that you require is present. We need it to provide a way to run existing payloads without having to recompile for Nano Server. So this doesn't eliminate the need to refactor code. This is merely meant as a stepping stone and having a fully Nano Server compliant product. And so, there's a bunch of stuff that runs with reverse forwarders today. Here's a sample of some of them and some of those versions have incremented as we've gone on and we're letting up more and more that the intent here is to try and have these applications and these tools work directly on Nano without the need for reverse forwards, like I said, it's a stepping stone, but they are there. And so here's the colorful part. So let's talk about some of improvements. I know some of you will have seen this, but let's talk about what Nano Server, what benefits there are graphically. So servicing. These are servicing the full patch cycle from 2014. It has not been updated for 2015 yet, but this is kind of what it looks like between Nano, Server Core, and Full Server. Critical bulletins. You can see that the number there is significantly more for full. And number of reboots. So the point here being that like I said at the start of this, that the intent is to work down to a point where we take all the benefits and all the great functionality that we brought when we went to Server Core with a little cost that people had to incur there to go to Nano Server because the benefits far outweigh the cost as you can see just from a servicing perspective. Security. Drivers, I mean it's every driver is one more that you wish you could run without on Nano Server compared to Server Core, we reduce that footprint by about a quarter. Services running. So this is always a great thing to do when you first spin up your Nano Server VM is take a look at the list of services. It looks a lot better than Server Core. And open ports. For those of you security conscience folks, I know that jumps out at you right away. Going from about 27 to 12, I believe. And deployment. So setup time. What I'm going to show you at the end of this is a great way to get off the ground with Nano for those of you that haven't. Those of you that have spun up your own Nano image and injected all that stuff and spent hours and written great blogs about how people can do it easier are going to feel like I should have just waited. We now have a URL that'll get you a Nano Server VHD. No more of this fuddling around with it, so with that setup time, under 50 seconds. Disk footprint, half a gig! VHD size. So I mean on the way here, I realize I don't have a Nano VM with me. My box is in my car for this afternoon's lightning demo, which I recommend you all come with all of our Nano VMs on it. But this right here took me 45 seconds. I had downloaded the VHD through into Hyper-V and it's what I'll use to show a demo here in a sec. Sorry. (Audience question) Yeah, hang tight. So an installation option like Server Core here for 2016. It's not the installation option. It is something that you pull off of media. It's not listed in setup because there's additional steps that are required. It wouldn't be really logical to step through the Windows setup process when what you really need is to configure an image that you can reuse. So as you can see on the Windows Server TP4 ISO, there's a NanoServer folder. And for drivers, for the leanest image, you just bring in what you want. So we provide a couple of conveniences that I'll again show you in a minute, but it's as simple as just injecting your driver as you vote to configure image. And all of the drivers that you experience, that you expect from Server Core are available through this Nano Server OEM Drivers package. And to run as a VM, define that Guest package. So what I was talking about is features being available as you want them. You'll notice that there is a subdirectory of packages. As we move forward, this will slightly change. It will become much more logical in that they will not be static packages available on media. I'll let you put two and two together. Right now, this is how you add packages. Again, this experience will be greatly improved and for those of you that decide to take that VHD that I'm going to show to you today between now and the time you start playing with that, in the next release coming out, you may not have to fiddle with this too much because you might have everything you need, especially if you're running in Hyper-V. So let's talk about installation agents, tools, other software. So no MSI support in Nano right now. Current builds require that you drop whatever you need on there or have a PowerShell script and there is new installer technology coming. There's been some blog posts about it. Again, upcoming release of Nano Server. So you'll be able to install, uninstall, inventory, do offline and online installation support to kind of integrate it into your environment. And then this is what I was talking about deploying. So up to now, you could go and do a bunch of work. There is a tool now, the NanoServerImageGenerator, you can import that, and then you are able to just kick off building the image, kind of just put the pieces together, run your Convert-WindowsImage and you're good to go or there's a link, aka.ms/nanoeval will take you to a page that will describe the TP4 Nano eval process. There's a ULA, you click the link, and it downloads a VHD, you put that VHD into Hyper-VM, and you're good to go. I'm sure some of you could do it now and before we're done, you'd have Nano running. So accept it, deploys in a few. (Audience comment) Sorry. Yeah, or use your great cellular data connections. But we have this available and if you guys want to sync with me, I actually have it available on my GUSB keys, so I can hook those up and you can just copy it off to speed up the process.

Emergency Management Console

Okay, so some functionality that's been added in the past few releases is the EMC, the Emergency Management Console. So we showed you remoting using RSMT. You know that you can remote in PowerShell, but what happens at the local console, right, what happens at the keyboard and mouse you're on locally, if you're on bare metal, or when you connect on Hyper-V. So we need to provide a way… Yeah. The one thing that it doesn't do in TP4 is allow you to start the DNS service. Is that changing? The one thing that the EMC doesn't do in TP4 with the DNS service? (Audience comment) Okay. I'll take a note of that after it and if you want to sync up with me, we can make sure that your feedback gets heard. I'm not sure of the latest status on that. So EMC, like I was saying, is the local way to get access to your configuration settings and now most recently, network configuration and firewall settings. And with that, we can jump into a demo. A few minutes left, so here we go. Connect to this guy. So that's what it looks like. So I'm just going to log in here and that's what you got with EMC. So from the top, you can see you've got your computer name, your workgroup that you're a part of, operating system, and then basic time/date info, and then network addresses. I'm not connected to Hyper-V switch right now, so this one isn't network accessible and you can see your two options down there. So this has actually changed since TP3 when you guys last saw it for those of you that did. So just tab in and then you have these two options, networking and firewall, and you'll also notice along the bottom, the Ctrl+F6 and the Ctrl+F12 for restart and shutdown. So this will list all of the network adapters and this shows you the info you need to make sure that you're up and connected. Link status. Whether you're connected or not, interface, and the driver info, so driver name, date, and version. And then notice along the bottom there, F4 to toggle the network adapter, you can disable it and enable it like that. If you do F10, you can get the routes table to configure ARP routes, add a route with F10. And then, IPv4 settings, so if you want to that, you can specify all that stuff there. And then, IPv6 settings. So it's all accessible to make sure that you can get your network interface up. And then the next piece of the puzzle, firewall. So these are all the firewall rules that ship in the box. For my debugging purposes, I add a global WINRM 5985 rule. That's really easy to do. From here, you can enable it and disable it. And notice that it's state toggling between enable, no, and yes. Is there a plan to allow you to change like the local address from the EMC, for example, like you could toggle them on and off right with the firewall rules, but you can't actually alter firewall rules. There are discussions within a server team of how to best achieve that. There are ways you can do it now using PowerShell, right, if you remote in, but from the EMC currently, you can't add in TP4. Okay, and then I think that's it. Yeah, and then you can just log out with Escape. And this works great on domain connected environments as well. Within the company, I'm able to use my domain credentials and hop into a fresh Nano server.

Containers, Road Map, and Next Steps

So before we start that. Containers. Hands? Who's heard of containers? Fantastic. Who uses containers? I love it. No, I mean that's the whole point, right? We all know of this stuff, but until there's a reason or a benefit, we're not going to go and explore it, so hopefully this is a good first introduction to this stuff for people that want to go explore. Look, we want to kind of lessen this tension. It's a two-way thing. Developers want to write their apps, IT needs to maintain it, the devs don't want to have the IT folks reasonable understand and be equipped to do their jobs, and the IT folks end up thinking that the devs are just running their stuff on their machines, but not thinking about the bigger picture of deploying it to the actual production infrastructure. IT has all these policies you have to follow, all these procedures that seem archaic and bureaucratic to the developer, but they're required to run a production service, I mean that a developer often is not required to do and I have a feeling that a lot of the folks in this room are in the middle of that arrow, so you guys kind of make the bridge between devs and IT. So the focus here is how do make it so that developers can innovate and not be just kind of overloaded with all of these processes and procedures and have it so that they can do what they need to do and IT can still have some reasonable administration over the system. And the answer to that is containers. So with containers, we have this completely new approach to building and shipping applications. We traditionally would put out applications, deploy them into physical systems that we've built up with some hydration script, or with a complex written manual, or a bunch of PowerShell scripts, or worse, a bunch of batch scripts, but the whole point being here is that applications often require a lot of physical prep to be able to be deployed. With virtual, we're a lot more destructive and we can pull stuff down if it doesn't work. I don't know about you guys, but my dev box ends up having 100, 150, 200 VMs on it, not all of them running, 5% of them running, but it's not something I go and clean up often. When I run out of disk space, I go delete the last 20% and the issue here is that while it's still at a fast deployment, a lot of those same processes that apply to IT in the bare metal space is starting to apply in the virtual space, so you're having now, we thought we had tricked them, right, we thought we had gotten around it, but really what's happened is that now there's all sorts of policies and rules within organizations for how you can use VMs. So with containers, you kind of get the best of both worlds and a couple of additional benefits. You can push your app development forward, kind of get rid of a lot of the excess work in getting the infrastructure up and mostly just consolidate a bunch of roles into a specific, into one server where you're previously running an individual service for each. Here, just stepping through a couple of the principles here, every application has its own dependencies. The virtualization happens where the container engine, this really lightweight mechanism, isolates all this stuff and packages them into these virtual containers. You have this shared Host OS that kind of lays on the bottom. And then the flexibility that this gives you in the underlying OS is pretty great because the containers are abstracting, the containers support within the OS is abstracting to those specific containers, what is the host underneath it that the server was underneath it. And it's fast, right, containers can be changed rapidly, you can scale them up, scale them down as you please. A lot of the benefits that we got going from physical to VMs happen again with VMs to containers. So this is what it looks like right now. Container operating system environments. You've got Nano, highly optimized, born-in-the-cloud applications. If it's something that has been developed, created in the past couple of years, Nano is usually the way to go. You have Server Core, highly compatible, right, all the functionality that you expect is there. I still see Server Core as that, what was that logical stepping stone to Nano Server and I think we're all into Nano Server. I think you guys, if you're going to start experimenting, if you're still running full server workloads, jump on board, come join the Nano Server train. Nano Server is the future of Windows Server. It's a target for all internal iCloud components and new applications. It's a new foundation for everything. It provides that Just Enough OS model to really mitigate the scope of what could happen to a server and the possibilities, want that thing to be a role, an application, and really be easy to maintain. If you haven't gotten this point yet, not everything will run on Nano Server. There's going to be some work. Server Core will provide compatibility with a pretty long lead time for existing enterprise applications to start coming aboard Server Core, sorry Nano Server. And I mean yeah, you can see the picture there, right, let's move that way. So next steps. I'd ask all of you to kind of embrace remote management of Server Core if you haven't already. Understand that the GUID server is a thing of the past, my friends. Let's move on. Server Core gives you a great way to do that today. If you're running workloads in 2012 R2, a great place to start, so it's an easiest transition. And make sure that things like tools and agents that only run locally, we've got to explore options that are remote friendly. If you run into things that are big blockers for you, your partners, reach out to nanoserver@microsoft.com and then get playing with it. So deploy it, the guide is there, feedback is on UserVoice, and that VHD link again aka.ms/nanoeval. And lastly, RSMT feedback, you can go onto the Windows Server UserVoice. If you go there, there's a link for it. That's it! Thank you so much!

PowerShell Team: Present and Future with Kenneth Hansen

Introduction

Good afternoon. You know, normally, what you have seen in the past is that Jeffrey Snover tends to be here to talk a little bit about the future, but this time Jeffrey's on a trip. He's in Hawaii somewhere taking a vacation. Can you believe it? But anyway, so Kenneth and I, we're going to spend a little time today talk about the past, the present, and the future of PowerShell. For all of you guys who don't know me, my name is Angel Calvo. I'm the partner group software engineering manager for Enterprise Cloud Management. My team is the team responsible for the management technologies for Windows, as well as PowerShell, Desired State Configuration. Ken Hansen and I guess I've been on the PowerShell team for 13 years, a very long time. Turn the mic on. Who didn't turn that on? Is that working? Yes. I'm Ken Hansen and I don't know how to work these mics, but I do PowerShell for about 13 years, so that's---I need a command. Where's the command turn on mic? Alright, good deal. That's it. So let's talk a little bit about PowerShell. This is great. So it's 10 years, 2006-2016 and I couldn't resist putting up the actual original book that came out with Monad. Anybody remember MSH and Monad? How many Monads do we have here? Thank you very much. You guys have been with us a long time. It's been a fun journey. I just couldn't resist. Lee Holmes actually had it on his desk just so you know where that photo shot came from. It's kind of a fun little thing. We thank you for your great support. Our mission we actually started with a while ago and this is a great one almost worth reading. Deliver an extensible scripting environment that is secure, interactive, programmable, production-ready, that's a lot, to enable consistent, now you're getting really out there, consistent and reliable automation of administration tasks. What's fascinating is, we're not there yet, but we're going to do our job eventually. Yeah, we are. Alright. The good news is we've had 10 years of progress. With your help, we made great strides, but there's a lot to do. In 10 years, I mean this year, November? November 14th. November 14th, 10 years of PowerShell. I mean it is quite an amazing journey for all of us, especially for all of you who been since day 1. A few gray hairs to prove it. And actually, the community, I mean what an amazing community. By the way, how many of you started using PowerShell 10 years ago? Isn't that amazing? Give these guys a hand. This has absolutely been a phenomenal journey. It's these people who together with the rest of the community made PowerShell what it is today. I mean this was known not only at Microsoft a journey, it was a journey that we took with all of you, so we really appreciate it. You help to get us to what we are today. Oh, and to make a point to that, these are about 80 names, I think it's 81. These are actually people, the community, not members of Microsoft, these are all community members who filed bugs in the year 2004 on PowerShell at the time. Isn't that an incredible list? Eighty people. From the very beginning, it's been a community effort and one thing I couldn't resist because we did miss this, we're going back in slides, a complete no, no, but I can't resist. Right there at the bottom right-hand side, if you want to, for those of you who are archeologists and you want to go see the original BNF, go see Jim Truher. He's actually got it. It's kind of fun. And Lee Holmes actually had at one point a VM running the old PowerShell. Just wanted to point that out. It's kind of a fun thing to have.

Past Year

Alright. Enough of the distant past. Moving to the past year. So what an amazing year. I mean we made a lot of changes in PowerShell. And really the PowerShell ecosystem and the technologies, I mean we, one of the things that probably you are obviously very familiar as the work that we do in Windows and Windows Server and you know it started only last year, there was a number of technical previews that we have released. It's through those technical previews we have released as well multiple WMF previews and it was an extraordinary journey for us because we learned tons and we're going to talk a little bit more about that. But actually we want to highlight that while we're still releasing WMF to each of you in a frequently basis, we've also been releasing management technologies in Windows and Windows Server. And in a couple of weeks, we're going to be launching the technical preview 5 Windows Server 2016. You're going to see some additional innovation that we are planning to release for both PowerShell and DSC. And talking about WMF 5, I can't resist, how's that for a download list. That's probably the fastest adoption I've ever seen of Windows Management Framework, and in fact, it's going, we don't actually have the next week there. We actually just finished week six and it keeps accelerating. So certainly, we've got people interested. They're landing it and just to point to the final PS resistance, that's what's in it. That's a carefully curated list and we expect there will be a test on it somewhere at the end of the summit to make sure we've covered all possible capabilities. You know, there's a lot there. There's a lot to tell people about. There's a lot to speak to. And you know the area that I think that, well recently, about a month ago we released the PowerShell gallery. How many of you are using the PowerShell gallery today? I mean, wow. This is pretty awesome. We had this vision back about a year ago about creating a repository, public repository for all of our community to come to basically share and make available resources to help us just adopting some of the new features of PowerShell and one of things that we learned when we did the launch, I mean look at the downloads per month. It's quite remarkable. I mean, right now is one of the faster galleries that we have in Microsoft. I think of the number right now. How many downloads we have so far? About 1.3 million. One point three million downloads of PowerShell artifacts I have already have occurred in the gallery. Now as we talk through that a little bit, there is something worth noting, so the caveat, and that's that although there's, I think this month we had almost 450,000 downloads, so it continues to accelerate rapidly. Some of those do come from the Azure RM profile, and so there's kind of, right, about 150,000 over the last 6 weeks. On the other hand, 150, the 450, probably 500,000, 600,000 in the past 6 weeks minus 150,000 is still a hell of a lot of downloads. So absolutely amazing ramp up, in spite of the actual sort of thing. We'll work with Azure to make sure the numbers get a little crisper, but it's been a phenomenal adoption. One thing that you heard probably earlier today from Neema, the product manager who presented the Nano coverage, is that the effort that we have made in PowerShell to also read faster PowerShell for the full CLR dependency to a core CLR dependency. This was something that was really important and in order to bring together a lighter, a smaller version of the OS. You make a high performance. When you're looking at the numbers, they aren't showing in that graph. They intend to actually show you kind of the impact that we have. In delivering the PowerShell, there's going to lean, it's going to be faster, and it's going to actually start moving towards to a more CoreCLR dependency in the future. I think this is really important as we start building the new server in Nano, so here that's something to highlight. Open source. We have been moving about as fast as we can within the resource constraints we have and the business constraints we have to go to open source. That journey just started and already we've actually got a ton of opportunity, certainly the gallery is certainly one thing. At that gallery, it's where we sometimes forget that we just launched this gallery in like April, May. It was just like almost a year ago, 2015. It's not very old. We're certainly out of the preview mode and people are using it in production all the time, so it's a great thing. We've also got the Script Analyzer, a bunch of editor services, the Visual Studio Code, and of course, the OpenSSH, which we're looking forward to. To making available. We'll talk a little bit more in the future about that. We want to thank each of you because I know that many of you in this room, the other people who are going to be watching this, they really have helped the team, engineering team, in particular, about just start evolving ourselves into this open source culture. So an interesting story is even before, we had just finished putting the gallery up and we hadn't yet made it public. We hadn't told anybody. We had just put it up there, no it wasn't in preview, it was just they didn't tell anybody it was up there and we were going to announce it the next day, and within hours, we got 600 lines of submission of poll request that actually go would you please take these in. How the hell did they find it and now they decide they're going to just? This guy must have been waiting for this. Oh my gosh. Oh, they did it and just landed on it. Within a few minutes after that, we got back to him and said oh could you make this tweak to your code and then we'll take it and he did minutes later. So just a great, it's been a great experience. Thank you very much for your overall support of that effort and of all of our efforts. We cannot do it without you. And especially we want to thank some people for their contributions because we've got some names here of people who really had an impact in our community on and delivering value into our open source projects. We have Iain Brighton and our thanks for the Hyper-VM, the DNS, ActiveDirectory, and DHCPServer, some great effort and work there. We really appreciate him. Aleksandar Nikolic for all the documentation. Boy, open documentation. Thank you. Very useful and helpful. Everybody cares about that. Keith Hill. I know I've got a Keith Hill here. Don't I have a Keith Hill? He's been around? No. Did I see him? I didn't see him. Alright, I know what I don't know. Apparently, he's not here. Clint Branham. We have a Clint. And Daniel Scott-Raynsford. These guys have really done a great job and we just want to say thank you publicly here, and of course, we'll say thank you to them also in person for their great effort. And this will continue. Every while we'll say thanks to a few key contributors across our community who have really helped us move forward. And I think, are we giving something away for the contributors? We are, yeah. Well these people in this particular case, Michael Green, if they are here, they can just see him. Michael Green has for them $15,000 worth of Azure money useful for one month. So one free month, that's a subscription to Azure. So basically, one month unlimited almost. Almost, depend on the usage. A lot. Depending on what you use. That's a lot. So it's a substantial amount. We really do want to say thanks. Our appreciation, not just to them, but to every single one of you who have contributed over time. If we could do it, we'd give it to everybody who contributed any code. Absolutely. Well we thank you. We really appreciate it. One more hand for these guys who contributed, even though they're not there. Thank you. Our thanks. Oh, the home page. The home page. We released the home page last year. Yeah, last year again. Last year a side project out of Joey, Neema, and a few others. They've done a great job. Okay, how many people have actually been to the PowerShell home page? Wahoo! Not that many. It's worth going to. It's not that hard to find. It's microsoft.com/powershell, so you can find it pretty easily. It is where we'll probably start driving a little more traffic. Even right now, of course, we have I know 60,000 I guess home page visits here in this past month or so, which is not huge for our traffic, but it's a substantial. The key is we'll start probably putting more announcements actually on the home page and then blogging almost directly referring to that. So as we'll start doing stuff, it'll start coming up, we'll start doing the advertisements, we'll start talking about maybe PowerShell around the world as we started doing any celebration stuff for the 10-year exact date anniversary, we probably will put it up there first and then we'll blog about it and that will enable us to actually to shift our blog a little bit, and I'll talk about it a little later, to a little more chatty, a little less formal tone, a little more the formal stuff up on the home page, but it's well worth your perusal and occasional look at. And if there's things that you think ought to be there that are not, feel free to tell us and I think it's even worth a UserVoice. Feel free to actually put it on UserVoice if you want to. It's just a way to actually communicate with us. And then we're just going to I'm just going to go very quick about other things that we delivered last year, well, actually even within the last few months. Right, you know one of the things that we've been doing, it started with Lee Holmes with JEA. How many of you are using JEA today in your enterprises? The few, the brave. I highly encourage to really look in JEA as a vehicle to really think about security and protection in your environment. Definitely one thing too that we are doing is the editing piece and as well as some encrypting MOFs. I think that this is something that is becoming more and more important. One of the things that we are learning with security is that PowerShell is becoming quite popular and you'll probably start seeing in the news, actually today there was a new article about DSC, in Hacker News, yeah in Hacker News and we're going to start seeing more and more of this. It's fantastic that we see the popularity and the send time, this creates obviously a challenge for all of us about how we start protecting PowerShell in investments and implementation in the enterprise. So this is just in a starting point of what we're going to be doing in the future and you're going to start seeing a lot more and more security focus, but here's to highlight some of the things that we have released so far. Now there's a thing coming in the next 5.1 update. Yeah. There's this concept about assume breach that we are focusing as well, so we'll talk a little bit more about later. And the PowerShell support and DSC integration for the OMS automation. I think that was, I don't know, how many of you know about Azure automation? Okay. So one of the things that, thank you, one of the things that we learned with Azure automation is that when we introduced the integration of native PowerShell and DSC, we saw an incredible adoption and I think it like about 25% of people in Azure automation today they are using some shape or form of DSC and I think that this is an important thing to highlight because one of the things that we are learning now with PowerShell is that we want to start integrating more and more into the cloud. It's starting to provide more of an extra service for our customers and the community. DSC Linux. It's been great because they finally put it out on the, I think, the open source code actually got it in December 2015, so we're sort of starting down there. This is another good path for us. We are going to slowly, but surely as we know try to give PowerShell technologies so you can use them where you need to. Overall, and I think DSC on Linux is the first kind of effort, sort of get our feet wet, and sort of understand what that process looks like, so we're really excited about that effort and hope it moves forward. I think over time trying to consolidate. Right now, they're two separate code bases. I think we're going to bring the code bases together and that is the plan. And over time want to bring them together so the Windows and the Linux code base would perhaps be the same. That'll take us a little bit of time to do that though and some shifts, just work with us. One of the advantages of doing stuff open is everybody sort of gets to see it and use it early. One of the disadvantages is everybody gets to see it and use it early and stuff changes a little bit, and so we're going to try to be a little more directional, I think, in our communication, in our guide and say look here's how we're, we'll be a little more open about here's where we're going with this, here's what we're doing on this source project, and work through both the RFC process. and we'll cover that a little more later, but just get your heads up. This is one of the great things. One of the other things we sort of skipped by is the Azure Extension Handler. We've had some pretty good traction with that right now with here right now where, of course, you can automatically do DSC, get stuff uploaded, downloaded, create your Azure VMs a lot more easily than you could before. Yeah, I think we are planning to continue updating the Azure Extension Handler with additional modules as well with this configuration and yeah. And so that's something that is going to be a continued this point moving forward. And then finally, the DSC integration with Chef, in particular, but as well as with Ansible. You know actually we should really thank Steve because Steve has done a lot of work to really take the power of DSC as a platform and really start to integrate it with Chef and I figured that was an amazing collaboration, so thank you for doing that. That's great. I'm expecting a t-shirt by the way. Looking forward for the t-shirt. And we also worked with others, so we can't only love him. No. When you build a platform, we have an awful lot of people. You can have preference, but at the end of the day, we are building a platform and I think that this is something that I think you're going to start seeing a lot more continuity is how we start basically integrating the platform with other open source projects. This is something that we're going to be doing more and more. And we appreciate anybody who layers tools over the platform that we have, we'll have to do some, but God bless the ISEs and tooling. Yes.

What Did We Learn?

So with the great things in the past, I'm really happy about what the team has accomplished, what all of you help to accomplish and it's that we learn a number of things and we want to share some of those learnings because I know that some of the things that we're going to talk about today are things that you are aware and we are aware as well and I want to address it, we want to address it from both angles for an engineer and as well as for product management because when you're thinking about, for example, agility, I've got to say something that we learn in Microsoft, when you try to ship in Windows and be agile, we learned that there are some interesting challenges because engineering processes, cadence tooling, we are evolving in the company. Yeah, sorry about that. So I'm going to give you a good example what happened in WMF 5.0 RTM and I know this was painful for some of you. What happened is that frankly we had a bug, we recalled the bug back in December. The amount of time it took us from taking that bug fix, we release a WMF 5.0, it was roughly like about two and a half months, almost three months. It was too long. It was frankly a---you can ask tons of questions like you see if I was in your place, I would be asking like wow what kind of bug is this. This is like so dramatic that it's taking the team so long or they didn't know the uses of it, are there more quality problems, right? In the instance, frankly is that yeah, we had a bug, part of our agility, we missed this bug, but it was that reaction. The amount of time that it took us to react, it was primarily driven that when you got a bill, a Windows package, it just takes a lot of time. We've got to go through a number of days and processes that really doesn't tie well with the Windows shipping vehicle. So moving forward, we're going to change that. We'll talk a little bit more about that. We're working on that and we're working with the Windows team as well. People have this in focus. It's just a whole corporate effort. Yeah, absolutely. And our efforts as well. We're addressing it at each layer as we can, I think is the right answer. So thank you for support and apologize because I know this some of you took a dependency in the December release and definitely I know that counts and it needs to and is something we're addressing, and we'll talk a little bit more on cloud. It was a year ago Kenneth and I we were here talking about this transformation in the company culture, right. The transformation is that when Satya took over the company as the CEO, it was very clear that we needed to adjust, not adjust the way how we worked, but actually in the way how our engineering roles they were evolving. So if you recall, the engineering role evolved in Microsoft where the developer and the tester became one single role called software engineer. So we still, we've been evolving this. They have a significant impact on all of us because when you think about Microsoft, we've been operating under that moral for 35 years. In one day, we decide well we're just going to change all that. Well a key example goes back into the open source stuff because like a few years ago, if I considered any of those projects, I would have been escorted not so graciously out of the building, right. SQL and Linux? And so now it's a very different culture. It's a very different top/down mindset and there's still a lot of how do I do my day job, exactly what am I responsible for that we still have to work through and make sure we land the process, and occasionally, we're going to make just a few errors on our way, so bear with us, we're on a, I think, great path, but we're not quite done yet. I think we are definitely closer than ever before. Amen. That's exactly right. Okay, we're sustained there. As a part of that, we're actually going to continue to evolve how we're handling communications, right. So let's talk about a few things going through there. One is even just during the re-release, we felt we were pretty aggressive on the communication for the WMF recall. As we started the re-release over that period, we kind of went a little quiet. It was like okay, we think we're going to have it the end of January and then it was like oh, okay new problem, okay, into February, and I was a little frustrated from our standpoint. I'm sure it was from your standpoint. And we're sort of working through exactly how when we have nothing to say, we say something, so the people know we haven't forgotten you, but I've got nothing more I can tell you and how do we do that? There's some pros and cons to doing that every minute versus every week versus every month and sometimes as you go through that as we work this journey, there's times where as we communicate more frequently, we will have a tendency to get it a little wrong because we even have been a place in that case where I would have said four days before an event, hey it's going to be this date and then like three days later, oh no, it's not. And so, we're struggling with ourselves and we'll work through that with the community, sort of get a sense as to how we communicate these events. So just to be clear, A, we definitely want you to know everything you need to know to get your jobs done and to make you comfortable, and B, we're working pretty accurately through that process and how we can address our needs and your needs simultaneously. We do ask for your continued support and a little patience as we sort of fight through that. In general, we'll probably opt to communicate a little more than what we have in the past, but just be aware that there's always trade-offs here. Just as an observation, we continue to move still fairly quickly through the preview and the various models and I would say finally we're beginning to get it, and on the other hand, we just started the home page based on people's lack of how to where do I go to get a canonical sense of what PowerShell's really doing. I get all these different blogs and so forth. We started the home page for that. We started the gallery. We started all these projects up and I would say that over the next five to six months we're going to be sort of more operationalizing some of the communication efforts that we have around them. How do we do this? One thing we've thought about doing we'll see how it flies, but it's certainly on the fun idea list is maybe we'll end up doing it every other month little top of mind where I'll sit down and maybe Angel will join me and we'll do a looking back last month, looking forward the next month, and here's some key technology things that we're thinking about and then maybe once in a while, we'll also do maybe sort of more of a Q and A, a broader Q and A with the whole community. We just finished a couple hours this morning with a group on DSC. That was a useful, it turned out not to be DSC, a bunch of things, but that kind of thing is on our mind. I don't know why or where we'll land on them, but we certainly want to make sure that we involve and integrate more and more. Let me, I'm going to spend just a little more time, I know I should probably talk about all the stuff we're going to do, but since I'm here, I guess I'll end it here and then we'll come back to it. Innately, the community is immensely hugely larger than the PowerShell team. Right. There are just more people out there, which is good because that's how we make money, right. If we didn't, then Windows might be bad. With that many downloads, you know people are using, 1.3 million, even if some are Azure, people are using the technology they've got in production. There's going to be a ton of questions. I have always been astonished that for the past 10 years when we do a new PowerShell session and even the getting started sessions that Don or Jeffrey occasionally do are packed with 1,000 people. After 10 to 15 years, alright 13 years of talking about PowerShell at the various Microsoft events, we still have hundreds and hundreds of people who have never touched it yet. That's going to continue. What that means is we got a lot of people not only in the community now, but a ton coming in and then we have the audacity to create new technologies like DSC, which for people who are already here create a whole new set of questions and issues and things, so we rely on the community. We rely on the community to help the community. I know we can't scale to do that. What we can scale to do is to engage and understand when there are firestorms and engage in the firestorm, try to prevent it with some documentation and we can educate the educators. We can talk to you guys. Does that make sense? There is this layer between us where we just have to actually use you and other people like you. When we started the first PowerShell summit, I spent a little while on this, but when we started the first PowerShell summit it was because in fact we were at a Microsoft conference, TechEd Europe, I think it was Berlin, and Jeffrey and I had just finished a, we'd had again a massive intro session and then we'd had an ask the experts kind of a thing and we filled it and we filled it again the second day we actually had to turn people away and we just said okay where can we go to give people really in-depth knowledge of PowerShell. And we said, we've got to have our own little conference, don't we? And then we said, that was one breath, the second breath is and we can't do it. Because if we do it, Microsoft has an attention span of a nat. It's like oh, a new thing. And so, we said the community has got to be able to come together and do that and that's when we worked with Don and others to try to pull together the very first PowerShell summit in Red Rock Las Vegas four or five years ago and the intent there was we'll talk to you guys and we'll get you guys to help us improve our product because you have higher access and they have a high bandwidth conference for you guys to make sure you connect with us later. So I'm sorry for that long soliloquy. I just want you to get a sense of that's how I think of it, that's why we spend so much time on these things. We really want to get educated by you and to make sure that you guys have the right message. So if there's problems you're having in the community, if you're answering questions, unable to do them, answer them, answer them, answer them, when you hit a brick wall, reach out to us directly. Go ahead and reach out to the Powershell PM team at microsoft.com, go ahead and reach out directly on UserVoice. Do that kind of thing which will inform us that you need help communicating with other people because we don't always know. We can't always know. I think I just want to highlight one point. I think that, look, we are moving a lot faster than ever before and I know that we caused some pain last year with the WMF previews, right. We were releasing these previews and it sounds really good, but in reality, we are not keeping track and communicating how those previews they were affecting each of you with the changes that we were making in the technology. So we need some times the help of the community because as we start having these communication channels, if we start basically train the trainer I think at this point, which who is you, we need to be able to depend on our community as well to help us through Twitter, through Facebook, through all the different channels where we start seeing a lot of momentum about communicating about PowerShell issues or PowerShell feedback, so it's really, really important. Absolutely. And just to be clear, along with the general community engagement, which we'll up a bit, we'll go through that in the next slide. We do have actually in the past year, we've had hundreds, hundreds and hundreds of in fact, direct customer engagement with some specific cases. If there are people you think that have a particularly interesting set of use cases for PowerShell or PowerShell DSC, reach out to me, my team KenHansen@powershell.com or my team and actually just talk to one of my guys or Ron Hill and we'll put them on the list of people that we will potentially go and just chat with and drill a little bit into their scenario. We can only spend, obviously, we only have so much effort, but it's an important list to us. We're always happy to add to it and it's been I would say increasing by probably a fact of 20 over the past year from what it was before, so it's not just a casual invitation. It is intended to be used just use it wisely, pick good people, and we'll make sure we engage them a bit more deeply.

Engineering Moving Forward

So yeah, so we learn a lot and hopefully that resonates with you the way we are addressing some of the main areas. So specifically about engineering, one of the things that we are changing in the culture within the team is that we are becoming more and more of a DevOps culture about how we are developing innovation within the team. And what I mean by DevOps culture and model is that we really are focusing more of how we deliver innovation based on solutions that they are really addressing the customer problem. This kind of goes to the point that Kenneth was referring earlier, that direct communication with customers and with the community is surfacing scenarios and controlling the spaces that we need to start thinking about not only just delivering features, but it's about delivering the solutions. You deliver that that in way that it's a platform that enables others, you to absolutely take that technology and results in progress within your companies. Now here that is a really important shift culturally how we are doing that. The other thing too is the concept about functional and test code. Remember when I said only about the roles and responsibilities how they are evolving in Microsoft, so one of the things that we are doing today is start thinking about the software engineering. They need to start thinking about when you write functional code, you've got to have test code associated with it. So part of the code reviews prior to checking new code is to be sure that you have robust test coverage as part of your functional code. The other pieces have all the scenario validation and demos. For all of you who maybe has been adopting a DevOps model, this is really important. It's about really holding accountable the engineers to be able to demo the features and the scenarios that they are working on. You see something really cool. Today, actually later today, you're going to see a little bit of that. But I think it also will enable us is that PMP to partner with us, then when we finish the work, yeah, were also addressing that problem. And we go reach out to you guys and others and make sure it really is. So there's an ongoing process here that we're evolving through. How do we do our engineering in a way that's a little more test and demo-driven, if you will, almost to make sure we're completing the scenarios. A few places where we've not quite closed those gaps, we're trying to close and I think it'll really help us over the next year or so as we try to land the complete result if you will. Yeah, I think that through that journey, I'll ask the obvious question. How many of you think that we should do more about debugging and troubleshooting in PowerShell and DSC? It's a rhetorical. It's a rhetorical question because we know that. So yeah, where I think it's going to help is that the engineering team has started developing the scenarios to these solutions. We've got to start having the mindset to say how easy for me is to also troubleshoot the scenario. What kind of instrumentation I need to put in place? What kind of eventing I need to put in place? It will really help you have that telemetry to help us to troubleshoot. You'd be able as well to debug problems within the environment. So this is a scenario that we're going to do little more, include like a script on alias has been a greatest start. We learn a lot of that. That is a great approach and how we can start helping you and helping others to really focus about how debugging and troubleshooting can be done within PowerShell and DSC both in the private cloud, as well as in the public. Another thing that we do, either this is going to start becoming absolutely more important is about implementing this process of tracking things, right. I want to start providing a _____ engineer monitor, visibility, and transparency to you about our tests, about the results of those tests, so you can have a tool as well to apply the same test. It could be Pester, it could be a set of other type of a technology. But it will help you to say when I deliver a module or I'm delivering a solution, that you have some tests associated with them. It gives you visibility about what the results are expected, so when you are doing your own development and deployment, actually that will have as a tool for you to actually do the same things that we do in Microsoft. And it's really an interesting approach because I will say that it's a little more, not just a DevOps approach, but also taking kind of the open source model to some extent. Even if the code is not open source, maybe we should pretend that it is and how we would actually do our development in those scenarios and that would actually help us out a little bit. So we're certainly, I think, kind of learning as we go from both models and trying to adopt it inside as well as out. So we'll see an increase in that. Absolutely. I think we talked a bit about the overall of what we're going to start doing here. Let me just, I'll bring these all up and then we'll talk about it. So we are, excuse me, we will be delivering more blogs, more, and I would say right now the PowerShell blog, and you can give me feedback on this after, I'm happy with this, right now the PowerShell blog occasionally is a little more formal. It's more casual. We're probably tone that down a bit, move the formality a little bit more to the home page announcements and hopefully just have a little chattier blog where people are more comfortable sort of having a discussion, if you will. We'll increase the overall, the community artifacts, and our engagement and this is worth noting. We do rely on you overall to handle the community in a broad sense, but I would say that we need to lean in a little more in a few areas and actually, A, make sure we're monitoring to check if there's okay, some fire blows up, how do we solve this really quickly, make sure we're really addressing the issues the community is beginning to have, make sure we're backstopping it and engage directly just as frequently. We had a great time when we had that new Twitter account that actually Angel created for us and it's been a lot of fun to watch people jump on it, it's been fun to use, obviously, you're not going to have an in-depth of design discussion on Twitter. Some people occasionally try to, in which case, we'll try to kindly redirect you, but we do have a process for that. Yeah, in fact, over time we want more and more our thinking, in fact, we should address this just a little bit. UserVoice is actually not up here and it probably should be. It's really not a social media, but part of what we do want you to do over time is to drive requests and issues into UserVoice, so we track them. They're being looked at across many levels of the organization it turns out and the team is looking at them regularly every week actually just to make sure that we actually got them. Now we move hundreds and hundreds from Connect, alright, and so those we're sort of going back through more slowly, but anything recent, we should be on fairly quick. And for some reason you haven't gotten a response within some reasonable period of time, feel free to reach out direct and say hey I just thought I'd note that I put this out there. You can do that just directly via email. But nonetheless, over time, now just because it's up there and we've looked at it, doesn't mean we're going to do it the next week, right. So we'll try to also get a little more transparent as we can within our reasonable business constraints about when we're actually doing things. We're working through ongoing planning on a regular basis, so it's a little, it will come and go our ability to share that. Sometimes it will be high, sometimes it will be low. Over time, we think that we will end up with the issues being kind of raised on UserVoice and then probably more in-depth discussions as we begin to flow this process more in the RFC GitHub kind of a discussion, so we can have those discussions. We just started. If you'll notice with some RFC stuff even today and we'll probably grow that, so sort of look in that space, if you will. Theoretically, if we were to look into the community holistically, you see Twitter and otherwise is broad discussion, broad quick tweets. The blog is a more casual interaction. The home page the more official announcement. UserVoice is the way for you guys to tell us hey we have a bug or a feature request to weigh in on that and then in-depth discussion will probably take more of the GitHub RFC. Now I think one of the things that I want to highlight also so well are involvement, how we are going to be working with you in the future. I think that this is an important thing and it's a change for us culturally is that we need to go also to where you are. I mean the reason why we've done the Twitter account or there is actually a really popular Facebook PowerShell page, yeah, it's got an alias, we start learning that the community is everywhere, which is awesome. We are like super happy about it and this then does create a challenge for us because the challenge that we've got to go and learn, the discussions that are occurring in all these different social medias, and then try to bring those learnings back into our team to figure out how to address a particular problem or how to prioritize work. Another thing too that I want to highlight about increasing the community's sharing artifacts. So we are going to go and invest a little more as we developing these solutions to give you with those solutions the artifacts to enable those scenarios. I think that that's really important, so it's pretty open source. I want to be able to make it completely available for you, not only just to use, but contribute as well. So help us. Help us with the contributions. There are tons of innovation happening in all your companies that you do every day, so if you see a vehicle for you to share that with the rest of you, this is the time to do it because I think if we evolve, I think that we want to become more and more open with all of us basically to share these artifacts across the companies. Absolutely. And one thing worth noting here, I know I got the direct customer engagement. We already talked about that. I won't land it again, but I will note that we are imminently aware that many of you are kind of forward leaning. You're kind of in the early adopter crazy innovator early majority phases and there are many companies that still exist and Microsoft leveraging our technology, which are more in the late majority lag are those who are more cautious in their approach. That's okay. We want to hear from them too and try to support them as well. So while we're pushing more and more aggressively in this open source and new world, we know that there are some people, in fact, there was an interesting survey read the other day where as soon as we said the word modern, like the entire room just died. It's like I don't want anything modern. I want what's working today. It's like, you know, and so there is this interesting a blend of culture that we work with on a regular base that Microsoft just because of the breadth of our offering. That doesn't go away. We're happy. We want to work with both aspects. We will be very forward leaning moving forward, but we're simultaneously, one of the reasons we go so far down with WMF 5 is we recognize that people are not necessarily moving forward on the core technologies, but they want to, but we feel that PowerShell actually enables them to run their operations across all technologies better, more efficiently, and actually allows them to adopt a new technologies underlying platform technologies sooner. That's actually part of the argument we have to make in order to go down level. So we're well aware of that. One thing I want touch while we're waiting for Erin is that we talk a little bit about this whole community artifact sharing. The one thing that we learn, there were a couple of questions earlier in the previous talk that we had, is this concept about versioning. You know how problematic versioning is becoming, right. As we start releasing more modules, scripts, previews, we realize that the current versioning scheme that we created literally 10 years ago, it really ties into Windows, it's not going to scale moving forward. So when you're still thinking about versioning of DSC resources for like for a class base resources, well when you start thinking about, it's a great example, when you start thinking about all these different cmdlets that we're going to be delivering in the galleries, I feel one of the obvious questions like how are we going to be managing versioning, right. So one of the things the team is doing right now, we are going through a journey to figure out how to redesign a versioning scheme, so as we start releasing more previews and more artifacts to the galleries, we'll start addressing these versioning scheme. I think we're going to have some discussions in the near future with many of you to give us feedback, right. We'll start some MVA discussions, we'll go over and eventually get to RFC sometime in the next while as we start talking to the versioning. And it's not just the modules. Even as you know, we've put PowerShell on CoreCLR in order to put it to Nano. Well CoreCLR does side by side, so how are we going to handle that? How are we going to handle that versioning story? So we're doing kind of an all-up think about. We'll just give you mine. Here you can take it. We're just giving an all-up discussion around versioning. We have to step back. And versioning is one of those tricky things. There is no easy right answer. If there was, we would just steal it. We're not proud here alright. And if you have it, I'm okay stealing it just to be clear. I'm not proud about this. We don't have to reinvent the version. I don't feel the need to reinvent any wheels here. We don't want to do that. But it just turns out none of these wheels really are great that we've found yet, so but we'll work with you to see if we get one that really works across all of the artifacts that work with PowerShell.

Looking Forward - Windows

Alright, I think we can move on. We're going to shift a little bit. Absolutely. We have a guest speaker today who's going to join us. How many of you know Erin? Yeah. Erin is my boss, that's one thing and she also runs Windows Server completely and a few other things as well, so she's going to tell us a little more about the future. Awesome. It's great to be here. It's amazing to see from kind of coming into this room, was it two years ago Don that I think we were here. Three, two years ago. Two years ago here. Two years ago and just seeing the increase in folks that are seeing and interested in PowerShell, interested in what we're doing in a DevOp space and here both learning from what we're putting out, but also sharing the collective wisdom that's in this room. It's amazing to see. As Kenneth said, I'm Erin Chaplin, the director of program management for the Windows Server team and I thought I would just share a few top of mind things around what we're doing in Windows Server in particularly how PowerShell and the whole DevOps experience fits into that. We're working, we're hard at work right now on Windows Server 2016. Our technical preview for it came out late last year. We're kind of on the eve of having a technical preview 5 that should be out later this month and that's kind of our last big release before we'll hit like the final version of Windows Server that's due out later this year. Windows Server is an interesting point, right, it's an interesting product and it's an interesting point in the industry and just in general, right. We're at a transition period and when we think about Windows Server, we really think about two roles that Windows Server plays. Traditionally, it's been very much part of the infrastructure and that whole space and as we see the infrastructure and where that's moving, that's undergoing a lot of changes, particularly as we think about the whole transition to the cloud, right, and I think one of the things for you to think about or have in context is when we think about the cloud, we really, we don't think about kind of public cloud or private cloud or something, we think about cloud as a model, right. And that's a big transition that we're going through just in terms of how do you deliver applications and services in a more agile way and we haven't got it perfect yet, right, but we're working through that transition as we think about where we go and how we help customers embrace that cloud model on whatever term that they have. And Windows Server plays a key role of that because it fulfills the on-premises sort of promise to customers of being able to stand up a cloud model and stand up their infrastructure on their terms. And so, Windows Server is kind of part of the whole cloud ecosystem we would say either running it on-premises in a data center running it, in some service provider, or Windows Server is actually the underpinnings of what the Azure cloud runs on top of, and so a lot of the work that you'll see with us in Windows Server 2016 that's coming out is focused very much on that cloud model and how are we the cloud-ready operating system. What are we doing in terms of taking the learnings that we have from Azure and running kind of this public cloud at very large hyper scale and then bringing that down and delivering it to customers to provide the Fabric of the infrastructure within their environment. So huge investments we've made in the whole software define space, so software define networking, really kind of providing that infrastructure that allows you to have the portability of your applications and be able to move them right between different clouds, between on-prem, between the public cloud, work that we're doing in the software define storage space to really let you kind of get the power of software and the high resiliency that you've had and historically through hardware, but delivered through software to lay as the foundation and then all the investments that we're making kind of in the Hyper-V layer and the compute layer that go along with that. One of the big things that I'll just kind of put because I think it's important in the context of this community is a lot of the work that we're doing around assurance, right, and we know the world has changed from a security point of view in just in terms of how people think about their operating environment and wanting that assurance that what they're running in the Fabric and the infrastructure is secure. We've done a lot of work in 2016 introducing a new set of functionality around shielded VMs and this ability to really kind of encrypt using BitLocker your virtual machines that are running on top of the Fabric and then have that, the Fabric tested in a way so that you can guarantee that the Fabric is the Fabric you want it to be and that the VM won't boot. And a lot of these things where we're thinking about, okay these things you maybe don't need to worry about when you're in a public cloud environment, right, because you trust the provider or what not, but when you are wanting to stand up your own infrastructure, they become super important for you to have. So we play a huge role in Windows Server just around that infrastructure and how we're taking that forward and how we're helping organizations in a modernized the underlying infrastructure that they have. The other part that's kind of super relevant to this audience is the role that we play as an application platform, right. Really you have an infrastructure. Why do you have an infrastructure? It's to run virtual machines and the new world containers. Why do you have those? It's to run applications. And we really see that whole space is changing pretty dramatically, right. If you went back and thought about how applications go from conception through to the deployment in an environment 10 years ago, we're in a very, very different world right now. That's why you're seeing kind of a lot of work that we're doing, particularly around PowerShell, Desired State Configuration, but also then the ecosystem around that of Nano and the new installer work that we're doing and contributing to the testing environment through things like Pester and all of those things as being super important to our strategy and we're trying to figure out how we modernize that as well and how do we take a lot of the agile concepts and things that we're seeing more in the OSS world and kind of Linux world and community of how people put together applications and how they go and kind of get the packages they need, put them together, integrate them, have configuration as code, do that in a kind of heterogeneous world and put that plumbing and that basis into what we provide from a platform point of view for Windows Server. So 2016, there's a ton of stuff in it. It is I would say the start of this pivot towards this more cloud model and how we're thinking about taking it into and landing that transformation in organizations all the way from the infrastructure up to the development space and we see PowerShell as being kind of in some sense a bit of the glue between those two worlds, right. It has a lot of and when we initially developed PowerShell, there was a lot of emphasis around it being a language and it having a lot of these properties that were really important from a development space and if you have a development background, those things being things that you come to expect and come to need in a scripting environment, etc. But it also, we've done it and put a lot of our IT hat on it as we implemented it to make sure it was kind of user friendly and it mapped into what you need to do from an IT infrastructure point of view. And so we really do think it's positioned as being the glue between these two worlds and a lot of the investments that you've probably had Kenneth and Angel talk about today are part of us thinking about how do we evolve it and take it forward and have it be a showcase in our arsenal of tools in this new model, in this new world as we think about really helping people embrace the cloud. Again, whether or not that be public, whether or not that be on-premise or through a partner or trusted service provider. So hopefully that gives you a little bit of context to kind of where we are in server, where we're going. It's a release that we're incredibly proud of just in terms of the amount of innovation that I think that we're bringing to bear. And so if you haven't checked it out, I would encourage you not only to check out the PowerShell and the kind of WMF aspects of it, but also a bunch of the core investments that are going in the platform underneath both from an IT infra-sight point of view, but also then as the next generation application platform when that's really starting to be designed or optimized for more of that cloud first world. So with that, I think I'll stop and yeah. Awesome. We're looking forward to it. Nano's going to be a great one in Windows Server. Nano, well I just want to say Nano and containers, right, are pretty big in terms of the transformation that we're making. I mean just seem odd where we talk about Nano and the impact it's going to have just for the size, for an automation, for a performance, so definitely the role that PowerShell is going to play there like Erin says it's going to be significant. We're going to need the help of our community to actually start taking some of the assisting cmdlets and investments that we manuscript and you bring those down through Nano because we start thinking about heterogeneous type of environments, but in management and automation, if we want PowerShell to assist in investments that you have to be reused when managing Nano as well. And simultaneously when the server, as we just talked about before, spans this broad spectrum of customers, and so we're actually able to land you for those who are going a little slower than necessarily the Nano adoption, so we really think 2016 is sort of began to both show the future and handle the past and kind of bring them a little more together in a more cohesive way. And so the last thing I can say is probably in a little bit of a request and an invitation, I would say, and that is we're going through a transition. I think if you, it's an exciting time to be at Microsoft right now. I've been there for almost 18 years and it's just the climate internally and just the willingness for people to take a look at how we've done things in the past and really to go oh does that really work, is that really what the world needs, is that really how we should interface externally, is just amazing. And so, we are going through this transition. I think we don't have it all figured out, but I think we have a pretty good idea based upon input from folks like yourself about what's needed and what the problem space is and what could help and we're trying to do our best to kind of match, I would say, that input that we have with what we put out. And so, it would be first kind of a request to have a little patience, right. We're going through this journey and this transformation together and we're going to get hopefully more right than wrong, but the good news is that we are increasing our frequency of getting things out there, and so we can adjust as we go along. But we won't get it right unless we have the partnership with folks like you that are in this room, and so please take advantage of the various different community organizations, the UserVoice site that we have, all of those pieces, in order to really channel your feedback in because I will tell you if anything this is one of the teams that I think it takes that input more to heart than other parts of the organization that have been less, more isolated in the past and like it's a little bit more of a heavy lift for me to get the folks that are working on kind of core protocol level stuff that they don't really have a community to interface with to go accept some piece of user input. I think the PowerShell team has a reputation of listening to those things and have the kind of request that we're not going to get it all right, but keep it coming because I think that only benefits both of us in the long run. So thank you. Thank you very much.

Looking Forward / Predictions - PowerShell

Well that was a good view of the future of Windows Server, but I think we want to talk a little bit about looking forward at our predictions for PowerShell in the future. We can't have a looking forward discussion without some random predictions, so we have to. Disclaimer, we are not architects like Jeffrey. These are predictions that I feel pretty good that I think that we can achieve. Well kind of in an order of like achievable to aspirational. Exactly. So I feel pretty good about kind of our predictions, so I've got to bet, I would bet in our predictions--- I'm confident of that one. I'm pretty confident about this one. So as Erin mentioned earlier, Windows Server 2016 is coming up and it will be available sometime this year. And with that, we will release a WMF 5.1 update that will contain all the changes from WMF 5.0 all the way through that what we call RTM release, so it will accumulate those changes, yeah, whatever we end up calling it, whatever marketing is used. GA I think is was we call it nowadays. I think so, yes. So GA will accumulate all the fixes and we'll re-release to help you take the latest changes. One thing that's really important, it goes back to the feedback. Please give us the feedback because this is the opportunity if there are some bugs, issues, suggestions that you want us to consider that is this interesting time between now and then, they can help you to get those changes and bug fixes into the WMF 5.1. And the goal of this release of 5.1 is a little more of a service pack type of thing, if you will, because in fact it will be the way that we support 5.0 moving forward, so we already put WMF 5.0 out there. We'll be making the usually Microsoft support issues whatever it is with it all that we normally don't have that many QFEs out of PowerShell, still we would do those kind of things, but we roll up any and all changes into 5.1 and that's how we would actually support the 5.0 after that and then we'll be making changes on the 5.1 base and you'll get the usual Microsoft contract on 5.1, which would be required then like 3 or 4 months after we put out 5.1. We'll do whatever sort of makes sense in that space. Will those updates be released as KBs? We're still in discussion to figure out exactly how those will also be released. That's a great, yeah. So we'll keep looking into that. That's a good question by the way. That's actually we are going through that, internalizing that right now. But this is a, anyway I just want to make sure people have got their head out, they know we've got a lot of done with 5.0. We didn't want people to be surprised. We said oh by the way, now you need to roll to the 5.1 in order to continue forward as it will align with the Windows Server release and all the support alignment with a Windows Server product, if that makes sense. And we'll blog about that later. Oh, this one! Yeah, OpenSSH. So OpenSSH is an open source project by the way. It's already available for many of you. Please feel free to contribute. We'll keep updating it. We will release an OpenSSH in Windows 2016. It will be available. It will be just to give you visibility will be first of all. It's not going to be perfect. Oh no, nothing is perfect in software, but I'm sure we can make it awesome. It's going to be great. But I think the point here is that initial, just to set expectation, it will be kind of like WMF. It would be what we call an autobahn, so it will be a package that we'll make available for you in the gallery that you can install it. It will be supported. And then over time where one of the things that we are considering is actually bring as part of Windows and install, but we need because we want to get some of the feedback from you and from the community from using it and then we'll continue improving it. We are working very closely with the OpenDSD community, as well. They are helping us, especially about together the crypto security of Windows with OpenSSH, which is really what we are spending most of time. And we're really excited about this project because I think it's going to help us a lot in the enterprise. This is a pick by many of you and by many other people who are not here today. That's right. A lot of fun. Oh and this one. We are continuing to invest in security and breach. I think Erin highlighted that a little bit. That hasn't gone away. It's only become more important to us and to our customers. Even as a part of that, from our perspective in security, it also goes back to some of the quality issues. We continually are asked hey we want the gallery to be the place to go for high quality stuff. Security also is kind of related to that. We're going to continue to focus on security and quality on a regular basis. One thing just to point out, I mean, this is a really important area for investing. How many of you know Lee Holmes? Okay, so Lee Holmes' role is changing. Recently, we made an announcement internally in the company. He's going to be our lead architect for security for Enterprise Cloud because we start realizing we need to start thinking about security, not necessarily just within PowerShell, but when you take PowerShell and you put it in the cloud, how are we going to be thinking about protecting, doing things like assume breach, and that sort of scenario. So that tells you that we are really behind the security efforts. It definitely is the top of mind for the company for us. And it's kind of cool you came out of PowerShell. Yeah. For all the security, you might remember our very first PowerShell don't turn on remoting because of the supposed exploit. And documentation. How many people would like documentation? And here to make sure that in fact you get that, where's Don? Right here. Don, please give us, stand up and give us just a few minutes if you will. So I didn't realize that the tail end of that was bet Don's job. That's right. I didn't warn him I was going to put that on the slide. So not only was it this Don, but it was the Don that just came up.

Future of PowerShell Documentation

So thanks for letting me swing by. I've been in PowerShell 14, 15 days. Wahoo! Not bad. Thank you very much. So I've been at the company for about 17, I'm with Microsoft for about 17 years in a lot of different roles, everything from evangelism, to developer support to ultimately landing in SQL Server and the data platform. Up until a couple weeks ago, I was the director of content publishing for that whole shebang bang, so if it's analytics, if it's SQL Server, if it's machine learning, real time, that was me and my group and we produced something like 18 billion topics on a yearly basis. You were looking at 60,000 page views a month. On average, I get 18 million page views over on SQL Server. So why would I leave there to come to this room? And if any of you have not seen Ed Wilson's blog. Why would you ask that question? I don't understand that. It's a lead. I was confused. It's a lead in. Okay, just thought I'd make sure. I mean, besides the extra pay I mean always gets to work in PowerShell. So why would I leave that to come over here. And here's the thing that is really motivating for me. What am I more passionate about than a job title or a particular technology? It's when I see a very active community that gives a crap about helping the person to their left, that's willing to contribute their knowledge back to the betterment of the community as a whole. Love that. I love it when we have a dev team that actually cares about the scenarios you're trying to do and whether we can get them all done and do them cleanly. That's one thing. It'll mature over time. But you put those two things together and you put all that goodness in an open framework where you have the ability not just to help the person next to you, but help the person across the country, help the person that's just getting started, help fix a problem up in open source. That's where things get really, really interesting, and to be honest, you can't move something like the data platform over to a system like that, but you can take a language and you can take that platform, and you can take this community and enable it to support each other. So I would encourage everyone to take a look at Ed Wilson's blog from yesterday. I was a guest blogger there and we're talking about what's the future of PowerShell documentation. The future of PowerShell documentation is actually sitting in this room and listening to this video ultimately long term. We're going to go completely open source with our documentation, which means you have access. If you see something that's bugging you, you can go fix it. If there's a better way of explaining some a hard concept, you can help us help the person to your left. All of our documentation is going to be up on GitHub, very simple contribution requirements, but everything that you do, you get credit for. Your names on that article, your picture's on that article. If you're promoting your personal portfolio and your persona, we're going to link out to your stuff all in an effort to get us to a point where we take a look at some of the other open source languages like Python. Now Python documentation is ugo. It is the way, it's like going back to 1985. It looks horrible, but the content itself is exceptional. It's easy to discover. They break the hard concepts down and there's not a bunch of paid writers sitting in a dark room somewhere just cranking out words. It's the community going up there hey there's a better way. Or don't forget you've got to remember these three things. So we're kicking it off with DSC right now. There's a pilot going on. JP who's one of my writers here. We've got another swath of content that's moving up into the open portfolio. We're actually going to be bringing core PowerShell as a reference, so as we talk about the shifts that are going on in engineering, documentation has been doing the same thing. If you look at how many product teams that we work with that are producing PowerShell, it went from 2 or 3 to I think our last count was 380 development teams at Microsoft are contributing to PowerShell in some way, whether through modules or extensions. To be able to operate at that speed, it can't just be the PowerShell team. And it can't be just the four or five writers that I have on my team. It's going to take a community to make this actually work and we're going to start that by investing and opening it up, getting it out of the way of you guys helping and supporting each other and sharing that guidance. So my name is Don Gill, my email address is the easiest one ever, dongill@microsoft.com or Twitter, you can always hit me up if there's something wrong, there's something we should be doing better, I'm the guy that's going to help make it happen, whether it's on the online, whether it's in the shell itself, we can do this better. Hit me up. Thank you. Thank you. Yah, thank you much.

Looking Forward / Predictions - PowerShell Continued

Yeah, yes. We can't resist. Now we're into the more fun predictions. We always have, we've always believed in PowerShell and in audit, it's PowerShell or no UI, it's UI or no PowerShell. We've always believed the worlds that peanut butter and chocolate work better together, unless you're allergic to peanuts, but that's not the problem. So what we're trying to say here is that over time we want to make sure we have great UI over PowerShell. And so, we've actually plumbed a few things, but we will get seriously considering, notice the word consider, it's might caveat. Seriously. But seriously, find some way to actually auto-generate UI out of PowerShell modules. Yeah, awesome. It might not be the prettiest, but it should be sufficient and then hopefully we're even looking at ways for you then to be able to do curate a UI on top of that. It's an interesting project. A few people told us we were crazy and that we would fail, but we're having a lot of fun and we are somewhat confident. Community. I think, we've been talking a lot about community and definitely the future of PowerShell is the community. It's all of us. It's the people here, the people who is basically around the planet using PowerShell every day and I think as we start thinking about communication, like how we're going to work together, we want you to be a significant part of the future of PowerShell. We are introducing an RFC process. I believe in the MVP summit for all of you who are MVPs, you got a preview of that. But this is something that we want more and more. PowerShell file. We've done amazing stuff together. We're going to continue investing in PowerShell when it comes to integrating into Windows, integrating with some of the Microsoft technologies, but there's this other aspect about you are the expert in PowerShell. We need your help. So if you have ideas, if you want to request changes, design, we want to provide you a vehicle to do that. UserVoice, that kind of stuff. Absolutely. And the RFC process, as well, right. Absolutely. And the other thing too about the GitHub, I think that this is another thing, GitHub will begin our official place where we want our communities to work together and you're going to start seeing a lot more and more of GitHub open source projects. A lot more. We just started. This is just the starting of what is coming next and we want that to be absolutely the repository for all of us, for you to start sharing and we're going to start providing, how many of you know Joey? Joey's somewhere in the back. He's what I call our community problem manager, it's the best description, and he's working right now in our government about how we're going to be doing these, how it's going to help the community to work together about sharing artifacts and technology moving forward. Yeah, and I don't want to hit, I know we've hit the community a lot and open stuff, so I won't belabor it anymore. The one thing worth noting is over time PowerShell gallery will continue to become a more and more trusted distribution surface for many things, possibly and besides just PowerShell artifacts, but certainly for PowerShell artifacts both from inside of those which we produce as Microsoft, but also we're trying to find ways where if another company produced an artifact that you know it's supported and validated, it's actually from them, that kind of stuff. So we're trying to make sure that it is a trusted place or at the least you know which modules are highly trusted you can go download and use and we'll continue to grow that over time. It is something that Microsoft all up is sort of beginning to learn and get their head around. We are in many ways the PowerShell team has been a great place to work at, I kind of stepped back a little bit, and a horrible place. It's been great because we always get to do something new and crazy. It's been horrible because everybody looks around and says what are you doing and then we have to sort of fight our way free of the entanglements that come with the past. But that's another case where we kind of stood out there and we started stood it up and you've guys have been really supportive, that's really helped us get the message out and more teams are looking at it going hey we could use something like this, and so we're working with them. Some of that is artifacts might show up here, some might show up somewhere else, but thank you again for your help. But this should become a trusted place to go. Yep. And I think in the open source, so just to point out a couple of things. One of the things there we highlighted earlier was the concept of how we started using our platform to integrate into open source technologies like Chef. And I think that you're going to start seeing this trend should become more and more broad. I mean, the goal, I mean, the dream for some of us is imagine the day that no matter what language that you use to actually build your application. You can use PowerShell and DSC as the automation language, the utility language, if you will, and the configuration method for actually deploy your application in the cloud and manage it in the cloud. I mean that is the dream. That's what we want to get to. I mean, how we're going to do that is by doing this level of integration with a number of technologies that are available today in the open source community. And just a couple of more predictions here moving forward, one, now this is kind of a different one than the others. This is where we're actually just beginning. We're going to use a lot more telemetry and metrics of success across our team. How do I know we've had a good community engagement? I'm going to have some statistic that I try to get some sense on it. Now that's not the only way we'll do things and the only way we'll measure success, but we would love to engage with those of you who are crazy statisticians and have some ideas as to how we can measure your guy's success because one of the things I do struggle with is I don't just care about our success, okay I've got tons of downloads, but I'd also love to reach pass that a little bit and say, okay, how do I know that my community has been successful in deployment? How do I know that people are not stuck on something? How do I know that the corporation received great value from the PowerShell, their use of PowerShell or DSC? How do I measure the fact that DSC actually, in some cases, it's easy. Well DSC reduced my deployment and reconfiguration time from a week to three hours. Okay, that's a great measurement. Anything like that you can give us or any other ideas you have around that, please feed them in because it's really more of a request for information this one is as a prediction because we're going to do something here, it be great to have it actually be great. Micro-services. I think that this is a, you probably got a taste of that with OMS Azure automation with our DSC integration. In reality, that's a pull server that we have basically deployed into the cloud using Azure. Yeah I think one of the things that which they're seeing is more and more of an investment in our platform than this one should just deliver in the platform. We're going to start delivering services that you can actually use for doing things like, David communicated yesterday in a presentation, editor services. How can I plug in my favorite editor in order to do PowerShell? How can I basically do validation of my scripting into the cloud, so I can have it hosted validation services, maybe Pester is a service, think about it that way. Configuration. DSC is a platform, well what is configuration as a service? Things like pull server. Pull server's been a big one. We just had a whole discussion around pull server and it's a great little product, in fact, we put it out there first originally, more as a sample. We sort of backed into it a little bit. It's like hey we got this platform, we got this LCM, how do we make sure you can actually use it in pull mode. Oh, we'll write a little piece of code to make sure it works. Okay, well that's a good, we need to make it a bigger piece of code. Let's make sure we actually have a good sample out there. Oh, okay let's take the next step. Let's actually oh people might want to use that sample as a test case, and over time, we've grown this thing to actually be something that Azure put out there with the intent that people would actually write larger tools and more complete tools on top of the basic underlying protocol. What we discovered is everybody says oh I want a pull server and starts using it automatically. They exceeded our expectations. Our adoption and the speed of adoption certainly exceeded our original expectation, which is great and certainly we're stepping back and saying okay how do we fill that need in a way that will meet the overall business objectives, but also just meet the community's. And so we're thinking through how to do that in a way again that would be open and thoughtful and bring everybody in and let us create a great experience instead of one right now, which needs some improvement. We're going to keep working on it. If you're hitting roadblocks in the pull server, file those, tell us, and work with us in the future. I think it's really important too that the role of micro-services in the hybrid scenarios, it just kind of goes to the point that Erin made earlier, is we start thinking about heterogeneous, we start thinking gross cloud type of integration. It's really important having the sort of services that allows you to do both. You can be on-prem, but if you want to go to the cloud, it allows you to easily integrate with other types of cloud services. So I think this is a scenario, especially like deployment it would be a good example of that as well. Absolutely. And another place is where this does pivot us a little bit towards, as Angel was talking about the scenarios and solutions, we will pivot. We're a platform as you mentioned in that other meeting, we're a platform first and foremost and we want to make sure people succeed and we'll probably start picking a couple of vertical places of like deployment or something like that. Make sure okay you can really get the bits out there, they're really going to be successful and have an all up solution with the package manager, DSC, PowerShell all combined together. And I think another thing that's about a good reason why I think the name of the summit has changed. I mean, I think it was a year ago, Don and I, and a few others, Jeff and we were having this idea about how we evolve it, how the summit will evolve and I think Don came up it's like well you guys got to think about these DevOps then, right. We were like looking at each other like yeah I guess this is for real. It's really a culture shift in IT. And we realized that wow there is this amazing opportunity for PowerShell to become the language that basically ties things together and the concept of both the tool, if you will, chain of DevOps, as well as, as people start evolving and moving the solutions that you want agile deployment and operations of those solutions, PowerShell and DSC can play a significant role of that. So I think that moving into the future, this is a scenario that we're going to be investing more and more, and we're going to be working with our community of tying that to a chain all of us together. And this is kind of the area which PowerShell is sort of born to do, right. The reason we have PowerShell in the first place is not everything was a file, document-based management in Windows it was goo, right. You had stuff in registry, stuff in API, stuff in XML, stuff who knows where, and so PowerShell kind of originated as a glue language within Windows and has continued to grow into a bit of a glue language as Erin pointed out, even a little bit of the we do spread our arms a little bit across the shell and the developer scripting scenario and that's actually on purpose, right. And there's that intent to have always be able to make you be able to do things in PowerShell. It's not a single-purpose language on purpose, in this case, unlike other languages that I've participated in. So we think in fact it as before in the DevOps world become more and more of a glue logic, our intention is that it will in fact become known there and be able to do that. As a part of making sure that happens, in version 2, we have no major announcements. And in version 2, actually in version 1 of PowerShell when it first went out, we had no PowerShell on Windows. You had to just download it. You guys might not remember. I remember this, right. You remember. Okay, it was just a pain. You had to download it all. In version 2, we wrote on the board our mantra was before everything else, PowerShell everywhere so that people didn't have to do that, right. So our goal, in that case, drive was getting all skews of Windows. And you'll see as we move forward and try to become more of both the glue language and technologies, that we're going to try to make sure we're able to push PowerShell technology everywhere whether it be DSC or gallery or different components, we want to make sure that in fact that we're able to push things. And we said before the open source will go or go on as fast as we can with as much as we can within the realms of reasonable business and within the realms of our resources. And so, we're continuing to evolve that story, but our goal is to make sure that our tools are available for you. Yeah, I think as Kenneth say, well we want PowerShell everywhere, I mean, I think is the--- Technology everywhere. Yeah, we want technology everywhere, but we want it to be a tool for everybody and I think that this is something that as you start, you saw some of the predictions in the past and some of the predictions in the future, you're going to start seeing more and more of that. Stay tuned. Velocity. We haven't, we did start moving things a little quicker as you noticed. That will continue. We're going to try to move them quicker and with higher quality moving forward and with a little bit of communication all simultaneously. We'll try to get that stutter step working a little better. There is a drive towards simplicity. Now this might not be obvious. This is our last line. This might not be obvious, but we do actually try to make this so, that was the whole intent of DSC. If you take a look at it from the user perspective, if there's a good match and take a look at the PowerShell code, you get about a 10x factor of simplicity out of just a DSC configuration when it matches. Now what we've got to do with DSC is make it easier to get there and make that more complete coverage. Does that make sense? But the whole intent was reduce the challenge of complexity, so that we can achieve the scale we need to in this industry and the rapidity of release we have to. That's why we do DSC. And so we will take a look at both DSC and other technologies to try to make it simpler and simpler for the end user or in certain cases. We recognize we don't always land in the first place when you start looking at things like the pull server. And then last, but not least, there's _____ of completeness. We talked a little about the scenario issues. How do we actually make sure we are complete and well-rounded? That's where we're counting on you guys to do some UserVoice and feedback and to tell us what those issues are. So with that said. Yeah, I think, thank you. I think that the best way to finish is to thank all of you. I mean, we come to work every day at Microsoft because of you. I mean, really. PowerShell has been an amazing experiment. I've been with the team now; I think it's my fifth year now coming in as a different role. Kenneth has been here for 10 years, 13, 13 years. Thirteen years, something like that, maybe 14. And it's amazing to see where we are. I mean, for me from five years ago to today, I mean I'm looking in this room and I'm looking at the other people and I'm looking at the momentum and it could not happen without you, your help. So we need more help. We need more of you to basically contribute. You know to help is basically take PowerShell for another amazing 10 years, so thank you from my point of view. I don't know. Thank you very much.

Active Directory Forensics with PowerShell

Introduction

I want to tell you a story. So I go help customers. That's what I do. Some people call premier field engineers, they're like ninjas jumping out of helicopters. So it was a Friday evening and I should speak to the microphone over here. So it was a Friday evening and I had just logged off for the day, getting ready to have dinner with the family and I'd worked it out where I had two weeks at home to do some remote project work so I didn't have to be on the road. I was really looking forward to the next two weeks. Friday evening, I sit back in my recliner, I've got my comfy clothes on and my phone rings. We have somebody called the on-duty critsit manager, and a critsit is kind of like you guys are on call for your company, right, but we're on call for every company in the world. Alright, and I don't get those calls very often, but I got the call and the manager says we need you. Have you seen the news? I said no I haven't looked at the news. He said turn on your TV or look at the internet, whatever. And so I look at the news and I oh no. So I, he says we need you, this was Friday evening, he said we need you on site Monday morning with the customer. Now this customer will be unnamed. I'm not going to tell you what industry, what geography, anything about this customer, alright, because we do this on a regular basis. So I get to the customer site, I show up to the standard security check-in where they give you your badge and all that with the security people, right. They don't have any computer. Their computers are turned off. They're handwriting nametags and writing them on a clipboard. This is the security check-in at the front desk at the company. I get off the elevator and there's a sign posted for all the employees that says do not turn on your computer until further notice. And that week, the FBI was at the company briefing employees on identity theft protection and how to guard themselves online. It was post-apocalyptic. This IT environment was pretty much cratered by hackers and they call me to come in and say we need you to look at the Active Directory and tell us if it's okay to bring back online. When we realized that we were breached, we started shutting down domain controllers and we had two that were offshore in other countries where we've got the VM captured and we've taken the time of a couple days to transmit that VM over our high speed lines to get it in an isolated lab. We want you to look at this DC and tell us if you can find any fingerprints of the hackers in the Active Directory data so we know whether it's okay to reuse this domain environment because otherwise, we're building completely from green field the entire company. That's kind of a big deal. I wasn't ready for that. So Active Directory Forensics. Here's our problem. How do you track malicious activity in your domain environment when you didn't have the auditing turned on? How many of you have auditing properly enabled in your Active Directory environment? Right, properly, right. Alright, about two-thirds of you, so that's about, yeah, that's actually better than average. So what our problem to solve is how do you find the footprints of what we call an actor in your environment if they're tampering with your Active Directory data and you did not properly have auditing turned on. It's a big deal. What we're going to do is we're going to dive into just the single DC lab here and look at some things and teach you some concepts, some ways that you can work with your Active Directory data and try to find some of these things. So what we're going to imagine is that you create an account, you create a group, you put the user in the group, you nest that group in domain admin somewhere to try to hide that it's not an immediate member of domain admins, you grant that account control of an OU, then you delete the account, delete the group, kind of what we would assume to be hacker activity is now. The truth is, with targeted malware aimed at companies sponsored by nation states, they usually don't even need to create accounts because they're going to find somebody on the network that's stored their password in clear text and they'll just use their accounts. So sometimes we may not, and that was part of the case at this one particular customer. The hackers didn't really need to create any extra accounts because they had already found all the passwords in a spreadsheet in the network. I know you guys don't do that though, that's okay. Alright. Don't be that company. Don't be that company, alright.

New Objects, Get Protected Group Member Changes

So let's dive into the lab here and take a look. So first off, I've staged some changes in the environment I want us to look at and what I want to do is look at the Active Directory RIDs, Relative IDs, essentially. I want to look at in descending order what are the most recent new accounts that were created in the environment? Now the scary statistic that I've heard is that the hackers have been in your network 11 months before you're even aware that they're there. So this is all best effort really here, but what we're going to do is we're going to look at a list of accounts, newest accounts, from the moment we discover the breech, we're going to look at the newest accounts and determine do any of these look suspicious in any way. Alright. So I'm going to take a look here, Get-ADObject. Now the SearchBase is going to be here, the Path, the root of the domain, and I'm going to do a Subtree search Get-ADObject standard kind of deal here. And so I'm looking for all the accounts that have an object SID. Any object in Active Directory that has a SID, that's going to be a security identifier, which would be users, computers, and groups. We're going to pull back those types of objects and then I'm going to put some funny business calculation in here for the date of when it was created to get something that's recent. And let's see. I'm going to do ResultPageSize. We were just talking about paging here a second ago. One of the things I discovered working in this particular lab environment that they gave me was I'm working with a large data set, thousands of accounts, and the Active Directory cmdlets were choking and timing out in places, so I had to put a page size, and I had to kind of play with how big the page size was, and I found that 100 was about right for the chunks of data to process. And so I'm going to pull back from those objects the whenCreated, the objectSID, and some other things, isDeleted. And what we'll do then is we're going to actually take the objectSID, which is the domain SID- the RID, that last little bit on the SID and just parse out the RID just for information and it doesn't have to be sequential necessarily, but we're just going to take a look at these most recent accounts in the environment. And if I zoom in here with mark's tool, so here we've got the objects SID, the RID, when it was created, is it deleted, and this is just looking at accounts in the last few days or the last week. So is it deleted, what type of object is it, user or group, and you can see GoodGuy and GoodGuys. Those are the demo accounts that I've created for us to play with here because we're the good guys, right. So that's just a quick way to look and see what's the newest accounts. Not a big deal there, just a quick query to say alright looking at around the point that this event happened, we're there any new accounts that were created that look suspicious. Now, how many have heard of something called Admin SD holder? Alright, so if you're not aware, there's a process that runs on a domain controller, your PDC, every hour, and anybody that's a member of those specially protected groups like account operators, backup operators, domain admins, it's going to run a process to make sure that people in those groups haven't found some nefarious way to elevate their access and it resets their access on their accounts, so this is called the Admin SD holder. Anytime you're put into a group that's nested into one of those groups, there's a flag that gets set on your account that's called Admin account. So let's take a look here. So I've got a little script that's going to query for this property AdminCount. If it's set to 1, then I know that's a protected group member. So an account that's somehow nested into another group that's nested into a privileged group that's going to be reset automatically on that hour interval. So what I'm looking for then is I'm going to process through these protected groups, and then for each one of those groups, I'm going to get a member list, but I'm going to do this a really special way here with something called Get-ADReplicationAttributeMetadata. Boy, that cmdlet is a mouthful. This cmdlet never gets old. I've been demoing this cmdlet for years and it is your best friend when it comes to forensics in Active Directory. So before I get into the function here that I've created, let's just kind of step through this a little bit. So I'm just going to pick a domain controller and give me an hour's interval of 24/7 and I'm going to query Get-ADGroup where AdminCount is equal to 1, and then I'm going to look at my protected groups. So here's my protected group list, print operators, replicator, backup operators, and then notice this GroupLevel1, GroupLevel2, GroupLevel3. I put this in here on purpose to show you that no matter how deep you nest the groups, these are all just nested. Group 1 has group 2, has group 3 and then group 1 itself is actually nested in 1 of these privileged groups like domain admins, I believe, over here, so we can look at those protected groups. So now, let's take a look at something called Get-ADReplicationAttributeMetadata on the Domain Admins group. Now somebody was telling me earlier today that they had used this cmdlet, but they didn't use the ShowAllLinkedValues. This cmdlet is going to show you, Active Directory keeps track of what you do, and if you update any single attribute on any single object, there's a version stamp for every attribute on every object. So I can see how many times that one lady in the company that's been married five times and her last name attribute has changed every other month, okay. I can see there's been version that many times every time we had to go update the last name attribute, okay. So ShowAllLinkedValues though is especially for groups with that linked value replication I mentioned just a second ago, so it'll show me all the users that are members of those groups and when they were added to that group. So let's take a look at this particular example here. Piping everything to Out-GridView, my favorite cmdlet, right after Import-Csv. So here we go. It's showing me that the object category is a group, gives me each one of these things, there's the Sid, there's the adminCount, it's a special adminCount is 1, it's a special group, here's the description and so forth, it's the cn. But if I slide over here to the right, what I want you to notice, let me scroll across here, I have to scroll across my view and this is all tiny. That's the one thing I don't like about demoing Out-GridView, I have to scroll a lot. So last originating change time up here says this is the last time that attribute was ever changed on that object and if I scroll all the way to the right, I'll see a version stamp. And that version stamp says how many times 1 is the time when it was originally created as an object, when it was born in AD, and then any increment above 1 indicates that attribute has been changed, how many times, just subtract one from it. So if I come down through here I can see look there's some attributes that have been versioned significantly. So let's slide back over here and what I want you to see is that in this domain admins group, you can actually take a look at the members in the group. Now you can see here's a deleted user GoodGuy that I've been playing with a couple times setting up for the demo. Here's a migration account from the AD migration that you had to set up. Here's Bob. Somehow Bob made it into admins. And there's GroupLevel1 that has a GroupLevel2 and 3 nested inside of it, so those are members. And over here, we can see the date that they were added to domain admins. That's interesting. Then if I keep going across here, it'll show me which domain controller the update originated from. So the ADReplicationAttributeMetadata will show me the last change to every attribute on an object and which domain controller initiated it and the date and time. Does that sound handy? Yeah. Sounds pretty handy. As long as it was the last change that we're looking for there. There's also another property over here that will tell us if it was deleted. So if the user was ever a member, here's a last originating delete timestamp. If we look at the column title here, LastOriginatingDeleteTime and there's the value of 2/23 whoever that user was there was deleted out of domain admins on that date. So we can see additions and deletions using this cmdlet to view group memberships here. Now the one thing to mention is that this is linked value replication. If you've got the old school 2002, 2003 early era, the group memberships are in a blob in a member's property and it's replicated as a blob, then this doesn't apply necessarily. But with the linked value replication that we enabled in 2003 and above, you should see this, so all your new groups should be seeing this. And to comment on that, if you upgraded, do you delete the members and add them back to the… Right. Right. You've got to reset the group membership to make them LVR. So if I add a group member, actually, he's already a member of that group. Let's say we remove Bob from the group there and we can go look at it again and then we can see that Bob here, I bet this is the one I've already demoed earlier. Let's see, is he still in there. Yeah, he's already been deleted, so it would update his delete date. So what are we getting at here. What we can do then is we can say for each one of those members, we can go through here and take a look. So what this report then shows me is these are all of my privileged groups, Backup Operators, Domain Admins, Administrators, Enterprise Admins, all those special groups, and here's a list if I scroll to the right who are the members that have been added or removed from those groups within a date range. So I can filter on that property over here FirstOriginatingCreateTime is not what we're looking for. We're looking for LastOriginatingChangeTime, which tells me that this attribute, this user was added or removed on this date, so this is just PowerShell objects filtering, right. So I can just filter that now and run that once a day, once every six hours, put it in my monitoring tools, and filter on all my privileged groups, filter on that date, and I can get an email now around the clock anytime somebody's been added or removed from a privileged group. Does that sound like a good idea? How many of you get notifications on privileged group changes? Alright, for the rest of you that don't have your hand in the air, this is a free no-frills way to do that with just a couple lines of PowerShell. And this is all documented out on my blog, so the code is out there for you to get publicly. I'll give you that URL here in a minute. So what I did was I just took all that code and put it into a function and the function just has a domain controller parameter and how many hours in the history you want to look and it'll go grab all your protected members, ProtectedGroups, enumerate through all their members, and see which ones have been added or changed in the last X hours, so that's a handy way to go drill into that.

ACL Changes

Now where I want to spend more time is on ACL changes. Somebody has granted themselves access to something else in Active Directory that you weren't aware of. Probably ought to check on that too, shouldn't we? So by default, in the schema somewhere, there are default permissions defined for all the objects. That'd be kind of painful to go actually delta all those to see what's because over the life of the directory it's going to be really messy to figure that out, but I want to show you a way to look at this. So using that same thing Get-ADReplicationAttributeMetadata, how many have used Get-ACL? Alright, yeah, you're working with, looking at the permissions of an object, did you know that Get-ACL actually works against Active Directory objects? When you get that AD drive letter, you can do a Get-ACL to see what are the permissions on an AD object. So what we can do then, the ACL on an AD object is stored in a property called nTSecurityDescriptor. So think about what we just looked at with the group changes, now we can filter the metadata of every object in the Active Directory to see which ones have had an ACL change in the last 24 hours or the last week. Sound interesting? Yeah. So now we can actually go look and see who has been delegating access in the environment recently and get a report of that. So what we're going to do then is we're going to get a list. Now you need to be careful here. Grabbing a list of every object in the directory, that's going to be, yeah, you'll be there a while, so instead, what I'm just going to do is I'm just going to filter on OUs. We'll start there. So I'm just going to get a list of OUs, and for each OU, I'm going to grab the replication metadata of that OU to see if there have been any changes to the nTSecurityDescriptor and then I'm going to sort that in descending order. And so when I look into this data, I see that the HR OU, let's zoom in there, so the HR OU on April 1st of all days had a permission change. Now the rest of these OUs, this is my lab, so it's been a long time since anybody's touched these. That was April 1st of 2014. Wow. Yeah, that's been a while. Alright, so let's make a change then. Oh, I'm sorry. That's whenCreated. I apologize. Let's look at the proper attribute here. Let's scroll across to the LastOriginatingChange. That's actually the date we want. There we go, that's the one. That's 4/6/2016. So right there I can see the LastOriginatingChange on that ACL was the 6th. That was this week. Somebody changed access to the HR OU. So what do you think we ought to do? Let's go look at the access for the HR OU, but there's a problem with that. Let's see what the problem is. What I can do then is say, alright, here's all of the ACEs that have been updated in the last amount of time, let's say within the last 90 days here. So then what we will do is we'll look at here's the HR OU in CohoVineyard. Alright, so let's get on down to see the actual permissions now. So what we can do is I'm going to do a quick little dosey-doe to query the property names for the permissions out of the schema so that we're not looking at GUIDs, that makes it easier to see the permissions, and we're going to look at objects with change permissions. And so here's our list and actually let me skip down one more section, here we go. So permissions that were changed. So what I'm going to do now is going to say okay, here's a list of all the objects, and why did that give me nothing? Oh, okay, I just grabbed the data, now list Out-GridView. There we go. So, here we go. Here's the HR OU in the top of the list and I see the change originated on the CVDCR2 and if I scroll across, let me zoom in here, I want you to see what we're looking at. I know this is a lot of data, but everything we want is right here. So let's take a look. I'm going to zoom in and I see these are the named permissions that are applied to that OU and it will tell me what's inherited, what's not inherited, GenericRead, Write, for User, Computers, and groups, things like that in the OU and as I scroll across, I will see here are the account names that are granted access to that and there's GoodGuy. But here's the problem, every one of these are currently on that ACL. We don't know which one was changed. We just know that this is the current ACL. We don't know which piece of the ACL was changed, which ACE individually was added or removed. So I'm going to cheat and show you another way we can look at that. This is the piece that I woke up about 6 am after a full night of conference sleep and I thought I need to do something else to really take this to the next level. So two years ago here I did a presentation on Active Directory snapshots. Some of you are nodding your head, most of you probably never heard of Active Directory Snapshots. It was a feature we introduced in server 2008 and you think, wait a minute, snapshots, Active Directory, I thought we're never supposed to snapshot Active Directory right. Well that's VM snapshots. This is the actual database. There's a feature in Ntdsutil where you can do a volume shadow copy of just the Active Directory database and logs, and keep that, and then mount it into memory on a different port, and query historical AD values. Pretty cool, huh? So now we've got this prior state of Active Directory in the current state of Active Directory. Well somebody I know happened to write a module to do all that for you, so you don't have to type all those long crazy commands, alright. So out on the blog, it's called Oh Snap! Active Directory attribute recovery, yes, Oh Snap! So what I've got is a function library here with my snapshots, and in this functional library, I have some functions here like let's see we've got a--- So an answer. Yes. These guys never bothered to audit something, but they did do an AP snapshot? Okay, alright. I'm glad you asked the question. I just want to make sure it's (inaudible). No, they never did a snapshot. That's oh snap. Exactly. Right. They didn't do the snapshot. That would have been amazing. It would have been amazing. It would have made this very easy. There would have been five people who were in AP snapshots. Right, right. So my part of my role here is to tell you to go start using snapshots in your environment because you can do, you don't have to buy some third-party popular product to do attribute level recovery. You can do it for free with this functional library on my blog. Alright. So you can do free attribute level restores copying the data back out of a good copy of AD when somebody blew all the email addresses out. I know some of you have been there. Alright. So I've got a snapshot here that I took in my environment. Here's the last snapshot I just did today. And so what I'm going to do is I'm going to mount this snapshot. So yes, they didn't have a snapshot there, but the moral of the story is go make snapshots and schedule them. It's all documented on my blog. So what's happening here is we're actually mounting a utility dsamain.exe, I'm giving it a port 33389 that nobody's using right now, and now when I look at my snapshots, I'll see that it's mounted this exposed name here into this $SNAP folder, so it's actually temporarily restored a copy of an older Active Directory database and logs. So I've got that mounted on a separate port and now I can query the live AD and the history AD and compare the results. And I have created some functions here for you that you can query an object and give it a list of attributes and it'll rewrite those attributes back into production copy. Alright, so free recovery. Yeah. This might be kind of hard to answer, but I'm just curious like what's the general size of something like that? I mean, is it huge or is it relatively easy to keep that kind of stuff? I mean, small? Yeah, it's pretty small. But you do want, in my article I give you some guidance around keeping that cleaned up. You don't want it to get out of hand. Just determine how many you want to keep. So today I wrote a new function in this module called Compare-ADObjectACL and I pass it in a OU path because HR was the OU where the attribute or the permission change happened, and so I want to compare that OU ACL between what's historical and what's right now. So when I run this, it dumps out a list of permissions. Here's a permission that was removed. Oh, look at this, somebody removed HRAdmins from the HR OU and they, I guess that's it. It looks like they just removed the HRAdmins was the change that they made. I could probably update my snapshot to see the other changes as well. But anyway the only challenge here is that I have to manage, I haven't built the logic in yet, but you have to manage the date that the snapshot was taken against the date that the change happened. Make sure you get those date windows set correctly, so you can go view the actual changes that were there. So that will allow you then to take a snapshot and view what was in place before somebody made the change. So I can go, it's like a time machine to look at my directory and see what was there before Joe Jr. Admin came in and fixed everything. You've had Joe Jr. Admin come in and fix things in your directory, right? So you've got to go refix them. So now that I've done, I've looked in there, I've found that I'm just going to dismount that database, mount it in memory, and this little LDP icon down at the bottom will go away, and then that is now unmounted, that snapshot is gone. So that's the way that you can do it, but it does require you to be prepared and take snapshots on a scheduled basis in your environment. What about deleted users? Let's go ahead and delete these two guys if they're, yeah. And let's go take a look then, let me skip through here ACL_Changes, Recycle_Bin, here we go.

Recycle Bin

So back in the day before the recycle bin, you delete an object, it gets tombstoned, and then there was Mark Savage's tool that you could put the object back and it looks really beat up when it comes back, but it's just got the SID enough to really preserve the identity, so you don't have to go reset all the permissions, but it loses everything else essentially. Now we've got the recycle bin. We don't have that concern because now we have the recycle bin. Depending on what interval of your tombstone lifetime was set when you implemented it determines how many, usually it's modern system it's 180 days, you've got the recycle bin, then after that, you've got a tombstone for another 180 days, so potentially a year theoretically to get an object back. So the problem is when you query in Active Directory in the deleted objects container, so if I look at the Get-ADDomain, there's a property on Get-ADDomain that tells me where the deleted objects container is. I find that exact property here. Deleted objects here and CohoVineyard is my sample domain today. So I'm going to pick that deleted objects container. So the other thing I was thinking was hey what if the hackers deleted some accounts or deleted some data in Active Directory that I want to go inspect. So let's take a look. I'm going to look at that deleted objects container here and I'm going to set that as the search base for my query. That's where I'm going to start my query, but when you're querying deleted objects, you have to include this parameter IncludeDeletedObjects because normally those are hidden by default. So I'm going to view those deleted objects and grab those under a variable called DeletedObjects and then I'm going to sort them by modified, but the problem I had here and let's see, there we go, and so you get the Dell thing in the name and all that. There's a last known parent attribute as well in here that gets populated when they're deleted. The problem is that basically there are some date fields in here, but there's no good date field to tell me when it was deleted, when or where that delete originated. So what I want to do then is look at the metadata just like we've looked at everything else, now I can see which domain controller initiated that delete putting it in the recycle bin and then go look at the logs on that DC for that time period to see who was making the change. Alright, so that's the way we track it back. We use the replication metadata to find which DC originated change, then we go look at the security log on that DC to see where it was. Now most likely these security logs are rolling pretty frequently, so you would have to have some type of event log collection forwarding something going on in your environment to be able to search those historically. So what I could do here would be to run through and let's take a look at with the metadata here, put that in the GridView. And so now I've got deleted objects, whenCreated, and whenDeleted right here. Alright, so that shows me when the object was deleted and what that's pulling back is it's a calculated value based on that metadata for the change date on the object. It says this is the last change was putting it in the recycle bin, so I know that's the date it was deleted. And so I can now see a dated list of what things were deleted in descending order from today and then go look at the event window, those date ranges that are suspect and see were there things that were wholesale deleted by the hackers in that window. Anecdotally, while I was surfing through this company's directory data, I found an actual account with the name Hacker. It turns out there's a user in Europe with the last name of Hacker. So they were legit.

Get User Membership History

Now this script I've actually---it was PowerShell Saturday in Charlotte several years ago, Mike Robbins, I think, came up to me after the event and I'd done some talk about Active Directory and he says man I've got this problem where this guy has been in the company for 20 years and he's been in every department, he's been added to every group and I don't know which groups he needs to, he's basically a domain admin at this point. He's got access to everybody's data because we never took him out of all those old groups. Everybody can identify with that situation? So how can I find that group history mystery? How can I figure that out? So I wrote this little thing with this AttributeMetadata cmdlet and what it does is it's going to show me for this user, let's go look at every group they're a member of and find the date that they were added to that group. So here we see for this user he was added to manufacturing in 2013, accounting in 2013, HR in 2013, 2011 he was added to legal and some other test group in there. Obviously this is just a tiny little lab environment, but in your real world you'll have 40 groups in there and you'll be able to see using this data, then going back forensically to see when they were added to every one of these groups they're a member of. Is that handy? Alright, so that's called the group history mystery. Yeah. I've run into something with this before where, I can't remember why it was limited to last 1,000 changes that had been made on a group. Have you run into something like that? First I've heard of it. Okay, I need to research it further. Okay. Deleted metadata will disappear after a while if the license was… Right, right. So the deleted metadata will disappear. These are the ones that are still active members of the groups. Yeah. Yeah, what I also ran into was a group that I have that has almost everybody in the company in it and when I looked up the metadata for it, I can only find the last 1,000 changes or less. Oh. That was an issue too. That's probably going to be a limitation on the metadata attribute field, how much it can hold I'm going to guess for a group of that size. Yeah. So its binary data. Actually Brandon Shell, BS on POSH, he wrote before this cmdlet existed, he wrote something that would actually, it was a very manly function that would go parse all the binary data in that attribute and give you the data, but now we have a cmdlet to do it. So yeah, it's a lot easier now. Does this require having admin rights for some of these to reset these? Probably so because if you're looking at delegated administration in some of these places, you're going to have to have domain admins because I'm thinking that every user has read to just about anything in the directory, but there are going to be places where they don't have it, so yeah, you probably should use an elevated account to do this. Yeah. So a smart malicious act is going to do as little as possible, as little destruction to the structure as possible. Right. So the holy grail is like finding these, the holy grail would be like finding a service account, Kerberos it could hash, or even the Kerberos to generate the account. In other words, you could generate a KRBG, whatever KRBT, that hash. Can we use these cmdlets to dig in and try to see those tickets or who's generating them? Is there an account that's generating more tickets than they should? You could look at the KRBTGT and look at the attributes on it and see if those, if the version stamp has been incremented for that like the hash and things like that on the password attributes. Now which version is it. When you do the prep on the environment, in one of the recent versions of AD, it actually changes that password, so don't be surprised if you see a change in there from the original value it's supposed to happen. Do you have to change it twice to change the password? I don't remember. But yeah, I haven't looked at it in detail, but it's in there. So we've got just a couple minutes. Let me finish up here and then we will take some more questions. So last up is a GP link metadata. I'm not going to take the time to display it, but basically part of the clues that we got from one of the other security vendors that were on site with this customer was that they thought there was some suspicious Group Policy activity during the event window and I wanted to see what group policy changes had happened in the environment. So now imagine I've got an OU object, that OU object has a GP link attribute. The GP link attribute is a string that holds the GUID of every policy that's linked to it. So now I can just go through all the OUs, look at the metadata, and see which GP links historically have been versioned recently telling me that one of the policies linked to it has recently changed. Again, I can't see specifically which one. So I found some policy activity on an OU and I could see though that it looked like normal behavior because there was a test OU where they had developed the policy and I could see that it had been unlinked from that OU and linked to a production OU and they went back and correlated it with their change logs and it was a scheduled change, it was normal, so I could not corroborate the story of the vendor saying hey there's some suspicious GPO activity because I looked at it all and it looked fine. So it can even take that, and there's a whole blog post on just that piece as well, if you want that.

Resources and Conclusion

So let me wrap here. So where can I find these scripts? Go out to aka.ms/ADForensics and that is a link to the blog feed that has all of my posts that are tagged with forensics-related topics. So you can go check all of those aka.ms/ADForensics. Also, in my Microsoft Virtual Academy Video series, I have eight video segments on MVA about doing Active Directory with PowerShell. The fourth segment in that is all about forensics where I talk about some of these same topics, but I don't, I didn't go into the same level of detail that I did today, but it's all there for you, the ReplicationAttributeMetadata and things like that. Also, Microsoft is doing a lot in the area of cyber security. I've been on a few of these covert dispatches myself where they don't even tell me the name of the customer. I showed up, they said here's the address, I'll meet you there. It was like I was volunteering to be abducted or something and I'm going to go in this secret door and it's a need---it's like cloak and dagger stuff. They don't even---in our systems we don't even track the name of the customer. We use a fake name all this because it's very confidential when we're dealing with cyber security investigations. So we have a full team of PFEs and MCS consultants that specialize in cyber engagements. If you're company has been hacked, if you get that call, if you become aware of it, please call us because some of the kneejerk reactions that you take as soon as you find out you've been hacked, the things that you think are the best thing to do, are not. And we have trained experts that will help you investigate and remediate your situation, so please contact us. We also, there are a lot of vendors in this space and I'm not trying to just say that we're the best, right, but we are because we have special tools in the world that nobody else has because we're Microsoft. We have lots of special data that they simply don't have. Please call us if you have a cyber incident. Satya Nadella at the end of last year did a special press conference with some people in DC about cyber security. The things that are really hard to track is when you're looking at millions of event log entries across domain controllers for the last year, how do I know what's normal activity and what's malicious activity. We're using machine learning to study event logs and identify abnormal activity in your environment. So we're doing a lot of research in this space. We have a lot of experts to help you with your cyber security needs. And so I mentioned at the beginning that you really need to make sure that you have your auditing set up correctly. Here are some blog links particularly to the Ask PFE Platforms blog that have some expert advice from some of my peers on how to properly audit your Active Directory environment so that you don't have to worry about these other methods to try to find the data when it's too late. So, closing then. Don't be that company. Don't put clear text passwords in a spreadsheet. It happens. These companies end up in the news. Audit your Active Directory environment the right way. Set up alerts. Take the sample code, even if, most people are always complaining about IT budgets, this is PowerShell. Do it for free, use these scripts, and set up alerts in your environment. Learn this ReplicationAttributeMetadata cmdlet. It's going to be your best friend. Cyber security is a huge topic. This is just a very small part. There's so much more we can do with cyber forensics in Active Directory. This is just to get you started there. So check out the new machine learning stuff I mentioned from Microsoft. And if your company gets hacked, please call us. And I don't say that to put other vendors in a bad light. It's just that we have experts that this is all they do and they live on site with customers for months helping them recover from these terrible scenarios. And finally, if this has helped you, please send me a tweet just so my boss knows that I'm actually not golfing, but I'm helping people in the real world. Thank you. Alright.

Building Modules Using Metaprogramming

Introduction to Metaprogramming

So my name's Rohn Edwards, and today I'm going to talk about Building Modules using Metaprogramming. So what do I mean when we're I say we're going to be talking about building modules with metaprogramming? Actually, I don't quite know what that means, but it sounded like a really good word, but. Now metaprogramming if you look up the definition, I think it's the writing of computer programs that have the ability to treat other programs as their data. It's also known as programs that write other programs, right. So what I'm mostly going to focus on is kind of a method of making modules that I started using to improve some old commands that I had where there was a lot of code reuse. We're going to, to get the concept, we'll talk conceptually what I'm talking about in just a second, but before we actually get started talking about that, I want to demo what I'm talking about. It's kind of hard to wrap your head around, so we will start with a very quick demo. So if I look, if I Get-Module, I have two modules here called, one called the AdventureWorksReader and one called the NorthwindReader. And so these modules have, they just have three commands. The first one has one command that's exported Get-AwDemographics, the other one is NwCustomer and Get-NwEmployee. So the Northwind and AdventureWorks databases obviously are just like database people use when they're learning how to read SQL statements or create SQL statements to read out of a database. But if we take a look at the syntax of, let's look at the AwDemographics one. So you can see we have some parameters here. It looks like first name/last name and if we take a look at NwCustomer, so we have different parameters from there. This will be important in just a second. We can take a look at the help on Get-NwCustomer and if you take a look, it actually has a synopsis and a description. They don't mean anything to you. I put it in just to kind of demo that we're getting help out of these commands and again this will make sense in just a minute. We have some parameters that have help. There were a lot of parameters. I didn't want to put help on all of them, but you can see CompanyName and ContactName have help associated with them. So if we go to use these, the purpose of these commands is to generate dynamic SQL statements to read out of the database, so we can do something like for the AwDemographics, FirstName starts with an a. Let's just select the first 5, I guess. So we're pulling data out of the AdventureWorks database. I don't know if it's actually anything that's completely useful, but we can filter on multiple things, things like let's see Education, I think high school is something that you can filter on there. We can come---you know there's---I don't know if you noticed there was a GroupBy and an OrderBy common parameter between the two modules and the functions we were looking at. So we can---you notice we have some IntelliSense here and we can do a GroupBy, let's GroupBy the Education and Gender maybe. So you can see that behind the scenes it's generating this dynamic SQL and that's not a big deal. There's a log of modules that can do this kind of stuff. What is, I think, kind of neat is if we look at the way these modules were actually built. So here's the NorthwindReader and you can see that it's calling another script, but here's how these commands are actually defined. So this first one's about 50 lines it looks like. So you come in, you can look that that's where that help we were looking at is coming from, but all this has is a param block. Actually, let's do this. The only thing that this, and you notice instead of function, we have this keyword, I guess, I put that in air quotes called DbReaderCommand, this little domain-specific language, but we give this keyword and we give the function name and then you'll also see these fake attributes. Right now they just say magic in front of them so that I can tell that they're fake, but that combined with a real param block with help mixed in with those parameters and that's it. That param block is enough to build that command that we saw, oh and we didn't actually, I did not demo Get-NwEmployee. So if you look here, we actually have formatting in there. If you take a look at, send that to format list, you'll see more is coming across than what we get by default. So let's go back and look at the Get-NwEmployee. You'll see that mixed in with these fake attributes, you have this FormatTableColumn. So putting one of those in is enough to tell it that we're going to want this in a view and we want that and behind the scenes it's generating some formatting ps1 XML files and adding all that. So this isn't, I'm not really so much trying to demonstrate these modules. It's more the concept behind why what we're going to kind of cover is useful. Alright, so why would you want to do that? Hopefully, after demoing that a little bit, it starts to kind of become a little bit clear. Right, so what those commands were doing, let me start with the precursor to this module. Years and years ago, about four years ago, I started making commands to look into the SCCM database at work. SCCM has an amazing amount of information and so I started out, I think the first command I made was like Get-SCCM computer so that I could look at relevant information on a computer. I could search by a name, by an IP address, by a specific model number. I could look for computers that hadn't checked in a certain amount of time. And so, I spent a lot of time, I made a nice param block and inside the function I made logic that I could map all that stuff together. I spent a good amount of time and I made what I thought was a very useful command. And then I wanted more information out of there. So I came up with another SQL statement that I thought was very useful. I chopped it up. I copied the original command, generated a new command, based off of that, I went in and changed everything that I needed to do. Within 10 minutes, I was able to have another very useful command. Fast forward 10 months, a year, and I have 20 commands that I've built up, but over time, each one of those commands are a little bit different and I added more functionality to some of them. As you saw there, we had the ability to do a GroupBy and an OrderBy. So maybe the first command I didn't think of that and I didn't think of the ability to do a negation, to be able to negate a specific parameter that came in. But 20 commands in, maybe I have added that functionality. And now I have 19 commands that I have to kind of go back and port that functionality into and I don't have time to do that kind of thing. So I had on my to-do list for a very long time a way, I needed to come up with a way to make it, that I didn't have to go back and do that. I wanted to make kind of an engine so that I could use the same exact code for each module or for each function and have it, by doing that, I was able to, if I came across a bug, I could fix it in one place, and then reimport the module, and all the bugs were fixed. If I wanted to add functionality, I could fix it in one place and all the commands could come along for the ride. And the only work that's left after you do that is to come up with that the command definition that we had, which as you can see there again, that was just essentially a param block. That was very, very, very easy I guess you call it a domain-specific language. So anyway, it takes code reuse kind of to the extreme and instead of having commands that generate static code that I can then save, I can just edit things on the fly and then import them. So it's definitely, it's probably really only useful in certain scenarios. I have a couple of code examples. We're not going to kind of cover how all that works because honestly and truly, the behind the scenes of that is pretty ugly right now. It's really big and clunky, but I do have a couple of examples I think that could be useful. They're very, and I think they're simple enough that we can kind of go through how to build the first one so because I'm sure somebody else can kind of take these same techniques and come up with something better than, you know I can.

Creating Multiple Commands from the Same Script Block

The very first thing we're going to do is we're going to make a little module that will allow us to just build dynamic commands that don't do much of anything, but we will go over how to, that concept of building a reference script block and then having all the other command and having that reference script block be smart enough to know to do something different. I mean, obviously if you make two commands that have the exact same definition, it sounds like they're going to do the same thing, and so why is that useful? So our first module we're going to make, we're going to set this up, let's see, can everybody see that? I'll zoom in a little bit. Alright, so first thing you're going to see, we're not going to actually create modules that are on disk, but this is going to behave exactly. If you were to write all this stuff that's inside these braces to a file and then call Import-Module, it does the exact same thing. There's nothing special about this. This is just an in-memory dynamic module. But again, if you were to take this, write it to a file, and call Import-Module on it, it's going to do the exact same thing. This is just so I don't have to switch between files. Alright, so our first one you'll see that inside this module we're going to define our reference command and this is going to be the thing that we'll go in and tweak as we want to add more functionality. And our, this is kind of our Hello World example. All this is going to do is call Write-Host and it's going to say I'm the whatever the command name is command. Useless function, but the more important part here is how we're going to spin these commands up. So we're going to create a little, like a little miniature domain-specific language. We're going to have this function called DynCommand. It will act as a keyword, right. So you see that it will take a CommandName in and it's going to take a scope right now. This is more to kind of, I'm going to show you one of the pitfalls to doing this. At first, when you create this, you're probably not going to want to play with the scope on this, but all DynCommand is going to do is take the CommandName and then it's going to take that ReferenceCommand, it's going to create a new function and it's going to use whatever scope you have there. By default, is going to use the script scope and that's honestly and truly the one you do want to use, but again, I'm going to show you one of the early pitfalls I came across. And then it's going to call Export-ModuleMember. So this function, well let's import this real quick, and we have DynamicModule, so we should be able to call, whoop, no I guess we can't do that. Get-Command -Module. At some point IntelliSense will kick in. Yeah, so you can see it has one command that's exported right now that's called DynCommand. So if we call it and try to create a command called Command1, let's do this, so if you take a look at the exported commands, you can see Command1 is listed and then you have, we still have remember DynCommand that was also exported, but there is a problem. If we call Get-Command on command1, you'll see that it can't find it. So if we create a command and put it in the global scope, again, you're not going to want to do this, but and I'll explain what's going on here in just a second, and then take a look at the exported commands, you'll see that this Command2 isn't listed in the exported commands from this module, but if you call Get-Command, you do see it there, right. Now Command1, remember we don't have it here, but if we were to actually call inside that dynamic module scope, it is in there. It did truly create a function, but it was in that script scope. So what's happening here is when the module is imported and when we're calling new module there, it's the same steps that are occurring. When the module is imported, there's some special logic that it comes through and realizes hey, I have this list of commands that need to be exported and it's smart enough to realize whether or not it's a nested module and those need to be hidden from global scope or whether or not it's a, those functions are supposed to truly be exported into global scope and it handles that for you. If you try to go in like we did a second ago and do this, you'll see that it wasn't listed as an exported function from the module and it is truly in the global scope. If you tried to call this as a nested module, it would come out into the global scope and you probably don't want that. So the whole point of that exercise there was to show that we probably, you'll probably want to keep that dynamic command keyword inside the module scope and not expose it and as long as you call it inside the module, you're fine. So that's why in the database examples that I was showing you saw that inside the module I was calling the commands there. It's not if you, this doesn't really work very well for allowing someone to at the command line create something, and to be honest, I don't know that there's much reason to do that. So we'll look at this and this is exactly the same. We had the exact same reference command. The only difference between this module and the previous one is that we call the DynCommand keyword inside the module. So we'll import this module. So we'll import this module in. If we take a look at what's exported, instead of Command2, I created one called AnotherCommand, but you'll see that those are both exported, so that's good. We can call Get-Command on it. That works. If we actually call these commands, that works too. But again, this doesn't do anything useful. They do technically do something different, but they're not doing anything that is useful. And as a matter of fact, they're still very similar. They have the same command syntax, which is they just take common parameters. I would show you the other command, but it's going to look just like that. There's no point. So the next step on here and then after this, I promise we will do something that has potential use. The next thing I want to do is show how you can make the ReferenceCommand scriptblock smart enough to actually present itself as a command with different parameters and how you can make it actually do something so that the commands can differentiate themselves. So if you look here, we have our ReferenceCommand again and the difference here, so we still write to the screen and say which command we're coming from, we're also going to output the BoundParameters. So other than that, the process block is exactly the same. The big difference is, oh and I did skip over one thing. One thing that's different between this module and the previous one is that we're going to create a hash table that will live just in module scope and this is going to contain whatever command metadata that we want to have and the reference scriptblock is going to be smart enough to go look into that and figure out how it needs to behave differently. So the easiest way to get the syntax to be different and to have different parameters is to just use dynamic parameters. It's not the only way that those database readers weren't using dynamic parameters, but that's going to be the easiest way to do it. And all this is doing is this is looking into that CommandInfo hash table up here and it's just depending on a dynamic parameter dictionary to already be present and to be returned. So we will make a change. Now we're going to actually change our dynamic command function here, so we'll still take a CommandName, but we'll also take a scriptblock that's going to be the definition and all we're going to do is end up invoking this, but we need a way to, when we create the command, we need a way to tell it to added parameters to it. So inside here, we're going to create just another function and this way you can't accidentally call this parameter keyword outside the scope of the dynamic command. But all this thing is going to do is it will take a ParameterType, a ParameterName, and a set of parameter attributes. And then it's going to do what the work you normally you have happen inside a DynamicParam block and create a dynamic parameter and it will put it into this same CommandInfo hash table that was defined above. It's going to add the dynamic parameter into the dictionary up there and in the process block of the dynamic command keyword, this is new, so this is where when it first starts to execute, it's going to create an empty runtime define parameter dictionary, then it's going to execute the code inside the scriptblock, which is where the parameters will be defined. Alright, so now if we have our Command1 and Command2, what we're doing here is telling it we're going to create a new command, we're going to call it Command1, and Command1 is going to have a single parameter, it's going to be a string, and we'll name it Parameter1. Dynamic command. Then we'll make another dynamic command. We'll call it Command2 and it's going to have 3 parameters. The first one is going to be an integer. It's going to be called Id. It will have two attributes. One is a parameter attribute. We're going to make it mandatory and put it inside of a ParameterSet and the other one is a ValidateSet. Then we will have a name parameter that's a string. It's going to be in a different ParameterSet, and this is just to kind of show you can give it any attributes you'd like, and then we're going to create just a generic object parameter that's going to be an object of, it'll take objects and it'll be named object. I'm simple. Oops. So we'll read that in and if you take a look at the syntax, you will see in just a second that Command1 and Command2 do indeed have different syntaxes. Command1 has our first parameter, just the single parameter, well, hold on, along with the other common parameters, but you can feed something to it, and Command2 has 2 and you can see up there where it does have 2 ParameterSets, so the syntax is showing correctly. And again, this doesn't do anything, let's do Id because it had a ValidateSet, but shows that we were able to spin up two commands that do completely or that have different signatures, I guess. So let's take that and actually try to apply it to something that could be potentially of use.

CIM Commands

So yesterday, I saw Jeff's session about building, coming up with code to help you build other commands and he had a really, really neat one where he took, he had a little GUI that you could create a command based on you give it a CIM class and you could go in and pick the different properties that you want and it would spin up a command for you that would just be easier to use than Get-CimInstance and with the CIM class and telling it all the properties that you wanted. So what we're going to do and this should look very familiar, so it's going to be almost exactly like the previous one, except the reference scriptblock will actually be, well first let's look at, it might be easier if we take a look at how we're going to define the command, so this will give you an idea of what the domain-specific language will look like. So we're going to create a keyword. Instead of DynCommand, we're going to have CimCommand and CimCommand will take a name and that's going to be the name of the function that we will create. These probably aren't very good because it kind of looks like Get-CimInstance and all that, so you would probably want to rename these. But we will allow inside the domain-specific language, you can give it 0 or more CimParameters. So when you don't define any CimParameters like down here, it's going to actually use Get-CimClass to figure out what all the parameters are and it'll allow all that stuff for you or it'll do all that work for you. But if you do give it parameters, Get-CimService should only have two parameters that it will allow you to filter on and two properties that come back on the objects that are output. But you also tell it which CimClass you want, so the first two are going to use Win32_Service and the last one is going to use Win32_Process. So that's what the domain-specific language is going to look like, so we'll very, very briefly talk about what the---implementing the domain-specific language again looks very, very similar. We make a CimCommand function that's going to act like a keyword. We have a string and a scriptblock. When you look down in the process block, you'll see where just like with the previous command, we take, we create a new parameter dictionary and then we invoke the definition and the only things that or really would be valid inside the definition, well besides other valid PowerShell commands, but the other domain-specific language constructs that would be valid inside there would be the CimClass and the CimParameter keywords. CimParameter is going to do just like the previous parameter from last time would do and CimClass is just going to take it and it'll optionally take a namespace, but this will take that CommandInfo and add the CimClass and CimNamespace information there. So after we've invoked that definition, we do a little bit of error checking. You need to make sure that a CimClass was specified. If not, write an error and the command just won't get generated. If it was, go ahead and, this is kind of, this has two purposes, we're going to Get-CimClass on and we're going to check against that namespace and that actual class. We'll make sure it's valid. If it's not, it's going to give us an error. But we'll use this CimClassInfo just in case the user didn't give any parameters. So if there are no parameters, then we'll go through and call the CimParameter keyword here for each parameter that Get-CimClass is aware of. And then of course, we define the function and export the module member, so that leaves explaining what our ReferenceCommand does and this is where all the work is going to happen whenever you use this method. So now, you notice our DynamicParam block is still really simple. All that work was done when we built the command. But now if you look inside the process block, what this is going to do, I'm not going to go line by line through it, but essentially what this does, we're going to create a hash table that will be splatted to Get-CimInstance and we're going to take a look, I forgot to mention that the reference command now has a parameter that we're defining, it will be on every one of the commands generated from this called ComputerName. This isn't a very good parameter to use because there probably are some CimClasses that have a property called ComputerName, so you would, so I'm seeing, so maybe that's not a bad name to use. (Audience comment) What's that? We'll likely use the name. Okay, so but that is something to think about. You may want to confirm that it's not going to be in a case where you're allowing almost any kind of property, this could cause some kind of a conflict potentially. But this is proof of concept code, so we'll let it ride. But you can provide a ComputerName, so every one of these commands that get generated will have that ComputerName. Think of it as a common parameter. And so, we create a hash table and we take a look and then at that point, we also get the information out of the CommandInfo hash table that's living in module scope, that should have a reference to every one of the dynamic commands we created. And again, we use the key for that is the current command's name. So we get that CimClass and let's see, then we go through each one of the parameters and we know that everybody has a ComputerName, so if the parameter we're looking at is ComputerName, then we had ComputerName to those Get-CimInstance params. We're going to end up splatting that hash table back to Get-CimInstance. So if there's no ComputerName specified, it'll just run on the local system. Then we go through every other parameter and we're going to just build a dynamic WQL statement. And so, we have a little bit of logic there that checks to see the type of the parameter, if it's a string, this is something you would definitely want to work on more. It's not going to be very resilient, but right now it just looks, if it's a string, then it's going to quote it and it's going to change the operator to like and then it'll end up, and this is to build the WHERE clause, so if the user doesn't provide any parameters, then it's just going to call it without any WHERE clause, there would be no where conditions. So then down here is where it actually takes, it looks at all of the parameters that are valid for this thing that are all of the dynamic parameters and then it ends up just building a select statement and from the CimClass and it does not, so yeah I just now realized this, this isn't taking the namespace into account. So yeah, don't do anything outside of Root CIM v2 right now. But then it takes our CimInstance param hash table that we defined earlier, it adds this to the query parameter and then it just splats that to Get-CimInstance. So let's read this in and see if it works. So if we Get-Module or Get-Command -Module $DynamicModule, you can see that we have our three commands. Remember Get-CimService, the first one, Get-Command Get-CimService -Syntax, the first one only had we told it we wanted the state and the name, so that is all that's there. If we call Get-CimService, you'll see. Now what's happening here, and as a matter of fact, let's actually filter this real quick, so State is running. So you can see that we were able to, it's working and it really is only returning name and state. It's PowerShell's formatting system is kicking in and for a service if you did a normal Get-CimService or Get-CimInstance on Win32_Service, all of these properties would be returned by default. We could very easily go back in and add these if we wanted to, but the neat thing is Get-CimService2, that was the one that we actually we told it to just let it put all the valid parameters in there. So we should be able to come in and do something like where -State is stopped, oop, hold on, something else you might want to do is transform the parameters as they're put in so that you replace asterisks with the WQL and SQL wildcard. But you can say where it's stopped and start mode is auto select, so and let's actually take that and run it against local host. And so you can see that you can take this command that we spun up with just a few lines of code. I mean behind the scenes it took a decent amount of work, but we were able to with just, what was that three lines of code for the actual command definition down here, with that we were able to have a command generated that we can perform filtering, we can perform it against any number of machines that we would like. And there is one more thing I wanted to show before we move on and that is if we come back in this reference scriptblock and this is just to kind of, this is like an idea, maybe you get an idea of you want IntelliSense to be added for some of the properties, so that kind of gets back to that idea of you make improvements and you want to use them so that every command gets to come along for the ride. So when we look somewhere I define the CimParameter, there it is. So inside the CimParameter, I have this commented out because it's actually pretty inefficient because there's not very many properties that fit on this, but what we're going to do is every parameter that comes through, we're going to add an argument completer attribute on it, and inside the argument attribute, it's a whole bunch of, and to be honest, it's only going to work on certain properties. What we end up doing is using Get-CimClass to look up whatever the current class is and it looks for any properties that have a qualifier, a value map, which I think and it appears to work. It's kind of WMIs way of having, telling you what the valid values are. So we just uncommented that. That kind of mimics adding that functionality and now we can call Get-CimService and we look at State and you can see that we now have IntelliSense added to it. So that was Get-CimService. It's the same way with Get-CimService2. State, well status has it as well, I guess. Let's see the StartMode, I don't remember, yep, StartMode. So you get IntelliSense for, this made it essentially for anything that has that value map qualifier and we never did look at Get-CimProcess, but it's there too. So anyway, I don't know, does that look like it could be useful for anything? Yeah. Alright. Absolutely.

Simple WPF Module

Next, I'm going to move onto doing this without a domain-specific language. This was an idea I had. I was trying to think of different uses for this and I came up with something that I don't know how useful this would actually be, but a lot of times I've tried to create little bitty miniature WPF user interfaces and first of all, there's a module, I think, that kind of does this stuff, Show-UI. I haven't used it myself and this almost definitely is not going to be better than that, so definitely go check that out. But what I've done because my interfaces are pretty simple, I'll use the XAML, and I'll have it read that in, and then I'll have to go in, and find whatever controls I'm looking for, and attach whatever events I want and I was wondering if there was any way you could use this to build a module that would allow you to make WPF user interfaces. So we're going to try that. Now again, this should look pretty similar. In this instance we're going to have, here's our hash table for command names. This is going to kind of be like the command info before, but we're going to actually just let PowerShell use Reflection to try to take a look at the different parameters to use for our WPF commands that are generated. So in this case, we're going to have, let's take a look at the reference scriptblock. This time around, the DynamicParam block isn't quite as simple and I'll go ahead and spoil this for you. What's it's doing is in a little bit I'll show you where it's going to write, it's going to look for every single WPF UI element, which I believe is the base class for all of the visual, the elements in WPF that you would use. It's going to take every one of those and create a new function based off of those and inside that CommandName, it's just going to store whatever name you used and whatever the type is. Now that makes it so if you want, you can add prefixes to your CommandNames or something like that. Then it's going to come through and here's where it uses reflection. It takes that type and it gets a list of all the public properties and then it ends up taking a look and if the public property has a public setter, then it's going to make that, that's going to be a valid parameter that you can use. It also takes a look to see if that property has an Add method attached. If it doesn't have a public setter, but it does have an Add method and that Add method takes a single argument, then it will allow that to be a parameter as well and we'll have logic inside the process block that will fix all that. Here's where it actually generates the parameter and then it does the exact same thing for public events. So what we should end up with is like if we have a WPF window, we'll have a command called window and we're going to have a whole bunch of parameters, anything that is a public property or a public event that can be set, we will have that available to us as a parameter. And then here's the actual code that's going to happen when everything's run. It's going to, it will go through each one of the properties that you've defined. First it will create an instance of itself, let's see that must have been up here or right there. Yeah, so it's aware of what its type is, and so it will actually create an instance of itself and then it's going to go through each one of those properties and we already checked to make sure it could have some, it could be set or if it's a collection it could be added to. So then the big foreach block here, goes through each one of the properties, and does that. And here's the part where it takes that reference scriptblock and associates it with each one. So let's see, we have, we'll take a look at the amount of code this is, so from Line 11 to 175, so a little over 160 lines of code, we'll take that and first, let's take a look at how many commands we have. So with a 160 lines of code, we made 181 commands. So things like, well let's take a look at Button and this isn't going to be pretty. I'm going to warn you. So yeah, this isn't something where you're going to use the help system to try to help you take a look. Yeah, I mean, you're going to want to go to online help for this, but what's neat about this, if you've ever written anything in XAML from PowerShell without help, which is actually not that bad, but it's, I'm going to just kind of retype what we have here because I want to show you that as you're writing you get should be IntelliSense title, so you can do all that stuff. So we have something here that it kind of resembles XAML without all the extra XML stuff on it. Now what this is saying is we're going to create a window and inside the window, now this is I don't like this part right now. We have to know that the actual property that contains the, in XAML you would just have a window opening tag, a window closing tag, and in the middle you would put the stuff you want. Right now, we're having to say Content, then we create a stack panel, and for stack panel, instead of content, we're having to say Children. But one thing that's kind of neat with that very, very simple code we're able to, this is kind of a weird syntax right here, but what I'm telling it is create a text box, assign it to the text variable and then output that. So you can in PowerShell if you were to do something like a = 1 and wrap that in parentheses, it will do the assignment and then also output what it is. So what this is saying, this is like saying create this text box, but we're also getting a handle to it so that we can, and here for the button, oh and I need to fix that, I was supposed to go in and show you how in the module, right now the events don't have a prefix. There was a little variable at the beginning of the dynamic module where I could add a prefix, so right now we have Button -Click somewhere, but anyway. So we're able to add an event handler in line in PowerShell like this. Again, the only way I've known how to do this if you do everything from PowerShell before is you create your XAML, you have to read that in, and then you have to kind of walk through, you could assign it a name, but then you have to go search for that and get that and attach the event later. So with any luck, hopefully this won't crash because that does happen occasionally. All this is doing is set up so you can say put a message here, you can type Show Message and you can see that it did truly, it was able to read that and we could attach that event handler in line. Now, the biggest problems I kind of have with this right now are the fact that I had to do that. Normally, when you're using XAML, you can just give it a name and then you can reference everything by that name later. So we're going to try to make it so we can do that. The other problem is I don't like how we have to know again that I would rather just be able to say window and then at some point have an open curly brace and a closed curly brace and have all that stuff in the middle. And WPF is really, it's really kind of cool. There is metadata associated with all of these different elements, so you can come in and look at the window type and go to Get-CustomAttributes and we'll tell it that we want to look at the attributes that were inherited. So you can take a look, oh actually we're running low on time. So what we're going to do now, we're going to make a couple improvements to that, alright, so the two big improvements are going to be we're going to make it so we can use the name parameter and it will automatically make a variable for us and we don't have to worry about it. And we're also going to make it to where you don't have to know about that content in that. If you still want to use that syntax, that's fine. So I won't bore you too much with the details. I'll just show you that it definitely is possible. Our code, we have several more lines of code, except now it's over 200, but now we can, I do want to show one thing here. Let's copy this instead of running it directly. Let's do this and before, so text box is still yeah, so text box is there because we did that earlier. Alright, let's run that again, and so now if you look here, so we called text box and we didn't have to do that weird assignment thing. We were able to just say the name is going to be text box. And if you come and look, text box is not in the global scope. We don't want it in the global scope. That's going to get real noisy and real messy. If you look, what I was able to do there is to add a dynamically, like a dynamic module that's attached to that window object and if we try to run this, you'll see that the text box lives inside that scope, so it shouldn't affect anybody and it should still, ShowDialog, this should still work and it does. So now you can have events that are completely in line. You define text box there and then you can use it down here. Also, if you looked, we didn't have to do the content, it was aware of. It knows that a Windows default content property is called content and it lets you do that. The next problem with this is if I try to add like something with a grid, so in XAML you have to have, there's a concept of something called an attached property. A stack panel, or so like if you have a button inside of a grid, that button doesn't have a concept of the row or column it belongs to in a grid or in a dock panel which one of those it belongs to, so you use what's called an attached property. So right now, I can't really specify one of those. If I were to come in and try to type Button -grid.row, which is kind of what the XAML stuff would look like, it's not going to let me do that. I can't create a dynamic parameter to do that. It turns out though with a little bit of trickery with that, you can end up, if you create a parameter, like a catch-all parameter that will allow all that stuff to come through, and we're just going to skip to the very, very end and run this. Oh, let's see. So you can end up with something that looks like this. This will work, hold on, the big thing I want to demo, so this is attached properties, right. So Grid.ColumnProperty, ColumnSpanProperty, right. So we're able to with a couple of extra lines of code, now we can start doing attached properties. You have this is what is called an attached event, so we have a combo box and a combo box itself doesn't have a text changed event, but it does have a, it comes from somewhere along the line I think it inherits from a text box base, which does have a text box event or a text change event. So anyway, just to show that this kind of works we can run that and hopefully it won't crash, don't crash. There we go. So and this was, I didn't really describe what it was going to do, but it's a silly thing just to show that it did assign everything to the right columns and again we had inline events that we were able to attach, so we can come in and grab something from here and it should change what's in that right span over there. Yep, because so. Anyway so that's it. Thank you. Any questions? When's your code going to be available? I will put it up right after this. It's going to be on GitHub, Rohn@ as a matter of fact, hold on, get this captured. So github.com/rohnedwards and if you need to contact me, Twitter @ magicrohn.

Converting to DSC in a VMware vSphere Environment

Agenda and Recap of DSC and vSphere

Good morning. We're going to do a second session on DSC in a vSphere environment. I'm not sure if some of you saw the session on Monday where I gave the high level view on what we're doing with converting, configuring your vSphere environment to a DSC-based configuration-based environment. First of all, this slide we can't show it enough. These guys did tremendous work in the last couple of 10 years and more even. Without them, this thing wouldn't be happening and what I'm going to talk about wouldn't be happening, so thank you to all of those on the list. Much appreciated. What are we going to talk about, while the one on Monday was kind of high-level overview where we looked at the concepts that I'm trying to following to convert your vSphere configuration to a DSC-based one, I'm now going to dive into some code. So for those of you who don't want to see a lot of lines of codes, you still have time to leave if you want or get an extra coffee. I don't know what you prefer, but there will be some code involved this time. I will show you one of the things I'm looking at and developing. It's in a far, it's in a stage that it's usable, but it's open for a lot of expansions as we will show later on. We'll handle some points more in detail like we mentioned on Monday. In your vSphere hierarchy, there is a tree structure with all kinds of different nodes in there. You have folders, data centers, clusters, and so on, so determining where your new resource will be created or defined is quite important that's why the location, location, location thing is in there. So we need to make sure that we position ourselves on the right note and that inventory tree the moment we try to create a new resource. There's a small example of reusing code like we always do. I do at least. I think it was during the last VM World that we made this famous claim, don't write the code, just steal it if it's already out there. There's a lot of valuable blogs out there, tons of code, so don't reinvent the wheel. But one thing you should keep in mind, if you copy something from the internet or even your best friend, test it. Don't run it like that in production. We never do that, of course, but just as a warning on the side make sure that you test what you copy from others. And automation, as it should be case with everything we do in PowerShell is our ultimate goal. Who am I? Okay, I'll keep this short since I already had in a previous one. I'm Luc Dekens. I do a bit of talking at VM World in the last years. We have a book. Second edition just came out. I will plug my book only once in contrast to some other people, right. And if you want to reach me, there's a Twitter handle that you can use to contact me and there's a mail link on my blog if you need more detailed questions than 140 characters in a question. Okay, a quick overview for those who didn't attend a session or already forgot. It has been two days of brain melt, at least it has been for me. DSC is a very valuable resource array of configuring your environment. Now unfortunately at the moment, VMware themselves, they don't provide any resources for that, but since we have the option, we had the option in the past and we still have it to script DSC resources. Now with WMF5, we have the option to do those class-based resources with even better, more optimal. I'm trying to start something going where we have resources defined for your complete vSphere environment. So like I showed last Monday, we already have quite a few, but it's still a lot need to be done, especially in the finer details of the configurations. Another thing, an important thing, that we want to obtain with the converting to DSC is that we want this famous separation of where and what. I just made a small two screenshots at the bottom there. So if you look at that one, the right one, is the one where we will actually do something. That is your actual folder name that you're going to create and the location where you want to create that folder name. This is the part that you want to put under source control. This is where the changes will be visible where you can make the drift with the previous situation. On the left, that's the what you will be doing. That's only a small extract. That is the one that creates a folder and it uses the data that we specified in the right side text that you see there, which is part of your configuration file. So this is the important concepts in what we are trying to do with DSC for a vSphere.

Get-Inventory...or Not

Now I mentioned it earlier, the vSphere Inventory tree is quite complex and this is partially because historically it grew like that and they added new types of resources that they had to fit somewhere in their inventory tree. What I did here and most of you ever use the vSphere classical client, they will know this view. There are four types at the moment that you can look at. Top left, you have the one where you have the view, what VMware calls the host cluster view. So there you see what they also call their yellow folders. You see the data center's clusters and your hypervisors in there. Top right you see the VM template view. That's the one where you will actually be able to set your VMs in specific folders and on these folders you can then apply permissions which will allow you to delegate certain parts of your environment to specific groups of users. What is newer and that appears later on, at the bottom left, you have the one for data storage. They have introduced the concept of data store cluster some time ago. That is the hierarchy that you find under there. And then the last one, the fourth one, is where your network configuration is stored in the hierarchy. Again, we have two types of network objects. Basically, you have the standard classical switch is the VSS as they call them and you have to distribute the switches, which is VDS. In these screenshots, what you see the yellow and the blue folders are the ones that you actually define. If you create for example, in the bottom right, if you create or distribute a switch VDS1, it will create a number of port groups by default. Uplinks, for example, will always be there. And then, as you add additional port groups through that specific switch, they will start appearing under this node in the hierarchy tree, so just to make it clear that there are different ways of viewing objects in your vSphere environment. So the path is quite important. Now if you want to make an inventory. So the ultimate goal that we want to achieve is we want to have an inventory of our existing vSphere environment. We're not going to start from scratch. Once these DSC resources are available, we all or most of us have a running vSphere environment, which we want to be able to convert to DSC configuration. So what is important is that we get a complete inventory of the environment to start creating your configuration files. So the obvious one, if you look at the cmdlets that are available in PowerCLI, for those of you who don't know what PowerCLI, PowerCLI is an at all. It used to be PS snap-in. Now it's converting to a module. There are a few PS Snap-ins left. What allows you to configure and manage your vSphere environment, so it's on top of your PowerShell tool. Now the obvious one would be there is a cmdlet Get-Inventory. When you run this, we see a lot. This comes from a test environment, which I kept obviously very small to make it manageable in one screenshot, in fact. What we see here, we see most of our elements, components of our vSphere environment coming back, but we are missing some. Remember we had screenshots of distributed switches, of datastore clusters, these are not in there unfortunately. So I did a count so we can compare it with something that I will do later. We have 22 objects returned from your vSphere environment, but it's not complete obviously. So what do we do? And this is a guideline that I give all of the people. If the cmdlets don't do what you want to do, you have to go through the API. Now luckily, PowerCLI from day one first of all the cmdlet Get-Few in this case to easily tap into all the API methods and properties. So if you're missing something or you want the very specific configuration that is not provided by your standard cmdlet, go through the API. The API is not the most obvious environment to code in. There's the API reference, there's an extract screenshot at the top right here and setting up or making goals through those APIs or obtaining those properties in there is not always too obvious. Now in this case, we're lucky. There is a basic concept, a managed object, which they use internally to retrieve inventories from a vSphere environment and that's what we are doing here. First of all, everything in vSphere and vSphere API starts from your service instance. This is so to say your base object from where everything else is starting. You even find, and that's what we will be using later on, you even find your root folder of your complete vSphere environment. One thing that is quite interesting with the service instance, you can use this against the vCenter. VCenter is the management layer on top of all your hypervisors, or you can use this exactly same API directly against your hypervisor, the ESXi node, in this case. So you use the same scripts to inventorize all of vCenter or an individual hypervisor if you want to configure an individual one. Now what we did here, there is like I said one method. We just called CreateContainerView. We used that one to get us a more complete listing of what is in your vSphere environment. Very straightforward. I'm not going to go in too much details. It requires a number of parameters where you start, in this case, we use, like I said, the root folder, which is also available in that service instance. You can specify what kind of types you want to return and types that will be things like data center, cluster, and virtual machines. And this also explains why with our cmdlet Get-Inventory, we don't get everything because they only retrieve, because it was written in the early days, they only retrieve the things that we saw, folders, virtual machines, clusters, host, and so on, but they skipped datastore clusters and network objects like virtual distributed switches. So what I'm doing in this goal, I'm just passing it null. I'm not passing any types. And then by default, it will give you all of the types that are available. And the last, the recursive on this is an obvious parameter. In this case, you just want to run through the complete tree. It's as simple as that. If we run this, we get this kind of inventory. So most of the stuff that we saw in the Get-Inventory cmdlet, we will see back, but we now have a complete view. Remember it was 22 in the previous 1 with the cmdlet, now we have what is it 28 of those, and we also see stuff appearing in there like dvs, dvportgroups, datastore clusters, and so on. So we have now a complete view of our vSphere environment, but there's too much in there. There are things in there that are normally hidden and that you don't see in the GUI or the WebClient interfaces.

Location, Location, Location

Okay. What we want to do and I explained that in the beginning, location, location is very important. If we create a new object or want to create or configure a new object with DSC, we have to be able to exactly tell them in which node we want to create that object. So this is a funny location picture, but it stresses the fact that location can be quite important in some cases. What I'm doing here, we're coding or trying to code in this case the part into which a specific object is located. The code again is quite simple. There are a few basic concepts in this script. We are again passing through a vSphere API method using the same CreateContainerView. We limit what we want to see. This is an advice that you should always use when you start using this Get-View cmdlet. These internal objects are quite big. There are tons of properties on there, so only retrieve what you actually need. It will make your script a lot faster, especially if you're running in bigger environments. You can imagine that creating something that has 100 properties and returning it takes a lot more time then, in this case, we only need two properties and the property that is returned by default, the pointer to the object is always in there. You don't have to explicitly specify it. Now if we use this code and we run this, now we're getting somewhere. Now we are returning the name of our objects, the type in the second column, and what you see on the right is the parent, so that is the node just above the one that you are listing here in this inventory. It is represented as a pointer, so don't look at that exactly, but it's something that we will use in the next step in our script. What we're doing here and I'm showing something that I always try to use in my scripts and my functions that I write. First of all, make it modular. If you have a specific functionality that you want to achieve, make it a separate function. That's what I'm doing here with Get-ParentName. Secondly, what is also quite interesting, instead of writing tons of lines of code, make your functions recursive. So traversing an object tree is ideal for a recursive function. So we had the same code as we had before. We are now just creating or going to try to obtain the part to the object that we're looking at and we do this recursively by calling that Get-ParentName function that you see over there. And then, we go a bit deeper in the function itself. We, in the end when we have the part, we just return that as a property on an object that we create for each inventory object. So this is the ultimate goal we want to see the object, the type of object, and the parent, the complete path, in fact. And this is what we're having now. What you should be noticing that some of these things in the path, some of the nodes in the path are not appearing. There is, for example, I'm not sure if it's on this one, yeah it is, there are the ones in the first red box there. These are what they are called hidden folders. In their inventory system, they made some hidden nodes in the which they can attach those four types of object we mentioned in the beginning of this talk. We have the VM view, the host view, network view, storage view. Those are the hidden folders. Those we don't want, in fact. We're never going to configure them ourselves, so the next step in our script would be to just eliminate those from our script and that is done quite easily with the WHERE clause where we can just say if you encounter an inventory node with that specific name, don't show it to me. I don't want to know it. And on top of that, in the function where we define or create a path to the object, we also skip those. You don't want to have to specify those in the path of the object that you are inventorizing. So this is all very straightforward PowerShell code, PowerCLI code, I guess there's nothing. If there are questions, don't hesitate to ask. Don't start writing this down. I will publish this when I get back home and you will find the complete function because it is a bit bigger in the end. You will find it on my GitHub repository. Okay, we are there. So this is now our inventory, the function that we started with originally. We have all the information we need. We skipped the ones that we don't need, so this is quite complete. It's better than the Get-Inventory cmdlet because we also see stuff like distributed switches, we see the datastore clusters. Datastore clusters are called internally StoragePod, that's why you see it shown this way, but it's the same thing. It's the internal name, which apparently is different from what cells in the end decided it should be named. Now, what are we trying to go to. We want to create, that's our ultimate goal, we want to create these configuration files automatically from an existing environment. So what you see in the red box there, that's the stuff that I want to generate with the script in the end. All the rest in my configuration file is rather static. I could use that in an inline string, code it once, and just create the file, but this part is the part that really depends on what it discovers in your environment. So what I'm doing here, I'm using an inline string and I just make a placeholder there. That's the one that is marked and highlighted in yellow. That is the one that I will replace with text that I generate based on the inventory we created with the function earlier. So it's a very straightforward thing. All the stuff that is fixed or semi constant, you can do that in the inline string and the stuff that is different or will change in your environment, that's the one that we will insert in the text. How we do that is very straightforward. Everything is there, it's a string, you just do a replace and place of that marker with the text you generated in the end. Now there are a few properties on these resources that are quite important. There is, for example, the one where you will tell it your specific node. Imagine a folder, which is a subfolder of another folder, so we have Folder1, Folder2. We want to define that dependency. So if we create Folder2, we have to find a way to say Folder1 should be there before. So DSC allows you to do that with the DependsOn. Now we have to create depending on the path. We have to construct that dependency dynamically from the inventory we are pulling. Again, this is quite, can become quite complex because a folder can be located under a datacenter or it can be located under another folder, so there's a bit of logic involved to find the correct dependency that we want to use in our DSC configuration file. In the last lines at the bottom there I'm actually generating the specific text for that specific invent. This is an example from the folder, but we have similar things for datacenter clusters VM hosts. This is just to show you how I do it in the end. There's a bit of logic, for example, for the folders where we decide if it's a blue or a yellow folder. Remember yellow is the VM host cluster view. Blue is the VM template view. And in the end, we will arrive with something that is actually usable in our DSC configuration file. There are a few issues that you might encounter when you do this kind of inventory. Suppose and vSphere allows you to do that, you have a folder with the same name, but in different locations in your hierarchy. So you have to find a way to actually find the one that you want to look at. Now I could have written quite some code, but luckily, and now I'm coming back to the point that I made at the beginning, don't reinvent the wheel, just steal the code. There is a guy who already wrote a function to do that, which happens to be someone I know. He allows you to give the path to a folder, so he will follow that function, will follow the complete path and give you the correct object. So no more issues if you have likewise names or folders with the same name. You just give the complete path and he will return to you the correct object that you need in this case. So we plug that in our script as well and then if we run it ultimately, remember it was especially the where part that we were interested in the configuration variable. This is the end result for folders in this case. I just re-include the two screenshots of those two views on the inventory here to show you what we are actually doing there. If you look, for example, at Folder4, that's the one at the bottom, the blue folder, you will see that there is a dependency Folder3 should be there before Folder4 is created, which is obvious. It's a subfolder on that one. But in a similar way, we have for Folder2, for example, which is a yellow folder top view. That's a folder that is hosted, the parent of that folder is a datacenter so you then have a dependency as we showed there in the DependsOn on that datacenter Homelab that we created earlier or in another line. Now once you have these dependencies, the LCM will take care of it and he will do the right or do the right thing to regenerate your complete environment. Once you have this stuff, it's quite easy to put this in the source control. You can use this to find drift in your environment. Somebody changes, somebody deletes a folder, this will discover. Once you have this defined, in DSC it will find it and correct it, but you could also use this kind of stuff to migrate your environment. Suppose you have a new vCenter, new version, an update, a new hardware, I don't know what, you just want to transplant your complete environment. You could create that configuration file and then just use the configuration file to recreate exactly the same structure on that other vCenter. So the use case is for this kind of configuration files are quite interesting, so source control, but also migration and recreation. You can use it in a DR environment where part of your environment would by disaster disappear, you can recreate it quite easily that way.

Future Extensions / Fixes

Now this script is producing the files that I showed previously. There are some extensions that I want to create on this script. I'm not there yet. I'm working on it, but they will be coming. There's first of all the integration that get, there is a very good module or announced at least that allows you to automatically once you have that configuration file, you will start it automatically, so automate will be our primary goal. You will automatically start it in your get environment. And then were some of you in the previous session with Steve Marowski and Michael Green I think on this release pipeline. In fact, this plugs it into what these guys were saying. This is the integration that we ultimately want in our automation of our environment. We want not only the creator's configuration files, that's only part of the story. We want to recreate, for example, you create a new distributed switch, okay, we can generate the configuration file, but I would also like with this script since we're handling the object, recreate or create the Pester code to test that switch if it's function is as it should. We have all the information available. We know the name of the thing, we know the configuration settings, we know where it's located, so with this ultimate goal of automate everything, this would be a nice integration that I'm looking at. Same goes for PSake, I want what they announced these composite resources. It's typical in a vSphere environment we could use this very well. I think I used the example of a cluster. You have a default cluster in just one click, but then again if you look at the components in a cluster, for example, there's high availability, you have the source scheduling, you have DPM. Those require additional parameters, so you could use those as SUP resources under your cluster and your complete cluster could become one composite DSC resource in that case. So instead of generating all these individual DSC resources configuration files, the ideal world, we should be able to create a composite resource and just plug in the values that we need for that specific instance that we are trying to move under DSC control. There's one shortcoming in the script at the moment and I mentioned the blue and the yellow folders, but you notice as well in the beginning already that there are other types of folders, the hidden ones, remember, there's network and data storage. Those need special taking care of because their paths are a bit different. In fact, they need to make sure that they are created under those hidden folders. If you create a folder where you want to store, for example, all your datastore clusters, that folder should be created under the hidden datastore folder, not under the VM or host folder, so I need a bit more expansion on that one in the script. Another thing that is quite useful and this is one of the beauties of PowerShell, you can have access to everything in your environment. In this case, I created, I'm working with this class-based resource VMW folder. You could look at that resource folder, resource class, list the properties and check for completeness. So somebody in your team adds a property to the class, you could use this to check if the configuration files that you are recreating are complete, are containing all the properties that should be there. I know not all of them are required, but still it would be useful to have all of them in there. So this is another part I'm looking at, use what is available or what is returned by the Get-DSC resource to make it more complete what you generate in your configuration file. And then of course, you noticed my text that I generated was all left-aligned. We need a bit more alignment. It makes it more readable, but that I consider one of the minor issues that I need to solve in the ultimate script. Are there any questions on this? I know it's a lot of code. It's 10:00 in the morning. You have more sessions to come. You have a question? (Audience comment) Yeah, this is something in fact that I discussed in the first session, but I can pull up the slide so you can have a look. If you give me a minute, I'll switch to that one. I think I still have it here. (Audience comment) No, no problem. That's just a minute. What we discussed in that, this is just a view on the lab I'm using to test this environment, but it gives you a good idea of the set up that I'm propagating here. Bottom right, this blue box is your vSphere environment, classical thing, VSCA vCenter hypervisors. Top left is your standard Windows domain, domain control and pull server. I need a work station to edit some stuff, so I have two of them because I want to test them on 8 and 10 at the same time, but that's just a practical thing. And the key thing in there is this vEngine that you see on the left. So on that vEngine, that is if because we can't install LCM agents, VCSA is an appliance. VMware doesn't like you to put stuff in there. ESXi is a closed hypervisor. They don't want you to put stuff in there, so what we're doing is we are installing the LCM agent on that vEngine and it's on the vEngine that we run the configuration files. On that machine, you need PowerShell and PowerCLI on top of it. That's the one that will actually do the changes in your vSphere environment on the right. So yes, you need PowerCLI on that vEngine. But in the end, you don't need to touch your vSphere environment at all. You just need this helper server. Does that answer your---? Yeah. Okay, thank you.

Q&A

Anymore questions? Yes, Josh. Those folders are inherently designed to obtain things. Yes. Do you also have resources for maintaining that alignment so that virtual machines that could be in a specific folder do not get moved to other locations because from an operation standpoint, that's what I see happening is someone inadvertently when they're selecting it and then drag it to another folder and lose it. Yeah. That's a good. Josh wants to know if the location in your vSphere inventory, if that is or can be controlled by DSC and yes it can, of course. If we look at one of the earlier slides, let me pull it up again. Okay, if you remember that slide from earlier on in the presentation, this part at the right, that is in fact the location that you want to look at and what you are asking, if I understand the question correctly, if somebody would move your VM from Folder2 to Folder3, for example, now with the VM in the configuration definition, in the configuration file for the VM, we will have this path. Path will always be there. That's why I'm stressing that obtaining this path is quite important in this complete DSC setup. If DSC LCM agent sees that your path is not correct anymore, you will automatically get the set call and that should correct it. So what we will have in that case, if it fails on the test of the path, you will generate a move to that folder. Same goes not only for folders, but also for datastores, for example, or even clusters for your VM. So ultimately, your DSC LCM could be even, in fact, vMotioning, SVMotioning machines to comply with the configuration that you defined in your configuration files. So it can go very far in the end. There are some tricks, there are some things that I need to resolve because you have to take into account high availability, for example. You're not going to specify that a VM should run on a specific ESXi node. There are some cases where you perhaps would be inclined to do that. I'm thinking of those VMs and I, at least, have them in my environment. Some of you probably where people have applications that still require a dongle or something like that. (Audience comment) Or yeah, something like that. You need to fix it on a specific VM ESXi node. There you would need to override what HHA is trying to do with your environment, so it's a bit complex on that level. Yeah. Would it make more sense to just create DSC resources to manage the DRS rules and make sure those DR rules are in place, so if someone changes a DR rule, it adds a rule back. Yeah, so DRS, the cluster configuration in vSphere has an option DRS and in DRS you can specify rules where you can create affinities those VMs should be running on those ESXi nodes. Yes, I fully agree, but that is more on the level of vSphere management. You should be managing that in any case, but rules. Now those rules in itself, you can configure with your DSC configuration. That is something that I will add on the cluster resource. We shouldn't rely on only the DSC configuration to define the configuration. I fully agree that it should be in the rules. Correct. Thanks. Good question. Honestly, I think that is an extremely valuable use case because we still see a lot of people who are running applications like Oracle or even SQL Server where you're paying like per socket or things like that. Being able to not only say that, I've set up affinity, but I'm also ensuring that then be can't be broken provides a lot of confidence if anyone were to try and come and audit you on that. So how will you know that it's not going to be implied? Well I have affinity. My score is just configured in a certain way so that it doesn't crossover and I'm using DSC to ensure that if anyone accidently makes a change to that affinity, it autocorrects so that we don't get into a situation where we're no longer compliant. I agree, but on the other hand, I would be more inclined like we heard that to do this with rules and DRS will also discover that its numbers were sitting on the right machine. Perhaps the DRS cycle is not as frequent as the LCM agent cycle, but I consider this more on management of your vSphere environment. LCM is another layer, another level. Getting those rules in the cluster, that I would leave to DSC LCM. But applying the rule, I would leave that to vSphere in this case. But you're right, you could envision doing it that way as well, but then you would be recreating the same functionality that is already there in vSphere. I'm simply implying that those affinity rules be set and managed so that they can't be staying with DSC. Yep. So vSphere is still doing its thing, it's still responsible for ensuring and guaranteeing that the workload doesn't move, it's just that you're simply saying that DSC is keeping an eye on DRS affinity to make sure that those rules don't change because someone could change one of those rules in DRS and virtual center server, which could cause a problem that would make you noncompliant, but if DSC is just keeping an eye on that, then that's less likely to happen. But yet you still let DRS do its thing, not have LCM do it. Yeah. Fully agree with that one. Yeah, good idea. Anymore questions? Yeah. Do you need a vEngine for…? No. No, that's the reason why, okay, I'm not going to pull it up again, you remember. With the vEngine, you can use multiple sessions in there. So each session could be connecting to another vSphere server be it multiple vCenters, be it multiple ESXi nodes or a mix of those two, no. Because in the resource, I'm going to pull it up in any case because this is quite important. That was on session one, but I can pull it up if I, okay, let me check if I know. I can't find it immediately, but I can show you after the session that I am doing is in fact in each resource entry in your configuration file, I had two properties, which is a vServer, which is the indication to which vCenter or ESXi you are connecting and vCredential, which is a credential you should use for that connection. So per resource you can define where it is located, so in the case of multiple ones, you go to the correct one in that case. Does that answer your? Yes. Thank you. Any more questions? Just what I also mentioned in the first session on Monday, these DSC resources are a work in progress. I have a number of them. When I'm back home, I will start publishing them. They are now in a private repository that I'm using to develop them. I will put them on public GitHub because this thing, its environment is so complex no one person will be able to do all that stuff, so I'm counting a bit on community contribution there in the coming months. I don't know what. Also, on the priorities because like I said this stuff is quite complex, I could imagine that I would now if I put priorities on my to-do list so that I would first be looking at distributed switches part. Perhaps the majority of the users say I would rather first see a fully working cluster resource, so it's in fact up to you guys to tell me or do the work yourself or tell me which one should be the next on my list. Yeah, we already reached that one of course since we are discussing your questions. No. Anymore? Okay, then I thank you for attending this code heavy session. I hope I didn't bother you. Thanks!

Creating an Agent-less Host IDS Using PowerShell and WMI with Matt Graeber

Introduction

Alright so, I'm Matt. This is Jared, and today we're going to be talking about Creating an Agent-less Host Intrusion Detection System using PowerShell and WMI. So what do I mean by agent-less? I mean we're not actually installing any software on our targets. We're using effectively permanent WMI event subscriptions to trigger off certain events and then to use built-in classes within WMI to respond to those events accordingly all without dropping any MSIs, exes, dlls, drivers to disk. A little bit about myself. This is my Twitter handle. Right now, I'm the R&D capability lead with the Veris Group Adaptive Threat Division. Previously, I was on the FireEye FLARE team doing malware reversing. And then before that, I did some red teaming on a government red team. Parts of that, I was enlisted Navy. I was a Chinese linguist. I'm a CDM MVP, although with my specialty obviously being in PowerShell, although I wouldn't really call myself a cloud or datacenter expert, so it is what it is. I'm also the creator of PowerSploit and PowerShellArsenal. I've also got various other modules and repositories on GitHub that you're welcome to check out under mattifestation. Cool. And I'm Jared Atkinson. I'm the Hunt technical lead. I just got a new job at Veris Group in this Adaptive Threat Division as well. Former, I was in the US Air Force on the Hunt team. I mentioned on Monday I was the 2015 Black Hat Minesweeper champion, so before PowerShell there was Minesweeper and I was obsessive compulsive with that as well. I'm the moderator of the powershell.com security forum, which if you have PowerShell security questions, please come ask them and we'll try to help you out. And then I developed PowerForensics, WMIEvent, and Uproot IDS, which the last two we will be talking about today. Alright, so generally, what is Uproot. I'm kind of setting the stage. Uproot is a host-based intrusion detection system built on permanent WMI event subscriptions, so permanent WMI event subscriptions allow you to monitor for changes in the operating system and why not monitor for activities that may be associated with an intrusion. That was kind of the thought process. And it leverages this WmiEvent module to easily manage subscriptions. What is WmiEvent you might ask? Well it's a PowerShell module that abstracts the complexities of permanent WMI event subscriptions. So WMI event subscriptions, there's a lot of background knowledge that you need to have in order to get them working, and so WmiEvent is trying to kind of bridge that gap for kind of a new user of WMI event subscriptions to be able to easily create and modify these subscriptions. That's a CDXML module, right? The base is built on CDXML, yeah. Sweet. Okay, so a little motivation behind our work here. In my previous job, we were investigating the group known as APT29 and some of the techniques and tools that they used involved using permanent WMI event subscriptions, both as a persistence mechanism, right, so their code would execute after boot every time and also as like a covert storage mechanism, so they were using the WMI repository itself as a means of storing their payloads and what that enabled them to do was both store their payloads, perform persistence, and also execute those payloads entirely remotely using nothing but WMI, which is pretty cool. So a buddy of mine during our holiday party was asking me because that they were, I mean they were kind of down in the weeds investigating this really extensive breach that had occurred and he was already pretty experienced with WMI permanent event subscriptions, but he was kind of curious like, so he asked me how would you go about detecting the like WMI persistence, right, so in other words, the creation of permanent WMI event subscriptions. So I started thinking and actually it occurred to me like you could use, you could create a permanent WMI event subscription to detect the creation of permanent WMI event subscriptions. So that's what kind of lead me down this path to think that there's like, there's all these built-in WMI methods, or sorry, WMI events and classes that you could potentially trigger off of to detect all kinds of malicious actions. And so it is actually kind of an interesting concept because before Matt and I actually knew each other, we both came up with the idea of using permanent WMI event subscriptions as an IDS completely on our own and then like the genesis of our relationship, I guess, was talking about this concept, so that was pretty neat. The reason why I kind of came about it was I was in the Air Force and as you can imagine, the DOD in general was very specific on what type of software can be added to their systems and there's a very long change process, and so we were often running into issues with being able to push out continuous monitoring applications, and so one of the reasons why I got interested in PowerShell is if you could script it, then it's not considered a software change, and so you could literally script anything. And so, the idea was to create a continuous monitoring capability that leveraged WMI and PowerShell as a mechanism to monitor for adversary intrusions. And now as a consultant, I still have that same kind of idea, right, so as a consultant, I can't go into a Fortune 100 company and say hey guys you're just going to randomly deploy this software that I'm giving you. We could just go in and make changes to configuration as opposed to actually changes in software deployment. And the great thing about WMI is that it's been preset on every version of Windows going back to like Windows 98. So theoretically, you could use like say the v3 CIM cmdlets to administer a Windows 98 box, although, I think you'd have to install WMI, I think, by default it comes installed in like Windows XP and above. Correct me if I'm wrong. But I actually use the CIM cmdlets very often to do WMI tasks in Windows XP where PowerShell isn't even installed, so it's great. So If you have a capability like this, then all you need is like your host machine with PowerShell on it to run like these WMI or CIM cmdlets onto and install these like these signatures on remote systems all without needing to have PowerShell on any of them, just the WMI service listening.

WMI Eventing Refresher

Alright so a quick refresher into WMI eventing. There's really like two event classes. There's extrinsic and intrinsic classes. Extrinsic are my favorite because they trigger immediately. Alright so if you've worked with WMI events before, you may have noticed that sometimes you need to add like a polling interval, so that's a requirement of intrinsic events. You don't need those with extrinsic, although, you're not getting quite the flexibility per say for extrinsic events just because there's not as many available, whereas, you can get really creative with intrinsic events. I always prefer to go to extrinsic first because I know they will fire. Whereas with an intrinsic event, say you wanted to trigger off of process creation, so if you had like an InstanceCreationEvent that triggered upon a creation of a Win32_Process class instance, there's a chance that you might miss that within that polling interval if that process started and stopped within that polling interval timeframe, so that's why I'm always hesitant to use those, but if I have to, then I have to because I mean, there's not an extrinsic event for every conceivable event in existence. So a good example of an extrinsic event would be like the registry key chained event. So like you would specify the hive and the registry key and then if there's any change within that immediate key, so like it doesn't work recursively. It would trigger. And then like I said like with the InstanceCreationEvent as an intrinsic event, you'd have to specify that polling interval, but you can get super creative. So like there's thousands of these WMI classes built-in by default in the WMI repository, so really just like use your creativity to think about the possible events that you might be able to trigger off of using intrinsic events. Alright there's, using PowerShell, there's really like two ways to register WMI events. The first would be local events, so using the older WMI cmdlets, you would use a Register-WmiEvent, and then the newer CIM cmdlets available in v3 and above, you would use Register-CimIndicationEvent. And most of the examples that I'll be showing you today will be using those CIM cmdlets. And the great thing about the CIM cmdlets, at least from my perspective, is that they make deployment of these WMI events and signatures so much easier because they give you the flexibility of being able to use either WinRM as a transport or Legacy DCOM, right, so if I wanted to use the CIM cmdlets to deploy signatures to Windows XP, I would just specify the CIM session option to use the DCOM protocol. And then there's permanent WMI events. So what you need to use for the older cmdlets will be Set-WmiInstance to create new instance of a set of classes I'll show you. Otherwise, using the CIM cmdlets, you would use New-CimInstance. With the CIM cmdlets, there's also like Set-CimInstance, so it's a little confusing, but actually it makes more sense with the CIM stuff. Set-CimInstance will set the properties on an existing object, whereas New-CimInstance will instantiate a new WMI object. Whereas, Set-WmiInstance kind of does double duty doing both. Okay, so in order to set a permanent WMI event, you need the following. You need an instance of an event consumer class and there are five standard event consumer classes I'll cover briefly. So the EventConsumerClass, sorry that should just be EventConsumer. This is the action that you want to perform upon your event filter triggering. So the event filter is comprised of just AWQL event query and I'll show you some examples of those. And then to install the permanent WMI event, you just bind the filter and consumer together using an instance of a FilterToConsumerBinding. Alright, so let me run through a few examples here. So on intrinsic events, can anyone tell me what this is interested in catching. I mean, it should be self-explanatory, I guess. (Audience comment) Yeah, so this will trigger anytime a service changes into the running state, so I emphasize changes by using the instance modification event class. Now if you're interested in a new service being created, then you could create a query using InstanceCreationEvent where TargetInstance is a Win32_Service. In this case, the polling interval is 5 seconds that's set within 5. Yep. So if for whatever reason this service went from a running back to a stopped state, there may be an instance where you wouldn't catch that, although, I think in this case you probably would. But again, intrinsic events can be kind of tricky with those polling intervals. Alright, this one I really like. I love the Win32_StartupCommand from a defensive perspective, it's great. So this will just monitor for any instances of new Win32_StartupCommand classes. So what this class is it covers all user and system runkeys and user and system start menu items, so I don't know if Microsoft had the intent that this would be great for defensive purposes for catching attacker persistence actions, but that's what we love to use it for. Alright, some extrinsic events. This one's kind of cool. I'll be demoing it shortly. This has nothing to do with the like volume on your speakers, rather like disk volumes. So what I'll show you is I'll trigger an event upon inserting removable media into my laptop. What's EventType 2? So I have it pulled up here. The question was what is EventType 2. So I just pulled up Win32_VolumeChangeEvent in MSDN and EventType 2 is device arrival, so insertion of removal media or any fixed or removable media. Okay. How is that going to care like if I were to dock my laptop on my docking station, would that detect all the USB-based devices that are already plugged into that docking station? So the question was would this extrinsic event trigger off of say like you attached your laptop to a docking station, would it trigger for all of the new devices that were attached thereafter. The answer is probably no. Unless you had like some like a removable drive plugged into the USB port on there. I think if it gets registered as a logical volume, so like it gets a drive letter per say, then it will be registered regardless of how it's attached. All of the other devices that are registered would be registered as like a plug and play device and there are WMI classes for plug and play stuff, so you could certainly trigger off like new device creation and what not. Again, you can be very creative with this stuff and there's a huge WMI repository at your fingertips to create some of these cool event filters. Okay, and the last one is Win32_ProcessStartTrace. I really, really like this one again because it triggers immediately. So let's say I was interested any time Chrome started up, I would be notified immediately with this event. Okay. So the event consumers, you have five built-in event consumers provided by Microsoft, thank you. So one is LogFileEventConsumer, so upon an event filter triggering, write some data to a file, and you can actually pass the arguments provided from the event filter to these event consumers, so you can have like contextual data provided to the event consumers that you would likely want. So like in the case here of the ProcessStartTrace, if you wanted to log the name of the process and the process Id to the log file, you would be able to do that. ActiveScriptEventConsumer, this allows you to execute any WSH scripting language, so that includes VB script and J script, sorry no PowerShell. So Jared's going to show you a case where he uses this to make a call to make an HTTP Get-Request out to like CIM and report some like security-related information. It's for the people that like pain, the ActiveScriptEventConsumer. Yeah. Attackers really like ActiveScriptEventConsumer too. They also really enjoy the CommandLineEventConsumer, so say you have PowerShell present on the operating system and then you have some interesting like attacker created event filter and then once that triggers, it would say execute something like powershell.exe-encodedcommand and then some basic C4 encoded, super evil thing. There's NTEventLogEventConsumer, so it creates an event log entry and I really like this one because the event log already captures so much like security-related stuff anyway, but what permanent WMI events allow you to do is supplement the event log. For everything that the event log doesn't capture say like the creation of permanent WMI events, you would be able to write those actions to the event log because those would otherwise never be detected. And then NSMTPEventConsumer shoot out an email if some event occurs. Yeah, don't do that when you're like monitoring process creation. Yeah. Yeah, and as I kind of go through the methodology that we have for creating these signatures or really when I refer to a signature, I'm referring to a WQL event query, you'll want to be very targeted in how you create your filter so that they're not firing all the time or that they're extremely high fidelity such that they only trigger when you know that something really, really bad is happening.

Signature Development Methodology

Okay. So as you start thinking about what events you might want to create, think about the following things, right. So let's say you want to start listing out the things that you'd like to detect, so service creation, registry persistence, remember I showed you the Win32_StartUpCommand class, that's a great one. Lateral movement, I'm going to be using this one as an example in a little bit. WMI persistence, again we've already kind of talked about that, and so go down your list, start writing out what you'd like to detect, and then start digging through the WMI repository and see what classes might be able to match up the action that you to detect to what's in the WMI repository. But before you go down that road, just consider that there might be other detection mechanisms already present, so like I think it's like in Windows 7 and above, I forget what it was backported to, but you can enable process auditing anyway, so that's all going to go the event log anyway, so that's all going to go to the event log, so you may not need to use WMI for process auditing, but you could. And then Command-line auditing, like Applocker. You can put Applocker into audit mode, like if you wanted to track all modules being loaded, like all device drivers, executables and dlls that were being loaded. That would already be logged to the event log if you have Applocker in force or audit mode. So consider other options first is what I'm saying. Now if you've determined that you really do want to use permanent WMI events, consider the following. Go through and discover all of the extrinsic events that exist first because again those are ideal. They will trigger immediately. And if not, then fall back to intrinsic events and just be mindful of like the polling interval and whether or not you will miss events from firing. Okay. Now how would you actually go about determining what classes are available, well we have PowerShell for that. And then in the latter portion of my demo, I'll also be showing you how I test out my events using wbemtest, the super old school tool with a terrible UI, but it's extremely useful, it works. It gets the job done. Alright, so here's my first example. I'm just showing you a local events subscription using Register-CimIndicationEvent and what I'm interested in is anytime a new volume is attached to my computer, so by attached, I mean EventType 2 and you'd have to look that up in the documentation and the action that it will perform upon that event firing is it will just print out the drive name. Okay. And to register all these events like well especially permanent WMI events, you'll have to run from an elevated prompt. Okay. I got my handy, dandy SAPIEN USB key here, so let's see if this fires. Yep. Cool. And now when I remove it, it shouldn't trigger anything because I've kind of filtered down on the event that filter a little bit, so where it will only trigger upon insertion. Okay. Alright, going on. Here is an example of how you do a permanent WMI event subscription. So here's what I want to trigger off of. It's the same query as before and then the event consumer that I want to use is I want to execute PowerShell. Alright, and what I'm going to do is I'm going to pass the DriveName here and this is how you specify it. So you're passing, an instance of this class is being passed to the event consumer and here I'm specifying one of the properties of that class, the drive name, and I'm doing something a little shady here. I'm Base64 decoding the Eicar string, which is like a test string for AV, so as soon as this drops to disk, in theory, AV should detect it right away. So I'm going to drop it to my USB stick without any user interaction. Okay. So just imagine that this would be something like actually malicious that would infect this USB stick that I'm going to insert. One thing that we didn't really touch on is that those local WMI events are only persistent within, I guess, the runspace or within that shell, and so once you close PowerShell out, those local WMI events are going to just disappear, and so that's why the permanent WMI events, they're going to last through reboots, they're just going to be there until you get rid of them, so that's why we're going that direction. Yeah. Demo-ville. Oh. Just to clarify, you said the intrinsic are volatile? No, the local, so when you use Register-WMIEvent. Just trying to think of how I can use this. Yeah, no problem. Okay. Alright, are we ready? So look in the bottom right of the screen and hopefully when Eicar.text drops, Defender will catch it. Oh, come on Defender. Oh, okay well it should have popped up, but it did actually catch it, so it won't even let me open the file because it's quarantined. So (Audience comment) Yeah. Good job Defender. You catch the obvious. Okay, now I want to show you how I would go about developing these signatures, so I wrote some helper functions to say I want to list out all of the extrinsic events within every namespace in the WMI repo. And so I would literally just start going through these one by one and see if anything sticks out to me. And there was one that stood out to me in the root\cimv2 namespace, which was this WmiProvider_ExecMethodAsyncEvent, and in the interest of time, I'm not going to go through all of this, but I will show you wbemtest and just show you that it's sort of my testing methodology here. So I'll connect to the root\cimv2 namespace, answer a notification query, and so say I'm not quite sure what is going to fire in here, but we'll just see what pops up. So I suspect that if I execute any WmiMethod that some instances would pop-up that I might be able to get some interesting information from. So I'll call the Win32_ProcessCreate method and also I'll pull a value from the registry using the StdRegProv class. And let's see if we get some stuff that pops up, okay? Okay, so we got two things here. Now I can double-click on one. There's a little bit of lag time between when you click on it and when you get this pop-up. I'm going to hide these system properties, so I can see the actual properties of the class here and what happened was, so here are some of the interesting properties. So I was notified that the Create method within the Win32\Process class within the root\cimv2 namespace was executed and what I found to be really cool was that it provides input parameters to that method within this embedded object. So I can view that embedded object and now I get the raw command-line of what was executed. So here's like ghetto, but super effective command-line auditing built into Windows, okay. It's pretty cool. One thing that we didn't mention also is that the Create method for the Win32_ProcessWMI class is one of the top lateral movement techniques that are used by attackers, and so, very frequently to get code execution on a remote system they will leverage that method, and so that's why detecting it is pretty cool. (Audience question) Lateral movement is a, if I'm an attacker and let's say I am able to phish somebody and get access to their system, I want to start spreading throughout the network to start enumerating and maybe get figure out where the domain admin is logged is, so I can steal their credentials. In order to get access to their system, I need some way to execute code on that system, and so that is that process is called lateral movement.

Signature Development Results

So once you've grown accustomed to like rooting out these like defensive signatures, here's some examples of what might be useful to defenders. So we just saw the first one here, although I'm going to filter it further. So this is what would detect that lateral movement that Jared was talking about. I'm interested in all method invocations of the Create method within the Win32_Process, so that'll give me the command-line context of what was executed via WMI, alright. Here if I was interested in any like remote WMI registry actions, this is what would trigger that. Now here I could do a Win32_ProcessStartTrace on powershell.exe, but as I was just saying to a gentleman up here, PowerShell is not powershell.exe. PowerShell could run in the context of any arbitrary process without it being powershell.exe. So this is the detection that, the signature that I came up with so anytime a module is loaded, so module being an exe, dll device driver, whose name contains system.management.automation.dll or .ni, which stands for native image, .dll, then let me know because maybe in our enterprise like we don't have admins running PowerShell everywhere so on those hosts where PowerShell should not be expected to run all the time, maybe I would want to be alerted upon that happening. Or if PowerShell is running an svchost.exe, then you probably definitely want to be notified about that. Yep. When I was talking about what HT29 was doing using WMI as a covert storage mechanism, they were creating their own custom classes. Here's a signature that you could use to detect that. Now you would eventually get some false positives say like you installed the latest service pack of Windows where there's going to be a lot of new WMI classes anyway, but hopefully, you'd be able to filter through those anyway because you're tracking when you're deploying updates. Lastly, this one's kind of interesting. If you were interested in maybe the attacker doing process enumeration using WMI, you would be able to detect that using this signature right here. So yes. Excuse me, on the left, I got them all, but on the last one, why is that important? So I wouldn't consider this last event to be super important, but say you're like thread intel has indicated that within your industry vertical, attackers really enjoy doing process enumeration using WMI, so this would be able to detect that action. So just by the nature of the attacker running say Get-WmiObject Win32_Process, this would trigger. Or if they use wmi.exe to do the same thing, it would also trigger, then you could be alerted. And so from like an auditing perspective, being able to know that somebody, what specifically somebody is using WMI for is very important. So like typically you'll see like wmiprvse.exe running when somebody's using WMI, but you won't actually know what class they're enumerating or what method they're calling. In this case, it would let you specifically determine what method or class is being utilized or enumerated. That would depend, the utility of that would depend on method that the attacker happened to use or use frequently. Sure, I mean you could make a very generic subscription and you would catch everything, but probably not necessarily what you want to do. Yeah, you don't want these signatures to be super loud. Again, you should filter as much as you possibly can. Oh, hold on let me set this to. So while Jared is setting up his portion of the demos, are there any questions that you have? What's the performance impact? So the comment was related to performance impact. So that's not something that we've like definitively quantified in our testing. All we have are like the qualitative results of the fact that, so Jared has deployed how many Uproot signatures in a large environment? Yeah so, that we have a client that is a Fortune 100 company that has 60,000 Windows endpoints domestically and we deployed Uproot to those, and so, and generally, we did some performance testing kind of like looking at the system that it was deployed to. Let's say we had 16 signatures deployed out and there was very minimal change from before to after, like literally we're talking like .1 MB of RAM being used and the processor doesn't even spike at all. And so, I don't know a like specific like we haven't done tons and tons of testing on performance, but from just like looking at it and kind of playing around with it, we haven't seen any performance with that. I can tell you one class to be extremely careful with is CIM_DataFile, so you will experience a performance hit with doing queries related to that, unless it's an extremely specific event filter that you're using.

Uproot, Permanent WMI Subscription

Alright, so building on Matt's use case for that lateral movement detection, he showed how we would use wbemtest in order to kind of detect what's going on for method calls. We want to go ahead and probably create a permanent WMI event subscription, and so the way that we would do that by hand, kind of like the old school way, I guess, is that we would have to go onto MSDN, figure out what the properties that are associated with an EventFilter class are, and then we would by hand create our event filter, right. And so in this case, I'm using a property hash table in order to allow me to pass arguments to the New-CimInstance cmdlet, and so, in this case, we have a name, which is EXT-ProcessCreateMethod. The EXT stands for extrinsic. We want the EventNamespace to be cimv2 and then this is our query, the same query that Matt was showing where we're looking for Win32_Process and the method name over here is going to be Create. And then the query language, in this case and most cases, is WQL WMI query language. And then we're able to store that filter once it's created in the filter variable. Alright, so that's been created. Next, we'll go, in this case, we want to write events to the event log. That's the ideal situation because you have things like Windows event forwarding to be able to centralize your event log. And so, why do some kind of like duct tape solution when you can just write to the Windows event log, which is the supported login mechanism for the operating system. And so, what we're doing here is we have an array called the template array and that's going to have the actual data that will be written into the event log, and so in this case, we want to have an alert that says lateral movement detected and then we want to store like hey the namespace, the object path, the method name, and the command executed and then we have the actual properties that the NTEventLogEventConsumer class needs. And in this case, we found that the WSHEventLog source is the best one and if you use EventID 8, in particular, you can write arbitrary data to the event log, so that's like the one EventID in a source that just allows you to write whatever you want, and so Matt I don't know how you figured that out, but. I was really confused by that. I never really knew how like event sources worked under the hood and I had a buddy who's really good forensics who kind of dug into that and explained it to me. So the EventID refers to a specific like template string entry within that event source, so that event source corresponds to, it's usually like a resource only DLL that within one of its resources is just a huge array of these template strings, and NWSH like the eighth string is just basically like a %S format string, which allows you to write any data that you want using that source and EventID. So that had been documented on the internet like this is what you would use to write arbitrary data to the event log, but it was never explained why, so that's why. Thanks Willy. Yeah, so while he was doing that, I basically showed the contents of the filtering consumer variables just to show that this is an instance of a filter, event filter class, and this is an instance of an NTEventLogEvent class, NTLogEvent, I don't know, I can't think of it off the top of my head. I'm now confused. Okay, and then what we need to do is we need to create the FilterToConsumerBinding and we're going to pass a reference to the filter and a reference to the consumer. And so, alright. And so, voila, now we have a FilterToConsumerBinding or a Permanent WMI Event subscription is the term that we're using for that. Okay, so now what we should be able to do is call the Win32_ProcessCreate method and we should get an event that's written to the Windows event log. And so, we're going to try that. Fingers crossed. This has given me some problems a couple times, but and it's a live demo. Alright, so now what we're going to do is we're going to read from the Windows event log using the source WSH looking for a message of lateral movement detected. There should be a couple examples because I've practiced this recently, but here is that cmd.exe. And so, in general, what we could do is Get-EventLog to kind of show that I didn't have those prestaged or -LogName Application -Source WSH and then we can do, what is it, Newest 1. And so that just got written at 18:34 and it's 18:35 on this system, so just showing that actually did in fact write to the event log just now. Alright, so we have now detected probably the most common lateral movement technique out there. Obviously, there's tons of lateral movement techniques, and so this is just one of them, but it's a really good start and there's not a detection mechanism for that I'm aware of in public. Right, exactly. Alright. So we're going to move on. Alright, so that was kind of painful, right, and in theory I would have been going back and forth to MSDN to figure out the different properties and things of that nature, so we wrote the WMI event module in order to kind of simplify this and extract out some of the complexities, and so we have things like New-WmiEventFilter, New-WmiEventConsumer, New-NTEventLogEventConsumer to be more specific, New-WmiEventSubscription. So let's take a look at this. It's also worth mentioning that Trevor Sullivan wrote the power event or power events module a long time ago and that's also a great utility. It also has an extremely useful whitepaper that shows you like how to register these things, how to use wbemtest, and use those notification queries for testing your event, so that's highly recommended as well. Alright, so what I'm doing now. (Audience question) It's just empty. Don't judge me. (Audience comment) Oh good. Yeah, no it's just empty. Classic June Blender question. I know, gosh. June, I have another module that I would like your help on in a little bit. So yeah, what I was trying to show before I got in trouble was that now what we have is we have the new WMI filter and there's a specific parameter for every property that you're going to need to pass to it, right, and so in this case, it needs a name, an event namespace, a query, and a query language, so I'm just kind of typing those out, Win32_ProcessStartTrace, so this is going to basically capture every time a process is started. And we've kind of gone away for this example from trying to do something fancy just to try and do something that we could trigger very easily, and so WQL will fill in automatically. There's that filter, it's called testfilter, in this case, so then if we do Help Get, this is a fun one, so whoops, not Get, sorry, New. Alright, so New-WmiEventConsumer. Like we said, there's five different types of event consumers that you could create and within each or different event consumers, there are multiple ways to call them, and so there's like 37 million parameter sets in this situation, and so you can filter through these and try to determine what you want to do. In this case, we know that we want to use the NTEventLogEventConsumer, so we can do New-NTEventLogEventConsumer, whoops, whoops, that's not what we want. Does anyone else know the pain of parameter set common notorial explosion? Yeah, that cmdlet or that function has I think like 400 lines of parameters, so yeah, that was a pain. And so in this case, NTEventLog has two different ways that you can use it. The first example is if you want to do it locally or use the computer name variable or parameter. If you want to use a CIM session in order to push it out to a remote system over CIM, then you can use the second parameter set. And so in this example, we want to use a name, a Category, an EventID, an EventType, InsertionStrings, which are what we want to actually write to the event log and then the number of insertion strings and the source name, right, so that's a lot of stuff that we would have to remember if this didn't tell us. Yep, that didn't work. Okay, so name, we're going to call it testconsumer -Category, we'll say 0, EventID. The things that actually matter in this case are the EventID has to be 8, and then EventType we'll say 2 I think as a warning. InsertionStringTemplates, I'm just going to write, what did I do, let's see. ProcessName, so we're going to say ProcessName Started. And so that should be written to the event log. Let's see what else we need here. The length, right? SourceName, yeah, WSH and then NumberOfInsertionStrings is a 1. So I just ran that. Now I have a test consumer, which is going to write ProcessName has started to the EventLog. And then, whoops, then what we can do is help New-WmiEventSubscription and this tells us how we're going to do this, how we're going to create a binding. I just kind of renamed it as subscription because that makes a little more sense. And so, we can do New-WmiEventSubscription. We're going to give it a FilterName testfilter ConsumerName or ConsumerType, it's an NTEventLogEventConsumer ConsumerName, and it's going to be a testconsumer. And so there it is. Now the subscription has been created. So let's go back, well actually, we just start our random process. Calc. Okay, so calc has been created, so now we can do Get-EventLog -LogName Application. It's not a security-related talk if calc isn't popping. Yeah, exactly. Source WSH. And so, there you see in the message, calc.exe started, so now we're monitoring for process creation. Process creation is obviously going to be a very loud signature, so you've just kind of got to take that for what that is. Let me go back here and kind of clear everything out. Okay, so that is the Generic WmiEvent module. Obviously, you don't necessarily for more complicated filters and things of that nature, you don't want to necessarily remember all of that, and so we've kind of made a mechanism for storing your filters and consumers and things of nature. And so in this case, we basically take that hash table and we store it in a file, and then we .source it, so now once we .source it, $props represents what's in that file. The caveat here is that you should before you .source a file, you should probably know what's actually in that file because you're running arbitrary code. So yesterday, the PowerShell team talked about how they're coming out with a way to like signature an entire module. That would be a very good use case here is that you're signaturing that none of the filters have been changed, basically. Or none of the filters or consumers. Alright, so then what we can do is we can just splat props, and so now we've created that filter, so we're kind of trying to make this a little bit easier. So Uproot has a bunch of these built-in signatures in the form of a hash table, which is then splatted to the functions that he just showed you. And so everything is known as props in the way that we've done it so far, so now I just created an event filter with significantly less typing and then we're going to, actually let's just copy this entire thing, and we'll run that, and now we've created the consumer. And so what this consumer is doing in particular is we're monitoring for a Startup command creation, so this is an intrinsic event. I believe right now it's pulling within 10, so every 10 seconds, it's going to pull and look for a new Startup command. And one of the reasons why 10 is okay is because if it's a persistence mechanism, then it's probably going to be there for longer than 10 minutes, if that's what we're worried about. And so now we can go ahead and create a value in the run key. And so, we're going to create a value called Jared with an argument of cmd.exe. So as soon as the system rebooted or in the case of the system starting up again, a command shell will be popped, and so I don't know if that's super cool, but it will get the point across. And so that was created and now we can check to see that it wrote, and as you can see, I've practiced this a few times before we did this, but in general, it writes out to the EventLog and says hey an autorun entry was added.

Generic HTTP Consumer, Signature Files

So now we're able to monitor for ProcessStarts, for lateral movement, for registry, autorun additions, but what if you don't have like Windows event forwarding working or you're a consultant like me and the company that you're consulting for doesn't have the time to set up Windows event forwarding, right. What we did is we decided to make this what I call the active script generic HTTP consumer, and so as you can see lots of fun VB script. I never knew that to escape a quote, you had to use three quotes or there's some weird thing like that. I have no idea. But generally, what we're doing is we're using, we're basically creating an instance of the Microsoft XML HTTP comma object and then we're using that to basically post data to a specific listening post or a CIM in some cases. So we've got it working definitely with Splunk, Elk, or Logstash and we've got it working with QRadar, which is, I think, I don't know who makes that, IBM I think. And so, what we're doing is we're taking that target event and the reason why it's generic is this works with literally any WMI, well not every WMI, but all the ones that I've tested it works with and it will basically create a JSON representation of that object and then post it out to your CIM, which is most CIMs are going to have some sort of REST ingestion, rest API ingestion. And then basically, we create the ActiveScriptEventConsumer with that script and tell it we want to use VB script as the scripting engine. So going back. Is there a reason you're not using like Invoke-WebRequest or anything like that? Because we're not relying upon PowerShell whatsoever. Yep. There's no reason you couldn't. Yeah, so we want to avoid the like startup of processes and because like PowerShell startup is a relatively expensive process when you're talking about monitoring every process creation and it will also create or even a yeah, an infinite loop. Alright, so what we're going to do is we're going to create a filter real quick. This is that ProcessStartTrace filter, then we're going to create this ActiveScriptEventConsumer and it's going to ask me my IP, so I'm just going to do it to localhost because why not. And so now you see like the IP was filled in. That's pretty nifty. And then we're going to create a subscription that binds those two together, so AS_GenericHTTP ProcessStartTrace. Next what I want to do, so right now I am literally every process that's created. I'm sending out an HTTP POST request to my localhost. This is really small. Can you guys actually see that? Yes. Okay. It's not like the details of what I'm about to aren't what is it called PowerCat, yeah. So a buddy of mine wrote PowerCat, which is a netcat implementation in PowerShell, and so we can listen on Port 80 and then start some random process. And so now we see that consent.exe, which is that what is it called, the UNC, I think, or yeah. That's the process that's associated with that, and so we see things like consent.exe, we see the ProcessID, ParentProcessID, SessionID, Time_created, so on and so forth, so this would literally work with any type of WMI class, in this case, we used the Win32_ProcessStartTrace class. One thing that Matt eluded to is that you have to kind of when you're looking at intrinsic versus extrinsic events, Win32_ProcessStartTrace is extrinsic meaning that as soon as a process is created, it's going to fire the event. Win32_Process can be used intrinsically with the instance creation event, but it's, you have a polling interval, so you could make that 1 second, but you could also make it a 100 seconds, and so you have a chance that you might miss processes being created. The Win32_Process class has significantly more information like the executable path and the command-line and things of that nature, so it's kind of a tradeoff which one you want to use. Do you want the one that's definitely going to fire or do you want the one that's going to give you more fidelity on the data. And so, maybe you might even use both. And so, in theory, there's more going on in the background, but netcat or PowerCat only will capture one packet at a time. Alright. How am I doing on time? I'm over, but. Alright, we'll skip over this part. In general, you can also enumerate WMI event subscriptions using WMI event, so you could do like Get-WmiEventFilter, Get-NTEventLogEventConsumer, so on and so forth. That's pretty simple. Okay, so we also created a Register-PermanentWmiEvent to where literally everything is done from one cmdlet, and so if we show the help of this, again tons of parameter sets, so you've got to kind of figure out how you want to use it and what. (Audience comment) Yeah, yeah. June, please help. And so what we're doing here is we're creating a, we're going to be splatting again, so we have a Name and MyFirstSubscription, EventNamespace root\cimv2, Query SELECT* From _InstanceCreationEvent WITHIN 1 WHERE TargetInstance ISA Win32_Process and then we're doing query language WQL. In this case, we're using a LogFileEventConsumer, so Register-PermanentWmiEvent has access to use all different event consumer types, and so we're going to write a file called temp\test.log and we're going to anytime a process is created, we're going to write that TargetInstance to that log. Does it automatically append? Yes, yep. Alright, so that's been created all in one-fail swoop. We can show that we have the event filter, whoops, there, let's see, and we have the event consumer. So now what I could do is start some random process called Notepad, right, that's a good example. We'll leave that running for now because remember this is an intrinsic event, so we need it to stay running for a bit, and then hopefully this file has been created already, but we're just going to get content, and so here's the contents, there's been a few processes created, right, and so Notepad.exe, scrcons.exe, which is an artifact of that ActiveScriptEventConsumer run-in. So that scrcons.exe is going to actually execute that VB script in the ActiveScriptEventConsumer and it does some work to like batch up those ActiveScriptEventConsumer hits. And then like wmiPrvSe.exe was running as well. And you could filter for those particular ones that you know were not a problem and are likely…? Yeah, exactly. Absolutely, yeah. You could build that in too. And so that's where you would want to like store off your filter so that you don't have to constantly remember how to type that entire thing. Alright. So there's that. Alright, one last thing before we're done is so when I'm deploying out let's say 20 signatures at a time, I don't necessarily want to sit there and type out Register-PermanentWmiEventSubscription and let me do this and let me do that one, and so on and so forth. So we created this concept called like Signature Files, and so what it is it's literally like a custom object that's going to have a FilterName, a ConsumerName, and a ConsumerType. This is assuming that in the filter's directory within the uproot module, you have a filter named EXT-ProcessCreateMethod and in the consumer's directory in the Uproot module, you have an Nt_ProcessCreateMethod, so there's a little bit of overhead here. It's not just going to magically do it for you. But that subscription is already defined out. I think there's like 12 subscriptions and all I have to do is say Install-UprootSignature -SigFile EventLog and this will, the SigFiles are in the Signature directory in the Uproot module, and so I could, let's see, I have two Basic and EventLogs, so there's like a dynamic parameter, which will look up your SigFiles for you, and when you run that, fingers crossed, oh well let me clear out everything. Now you get to see Get-WmiEventConsumer and Remove-CimInstance, so this is how you kill off the ones that you already have. I've done this a lot recently if you can't tell. Alright, and then we're going to re-run that, install, fingers crossed. Okay, so now it all installed. It kind of looks like it didn't do anything, but we could do Get-WmiEventFilter and there's 12-ish filters there, right, it's all just kind of magic, Get-WmiEventSubscription and we see all see all the different subscriptions, so we can enumerate that. And that is the end of my presentation. If anybody has any questions, I think it's lunch time, so I'm really to stay around and answer any questions anybody has and get in more depth. Yeah, we'll take them offline. Thank you. Cool.

Highly Available DSC Pull Server

Introduction

Alright, so how many of you have been to my other sessions? Okay, because if you haven't, you are really in the deep end of the pond at this point. This is the end, so to speak. And what we're going to do is we're going to talk about DSC. We've already in other sessions built pull servers. I already have a pull server. Pull server. Pull my finger, oh okay, no you're not going to do it, are you? And so, what we're going to do now is this is really the hard part that throws a lot of people off when they start with DSC is how do I get configurations on a pull server and how do I get resources on a pull server so that when a node goes ask that pull server says hey I want this config and the config needs additional resources, then it'll just download the resources that it needs. So I don't deploy these resources, it just gets what it needs when it actually runs. Now if you don't know this, here take a, use your phone, take a picture of this, whatever. Here's where all the code is. This is actually the entire code that I did for the PreCon. So this is start to finish DSC stuff starting out with how to write a config, how to manage the LCM, all that kind of stuff. Take a picture because I'm going to make it go away. So I put together a ton of code for you. What we're going to do is the last two modules and we're going to do it really fast because this is short session, so and also Morawski's little reading list on DevOps. It's an excellent reading list. And I think that's all I need to show you. So here's what it looks like when you get the code, you will get a zip of looks like this. Underneath a make a Scripts folder if you want to play with this at home and then copy DSCPreCon right there, and underneath DSCPreCon, these are all the folders of all the code and a ton of stuff in there, so you have a lot of fun. What we're going to focus on is the last two, so I'm going to go into PackagingConfigs and we're going to give this a shot. Everything starts with a demo script that actually walks you through everything one step at a time, so let's do this, shall we? Oh, that's not what I wanted. What am I doing? I'm in the wrong, ah. Now you have to do it all over again. There we go. Make this a little bit bigger. Oh, that's good. You guys see that okay? Awesome.

Configuring an HTTPS Pull Server

So here's the thing, when you make a MOF out, so you wrote your config, you run your config, your config makes a MOF. We need to change the name of this and there's two ways we can deal with this. First of all, I'm going to show you the old fashioned way, which Morawski and I actually still really like this way. Yeah, what? I like it too. Oh, you like it too. Okay, good, good. We're on the same page because this way works. Oh, this is not a good thing. So we need to change the name of this and here's what we're going to change it to. We're going to change it to a GUID, a GUID, that really long ugly number and we're going to then tell the nodes that we, the ones that we want to use that config, we're going to tell the nodes in their LCM what GUID number they're supposed to look for. So what this means is on your pull server, you're going to have 6,000 GUIDs out there for whatever configs you have. Now the cool part to this is it's really easy to manage. If I have 50 machines that need the same config, guess what I'm going to tell 50 machines LCMs? (Audience comment) Same GUID number. So I just have to have one out there and I can manage 50 machines with it, but it can be a little bit confusing, so let's go ahead and take a look here. First of all, the order or sequence I'm going to do this in is I'm going to start with the LCM of a particular machine, so let's do this. Here's what I need. The LCM configuration, let me drop this down a little bit, I need a GUID, so I've just got a parameter here where I'm going to pass the GUID. Now I'm just going to generate one and pass it into the LCM. I also need something else. I'm using a secured pull server and so I need the thumbprint for the certificate on that secured pull server. So there's two things I need for this. Now down here in the settings, I'm going to do ApplyAndAutoCorrect. I need to set the refresh mode to pull because by default it's push and this is where I'll put in the ConfigurationID and here's the interesting part. ConfigurationRepositoryWeb, this is how I tell it what web server to go to that's the pull server. So take a look. The server URL is the URL of your pull server and you need to put in the thumbprint for the CertificateID and set AllowUnsecureConnection to false. Pretty simple, right? Not that hard. Not that hard. So I'm going to run this. Oh let me show you down here. Now I got yelled at on Sunday. Here's how I make my GUID. Who wants to yell at me? Who was here earlier that wants to yell at me? Good because don't yell at me because I'm not going to. This is how I make my GUID. There's apparently, I never looked, there's a new cmdlet in v5 called New-GUID. So and here's how I get my thumbprint. I'm actually going to go to the pull server and this is kind of useful. I'm going to look at the pull server certificate store and where the FriendlyName is, whatever you FriendlyNamed your certificate, and then select ExpandProperty's Thumbprint. That grabs the thumbprint for me, so it's real easy. Kind of cool, kind of cool, kind of cool, kind of cool. So I'll run this. Woo! And theoretically, I'll get a MOF and there's the MOF. I could show you the MOF, but we don't care what's inside the MOF. It's a meta MOF, meta, meta, meta MOF. So I'm going to set server1. I've got two servers that we're going to play with. Server-1, I'm going to set his LCM. Now just to let you see here and I'll bring this up a little bit, you can get the information for the LCM and look at all this. It doesn't open up your configuration stuff, so when you're trying to troubleshoot stuff and you want to see what's in there, just expand property on that and it'll show you what's inside so you can troubleshoot including the thumbprint and all that kind of stuff if you run into issues with it registering. We good so far? Okay, so I have a node that has now been told here's the GUID I want you to look for when you go to the pull server I told you to go to. Are you with me so far? Now I need to make a config and name it with that GUID number. So I got a simple config. This is just going to install the SNMP service. It's not a, I'll show it to you, it's not a real big deal. So just something really simple. I'll run that. I want you to notice the name of it is S1.mof. We don't want it to stay that way, so I'm going to put it on a pull server. First of all, I'm going to get the GUID and a variable from one of the nodes that I had set. Boink. And now I'm just going to rename that file to that GUID. Boink. And I'm going to put it on the pull server, but before I do, I need you to understand something. This is tricky. This is also something that throws off people when they're trying to troubleshoot this when it doesn't work. What's going to happen is the first time it's going to work just fine, but at some point, you're going to modify the config, run the config again, rename it to the GUID, and copy it back out to the pull server and then you're going to wonder why your changes never go to the node. Got it. Here's the thing, we need a checksum file. So this will make a hash and no it's not Shaw256. I don't care what Steve says. We're going to make a hash and so you always have to make sure that every time you modify your configs, you make a new checksum for it and you put both of them up there. The LCM will know that there's been a change to the configuration by checking that checksum, so that's the thing that really throws people off when they do this. Now at that point, I'm just going to copy it up there to my pull server. I only have one, but I'm just going to copy up the GUID stuff or all that stuff and I just want to show you that it's on the pull server. Now this is the location where you put your configurations is WindowsPowerShell\DscService\Configuration. You're not impressed, but okay fine, be that way here. See them. See them? Oh, let me pull this up. Ahh. Okay, great you're applauding that. All I did was copy a file, right. Now I could sit here and wait and the LCM will eventually pick this sucker up or I can just kick him in the pants and that's what I'm going to do, so we don't have to sit here and wait all day, well at least 15 minutes. So I'm going to update that configuration. Nah, nah, nah, nah, nah, nah, nah. Now I can't that stupid thing out of my head. I hate that. Oh and by the way, right now he's running the config. Isn't that cool? So the node actually-- yeah. Yeah, okay you're not that impressed. Okay, but you should be.

Deploying the Pull Server

Now we're going to do it another way. A lot of people like this way. I like this way because here's the thing, GUIDs are hard to manage because you can't tell what's inside the config. In other words, you see a number and when you get 6,000 up there, well which one configures SNMP? I don't know. Now it becomes really hard. Now most of us have created our tooling where we put the GUID into a database along with a description of what it is and all that kind of stuff, so in other words, we're kind of writing our own version of Chef, but it's really easy to---did I say that word Chef, yes, I said that---but it's really easy to do your own tooling for this, it's just you're probably going to want it. So let me show you another way. This is a new way. This came out last April in the previews stream of things. I'm going to do the same thing I did before. I've got a simple config. I'm going to run the config. It's going to say s1.mof. Here's what I'm going to do, I'm going to rename the config, but I'm going to name it with something a human can understand. See it's going to be---you don't look happy about this, but you should be. So I'm going to rename it to RoleSNMP. I going to make a checksum for it because we always got to do the checksum. And then, da, ta, da, na, na, na, na, na, I'm going to copy it up to the pull server again. But now watch, looky, looky, looky, looky. Which one do you understand? Yeah, see how cool this is? This also brings up a security issue because when it's a GUID, nobody knows what's inside. When it's not a GUID and you use a human name, they know exactly what that config does. That could be an issue. That's why you probably heard the team talking about this is why we're encrypting the MOFs because of this right here. So, but it's easier to manage. It's really cool. Now what we have to do is we now have to do something kind of special. We need to teach our pull server how to work with this technology called configuration names. Now what Microsoft did is they're like, you can figure out what the configs are, so you can set any node to pull any config because it's English readable, we're going to set up a registration key. It's basically just a shared secret. We're going to put the shared secret on the pull server and we're going to teach the LCM what the shared secret is. Only if you know the shared secret---how can you ask a question about that? (Audience comment) Oh go ahead. I'm assuming that you do the name, the configurations, the pull server seem to be running on v5. Oh, I should have mentioned that. Thank you very much. You are absolutely correct. This has to be v5 deal. Config names, at least the last time I checked, did not work in v4. So and then also, then I guess it applies that this node also has to be running in v5. The node can be, as long as the LCM is version 2, so they updated v4 to have an LCM too. So the LCM could be a version 4 that's been updated. Again, a really great question. Guys, we need this registration key, so I'm going to create a text file with a registration key in it. Of course, what we want you to put in here is a GUID. Good. I'm just using some numbers so you can see that it doesn't matter what you put in here. So I'm just doing 1234567, I'll make this file, I'm going to copy this registration key up to the pull server to a very specific location, WindowsPowerShell\DscService. This is the root folder for the configuration and modules folders for the pull server and with that, thinking I don't have to prove to you that it's up there, but let's go ahead and prove to you that it's up there. There's my registration keys. Got it? So now all I need to do is teach the LCM and I'm going to pick on the computer too. I don't need a GUID anymore in this LCM config, so I remarked it out, so you can see it's out, but what I do need to do is teach him the registration key and the configuration name. So if you look down here, I set the ConfigurationID to blank because we're not using GUIDs and down here, RegistrationKey and what role or what config name do I want him to use. Does that make sense? It's actually really, really easy. (Audience question) Some day. Also, I want you, great question, because I want you to also notice the word configuration names and see my note that says there can be only one. Today, there can be only one. Perhaps in the future, you could actually ask it for multiple roles and it'll munge it down to a single MOF. (Audience comment) Yeah. So I'm going to run this. Whoops. Hang on, hang on, hang on. I screwed something up. What did I screw up? Connecting to remote server failed with username or password is incorrect. What the hell are you talking about? My invoke-command failed? Wait a minute. Wait a minute. Anybody see what I screwed up? Thumbprint because it's an empty string. Yeah, I know. Oh, come on. Invoke-command Computer dc. I don't think my, wait a minute, I know my pull server is running. Oh, I don't know why he's in the middle of a reboot, but he had, and the thing is, I have the LCM set on him to reboot if needed and apparently he needed. So anyway, back to. Sorry about that. Let's try this again. Hey, here we go. That's much better. So I'm going to now set that LCM on machine number 2 with that information and if we did everything correct, put the registration key up, name the configs, configure the LCM correctly, if we did everything correct, oh wait a minute, did I get another red splat? The underlying error is attempt to register agent, oh don't you do this to me. Seriously? Come on. I'll tell you what this is in a minute, what this error is. If I run update, it'll suddenly work. And mine's not and I'll explain to you in a minute why. Do you get the kind of concept though of using a config name? I've got good news and I've got bad news with config names. The good news is it's pretty easy to manage because now you can understand what the files are, it's easy to set up nodes with it, a lot of people really like this. I'm going to get to the bad news in a minute. Look at the bottom of the screen, see. So here's part of the bad news. If you want to use config names, you're pull server must be a graphical server. I only work on Core servers. I only work on Core servers. I only work on Core servers. I only work on Core servers. My pull server has to be a graphical-based server. The reason is that when they wrote all this up, there's some dependencies still on the graphical desktop. They are in the process of fixing it right now. As a matter of fact, the guy who is fixing it was here yesterday morning going I'm fixing it. Because it's kind of embarrassing, right? Your crap doesn't work on a Core Server from the team that made the Core Server and told everybody it's got to be Core. Okay, you don't see the hilarity to that, but I certainly do. There's another issue too. You are not going to reboot on me again. Oh, okay. Just whatever. There's another issue. I built my pull server and when you build a pull server, you need to get a module from the gallery called XPS Desired State Configuration. Unfortunately, that module has changed version numbers as they're trying to fix this configuration thing. Twice in the last few days. I have the latest one and the latest one is causing an agent registration error and that's the red error I got from server 2. You will get that error if you use config names today. Hopefully, 3.10.0.0 will ship soon and that will fix it all. Are you with me? I apologize. It's just it is what it is right now.

High Availability and Load Balancers

Kind of, sort of? Let's do resources real quick. Now this is supposed to have my beauty demo, but my beauty demo might fail because the one---I use config names and we'll see how it goes. Huh, what? (Audience comment) No, I don't want to do that. I don't have that much time. Well we'll see. We'll see the beauty demo is awesome. Some if it'll work. So let's go down here and let's talk about resources. Resources are more challenging. Before we go on, can I ask you a question? So is there any reason to use config names other than GUID? No. I like GUID names. I don't think they're a problem. Stick it with GUIDs then. Well here's one thing we all just now learned. GUIDs work, and quite honestly, like I said, Steve and I still prefer GUIDs, other guys prefer, yeah you prefer GUIDs, use GUIDs. All the GP guys always use GUIDs. There you go, so yeah, there you go. There's no reason not to use GUIDs. This was something they threw in because everybody was, you've got to imagine everybody that starts with DSC, they were complaining about the GUIDs because they didn't know what the hell a GUID was and they couldn't figure out what they were doing with it and there was no, they didn't think of, somebody said well let's make an Excel spreadsheet. No, don't do that, but you know. So that's why they put in config names. I like config names. It's really useful, but. Guys, I'm going to make a couple of, I've got a configure that's going to make some cool web servers. You're about up to here with DSC making web servers, aren't you? Yeah, I know. If you came to my thing on Sunday, we made domain controllers and stuff like that, but I know, it's kind of irritating. It's just easy to do. I'll show you this config here in, if you want to see it, I'll give you the details of it. This is just one of my configs. I separate my config data. It's going to build two different websites. I've got two different roles in here. It's going to use web as one of the roles, which configures a base web server with only the components that I want configured in it. Then one of them is going to do an open site, which is just a regular HTTP site on one server and one's going to do a secured site with an application and a custom resource. I'm pointing this out because you're going to be writing custom resources left and right. You've got to make sure you package them correctly for your pull server. So let me just kind of show you here. I guess I should run that because I'm going to need it. Oh, don't you dare. Oh. Alright. What was the problem with this the last time I did this? Could not find XDNS server. Oh, I haven't put the resources out yet. See this red squiggly line? that means my authoring box doesn't have the resources. That's part of my demo was to show you that. I saw that one earlier. I noticed the red squiggles. Yeah, I didn't even read my own comment that says yeah, look at the red squiggle. So here's what I'm going to do. I'm going to copy the resources to my authoring box. It goes out to Windows PowerShell modules and now the red squigglies, let me close this, and guys this is one of those things you have to close it and reopen it. (Audience comment) Yeah, yeah. It won't re, yeah that it's not refreshing. I saw that was getting voted up too, that's good. So now hopefully, my come on make the red squigglies go away. No. XWebAdministration multiple yeah, yeah, yeah, yeah, yeah, yeah, yeah. This is the problem I had yesterday. Find-module xwebadministration. By the way, this is something that will happen to you, so might as well. AllVersions. NuGet, NuGet I am up to here with you updating NuGet. Chocolatey NuGet. Oh. So here's the deal, a couple of days ago, a new version of this came out and that's what's breaking my demo. I need to be on 1.9 and it's currently on 1.10, so let me get rid of on my machine has 1.10, let me get rid of it and that's where I'm going to uninstall, uninstall-module -name xwebadministration. Did I spell that right? You did. Okay, good. MinimumVersion1.10.0.0. Let's try this again. No squigglies. Okay. Oh, don't. Golf clap. Anyway, so this is going to install. Let me just run this, guys, so we can start him off because he takes a few minutes to run and then we can kind of talk about him. Notice I got two MOFs out there, s1 and s2. I'm going to show you another trick because I had my cute little coding trick here and then Morawski yelled at me, so I'm going to show you, oh I'm going to change the names of those files, so let me just rename them. I'm using a config name for those and let me make a checksum for them. Ta-da. And I'm going to put them up on the pull server and here's the important part for this section. I'm just going to put the configs up there, but now here's the hard part. The resources have to get copied up there. Now they have to be copied into the DSC Service Modules folder, but here's the challenging part. On your authoring box, your resources are PowerShell modules. They have to be zipped and use a special name. The special name is the name of the module_it's version number. If you screw this up, it'll never work. You can't invent your own format for this name or the version numbering. Here's where you have to go for this. The version numbering is in the manifest of each resource. Now let me give you an example here. Here's one of my custom, this is my custom resource. I'm going to go into the manifest, see the module number? However that module number is listed must be the way that it appears in the zip name. People mess this up all the time. And I'm going to give you an example. I want you to see this because I looked at all the modules and got their version numbers. This one was 1.2.0.0. This one was just 1.0. It didn't have the other two, so it's got to match exact of what's in the manifest. Does that make sense? Now here's the beauty of this. Because that version number is in there, you can have newer versions of these modules. You can put them on your pull server. So you have version control on the pull server. Maybe you have 50 machines using version 1.2 and you want to put a 1.3 version of the resource up, but you don't want to break the 50 machines that are out there. You want to put a new version up and maybe put that on like three machines you want to test. The beauty to that is just astounding in production. There's only one small problem. How many of you know that class-defined resources, we don't have versioning in class-defined. So here's the deal, I don't write class-defined resources other than to entertain myself because I can't use them on a pull server because I can't version them, so I use standard resources. Anyway, it doesn't matter. Actually, this PSWA authorization is a class-defined resource, but I'm not putting another version up because it won't work. So I'm going to zip this. Zip, zip, zip, zip, zip, zip. And you need a checksum for those zips. That way we know if it's been changed. Yeah. (Audience question) No. You still have to put the version number in there that's in the manifest for the class. So that's actually really important. It's just you'll never be able to put another one on there until they fix this. (Audience comment) Because it doesn't know how to, it doesn't actually, the LCM doesn't know how to look for it with a class-defined resource. They know this is an issue and they're going to fix it, but this has actually been on UserVoice for quite some time. This has been an ongoing thing. And the thing is that in production is where you feel the pain. Literally, Morawski and I were going through this because we were yelling at each other back and forth because over skype one day because I'm like use class, use class, and he's like every version of a class-defined resource, I have to bring up a new pull server because that's the only way you can do it. Yeah. Ooh, and I'm like okay you win. Anyways, so let me copy these up and we'll run this and give it a shot, see how well it works here.

Testing the Pull Server

So I'm going to set the LCM for these guys. What I have is this is the way I usually do my LCM configurations, I separate the config data. You don't see this a lot on the internet. This is actually a really cool way to do it, so all the config data's up here for the LCM and then I can just tell it how many machines, whatever machines, and what roles I want them to have and I never have to touch this code down here again. So I just think this is a cool way to do an LCM. It's going to do multiple machines and here's where, I had this cute little line of code and apparently now in v5, you can just do this. Instead of the two lines of code, I'm going to show you, I'm just going to go bump, bump. Whoops. I did it to the wrong thing. I am a moron. This line of code. You just give it the path to them and it'll, all the ones that it sees there, it'll just kick them out. Well now I broke this update computername s1 and s2. Yeah, that's the one machine, yeah that stupid config name registration. You telling me it's not going to work at all? S2 is working. I'm going to try and see if I can get s1 to work because s2 just registered, so let's see here. It's going to take a minute for this config to run. I'm going to re-run it again to see if it picks up s1. But the idea here is besides the config name bug, we bundled and put the resources up. Here's the cool thing. One machine needs all four of those resources. The other machine only needs two of those resources, so we only bring down the resources that machine needs. We don't just copy them all down. Now here's what's going to happen, the resources must be available on the pull server when the LCM runs the MOF because the first thing he's going to do is look at the MOF and go what resources do you need and then he's going to go do I have access to those. So he's not even going to start jamming on it until he can get those resources down. I'll show you here in a second. Oh, that was awfully quick for you. That was way too quick for you. Cannot invoke, oh, it's still in progress. Okay. I'm going to do something I probably shouldn't do. You're not going to let me do it, are you. Well let me show you if the resources got out there and I can't believe this, but my beautiful, it worked fine on Sunday before they updated the module. No, it's not put them out there yet. Okay, let's do this. You're really messing up my applause line, man. Yeah. Yeah. That's just a shame because I don't think either one of them got set up because it was way too fast. Yeah, that's a shame. Well I'm sorry guys, but you don't get to see the pretty website. Did you even care about the pretty website? Of course you cared about the pretty website and I can't get it to work now! Ahh. (Audience comment) Well I'll tell you what. I want to make sure that we don't go over here, but maybe I can. Let's do this real quick. Let me see if I can, let's do this. Remove-DscConfigurationDocument to CimSession s1, oh he's only going to let me do 1, Stage pending. Now a lot of times you have to do this when you're troubleshooting something that gets stuck. Okay, he doesn't have one. Consistency check, yeah, yeah, yeah, yeah, yeah, okay. Now let's try to run it again. Maybe I can get him to go. Come on, do me a favor, just work. You're not going to do it, are you? Nope, there's something in that update that came down that he isn't going to do it. Oh, this says the checksum failed. Oh, maybe I can fix this. Hold on, hold on, hold on, hold on. Let's just make sure that I got everything zipped correctly and let's make sure I got some checksums. We'll make new ones. (Audience comment) Yeah, the force is overriding it. If you don't use -Force, it won't actually overwrite your earlier checksum. That's another little trick. And let me copy them back up there. And let me make sure I've got them all copied. You know, I may not have copied them up there, so we may still get to see a website. Now I'm setting your expectations and if it blows up, I'm going to look like a moron. Okay, there they are. There they are. On the pull server. Okay, let's try it again. Let's try it again. Cross your fingers, right now, cross your fingers. Oh well. Uncross your fingers. We're not going to get the website guys, but the moral of the story is that the packaging of those modules is very finicky about the names and I want to go back to this one thing on checksums. This -Force, nobody puts that in and they can't figure out why they can't get their updated modules to work because the checksum doesn't get updated. I think that by default, it should just overwrite it, but it doesn't. So -Force is like saying overwrite the existing checksum because what it does is it goes oh, you have a checksum, I'm not making a new one, which seems kind of ridonkulous to me, so -Force is very important right there to make him go just do it and life is good. Is it new? Because before I always deleted the checksum and then rewrite it. Is -Force new? Yeah. I think so because I used to always delete them too. Yeah, so I'm thinking that that's new. But I do know this, it doesn't work unless you have -Force. Ta-da! Is that broke, an error? No, it doesn't throw it because it's not an error. You say New-Checksum, it goes oh you already got one. That's not an error in his mind. I think it's an error. So yeah, you end up troubleshooting it for like three days because you can't figure out what's going on until you notice that the, you actually look at the checksum and you notice it's not getting updated in there. (Audience comment) Yeah, or if you think to manually delete it and do it. (Audience comment) Oh yeah, in verbose you can see it. If you read the verbose output, yeah you can see it in there, yeah.

Q&A

So what questions do you have? Come on, anything. Partial configurations. No, we're not talking about partial configurations. Next question. No, go ahead. Because with a partial configuration in pull mode, you have to use the friendly name and a configuration number, so are you stuck using the friendly names in that case? As far as I've seen, there's no way to do it using the pull mode right now. No, you're right. You need both of them right now. So I'll be honest with you, I'd stick with GUIDs if you're doing partials right now altogether and not use any config names. (Audience comment) And if I were you, and I was just about to say, if I were you, we have this whole big thing, matter of fact, I was just, you all know who Bruce Payette is? The great god of the linguistics in PowerShell? Okay, what we've been saying for days, we keep telling, we tell this to customers on it, don't do partial configs. There's only one case scenario for it and it's because you don't have an HR department capable of firing people. Don't use partial configs. They will hurt you. The right way is to use a composite config. That also goes through your build pipeline correctly and doesn't screw everything up. Partial configs, you will die a 1,000 cuts. So I was talking to Bruce Payette last night and I said what's the real, what's your view on partial configs and he just turned to me and went we shouldn't have done that. Thanks, Bruce. That's exactly what I wanted to hear. But yeah, so I know that there are case scenarios for partial configs, but please avoid those. Use a composite config. It just makes your life easier than running partials. What else you got? (Audience comment) Yeah, and you know what's funny is that is exactly the answer I was going to get. Your MOFs have to be versioned control by a version control source. They're not putting version control numbers in the MOF, so what you want to do is whatever your source control is going to be like Get, whatever in GitHub, that's where that versioning gets done. And I would dare say that you do the same process with your resources. That's going to be in version control. You just have to make the name of the resource follow that, you have to understand that versioning for those. That's a really good question. Are you guys going to use pull servers or are you guys going to come up with your own way of doing it? (Audience comment) It has. Right now, I feel like it either has to be both or I need to… Okay, so there's a lot of companies that do push, they build their own push environments. You're one of those. There you go and there's nothing wrong with that. They are working on this pull server. I don't know if you've looked at the Azure automation stuff. An Azure automation DSC pull server is a little bit more refined than our current on-prem pull server and they're going to make a concerted effort to get the on-prem pull server, they've just been focusing on Azure. Are you with me? You can use an Azure based pull server and let me tell you one of the benefits is we've got encrypted MOFs on the nodes. These MOFs aren't encrypted on your on-premise one. So somebody could just go hack the, just look at the ones on your and see things like if you forgot to secure your credentials and that kind of stuff. On the Azure automation one, that pull server has encrypted MOFs on both sides. It automatically encrypts the MOFs on the pull server and on the node, so that's kind of a benefit. If you can have that hybrid approach, that's a really good pull server option right now to use. This one, I use this in production all the time. While I'm not using config names this week, that was supposed to be a joke, but I use this all the time and they're working on improving it really rapidly, so. Also because, and I just want to tell you this, in the process of fixing some of this stuff, they've changed the database type from JET to ESENT. Does anybody know what ESENT is? Do I have any Exchange people in here? (Audience comment) It went from an MDB to an EDB. And now here's the funny part because I was asking one of the team members yesterday about this. I asked them, I said any Exchange guy that goes out and looks at this folder, let me just, is going to freak. Where do I have it? Dc\C$, Program Files, Windows PowerShell, DSC. Notice the old MDB is still out there because when they made this change last week, they didn't want to kill your database, existing one, but they made a new one. Yeah. Yeah, it's why you didn't notice it and I just needed see those things called .logs, here's what I don't know. Any Exchange guy looks at this, the first thing they go is Oh my God. And I said to the team member yesterday, here's the first question that should have popped out of somebody's mouth as soon as it, because mine did, was are you using circular logging? If not, how do I truncate these because there's no backup for this that's going to truncate these logs and every Exchange guy knows your Exchange Server crashes because your backups aren't working and wouldn't truncate the logs and it filled up all the disk space and it's going to do it in a heartbeat. So I'm trying to find that information out and I'll put that up on powershell.org when we have some more details on ESENT and how they intend to use this. I don't know if it's circular logging or not and I. (Audience comment) Well they could set it for whatever they wanted, so and that's what I don't know, so I'm trying to find that out. But this change has just occurred, so don't be surprised when you go out there and see. Oh, come on somebody had to watch Buck Rogers. Yeah. It's stage data. Oh, it was funny. Alright. What else? Have you tried exploring the contents of that? Not yet. I haven't had a chance. Because believe me I'm going to next week, but I haven't had a chance. Because this literally just happened last week and I went oh, I wonder if any of my demos are now going to work. (Audience comment) Yeah, that's another thing. Yeah, that's probably something else that we probably should get around to fixing at some point, yeah, or at least telling everybody that they need to make sure they delete that stuff. Yeah, that's really good. So pull servers. They're great, but we need some more work on them, so have patience. Play with it in your lab environment. Yeah. (Audience comment) I know. So was mine. Well because the engineer told me make sure that your web.config says ESENT instead of JET and I'm like what? What just happened? (laughs) Yeah, and I don't have many details on this. I think it's going to be really cool, but I don't have any details on this. The other thing is this makes high availability just a little bit more challenging because-- I don't think they have any block replication capabilities like a DAG set up for this. (laughs) That's my other question too. (Audience comment) Yeah, yeah. I think this is, from my initial set, I think this is probably a really good idea and those are really smart people on the team, so they don't do things haphazard, so I'm thinking this is a really good idea. I just thought it was funny because my Exchange eyes just went--wahahaha! Alright, guys. Well thanks. I'll stand up here if you want to ask any questions. Other than that, go to lunch, have fun, and enjoy the rest of your day. Thank you!

Package Management in PowerShell with Raghu Shantha

Introduction

Good afternoon ladies and gentleman, and welcome to the Package Management in PowerShell discussion, and we're going to talk about a number of things here today. First, I'll introduce myself. I'm Keith Bankston. I'm a program manager in the PowerShell team. My primary focus is on the package management, PowerShell Gallery, PowerShellGet and that area, and if you to talk about WMI and WinRM, I can talk about that too some other day. Not now. Also, up here we have Mr. Raghu Shantha. He's a senior developer, senior software engineer, I beg your pardon, in the same team that I work on and he's going to show you some of the things that he did a lightening demo on later and we'll do introductions about that. Quick review of what we're going to cover. For those of you who are familiar with package management and have an idea of the concept, put your hand up real quick. Wow! I like that. This is a good audience. Tougher than I thought. So I'm going to run through the overview material very quickly. It's just give you an outline of what it is and make sure that everybody is up to speed, but I'm going to run past that very rapidly. If you have questions about that, you can catch me later. Then we're going to talk about the things you know probably a little better. PowerShellGet, PowerShell Gallery, and then go a little bit more into the package management improvements, some of the things that we've done in Windows Server TP5 that's coming out, and some of the other changes we've made since WMF5 RTM has happened and also an EMWF5 RTM. And then we're going to put it all together and we're going to leave the majority of the time available for Raghu to talk through setting up a PowerShell Gallery, a private PowerShell Gallery, and the code that he did the five minute too-brief demo on yesterday. So what are we talking about when we say package management? The goal is pretty simple. There is one command that you can use anywhere in Windows that says install this thing, make it happen. A lot of folks are familiar with apt-get on Linux. You do apt-get something and especially on something like Ubuntu, where they've done a really good job of carrying in a list of things that's there, you say app-get foo, and if foo is in the list, you get foo. Well you have to do sudo apt-get foo. Now because we're Microsoft, we don't do apt-get, we do something like Install-Package because we think it makes more sense. Same concept, you've got the idea. We want to make it easier for folks to get stuff of a variety of types of installation mechanisms, use the same command approach to get it onto your box. Now in WMF5 and TP5, we've created the ability to abstract a lot of the Windows components and get those onto your systems and we've also added repository management, and we'll do quick demos of these things along the way. Package management though is going to become a lot more important when you start talking about Nano Server. You all have heard Jeffrey Snover's basic vision. Nano Server is this minimal box, and it by minimal I mean I've got just enough to connect to the box and do Install-Package to get the other stuff I need and control it that way. That's actually one of the key elements that we're trying to accomplish ultimately is by keeping Nano Server under your control, the set of things that are going to be installed there. So package management is central to that vision. And when we talk about package management, a lot of you guys, in particular, are familiar with this in terms of dealing with the PowerShell Gallery and we're pretty proud of it. We've had a few uses since we released that officially back in last June. But it's also about control and it's about giving you the control over where the packages are coming from based on the package type and that's one of the elements that we'll talk through.

Package Management Architecture and PowerShellGet

Package management architecture. This is stuff that you all should be fairly familiar with. There's a set of cmdlets. You can always use the install or the basic package management cmdlets to do this stuff, you just have to supply more parameters. So you can do the same thing that you can do in PowerShellGet, as long as you know all the stuff that we're feeding into the parameter set or you can use PowerShellGet and say and go find DSC resource and not have to do the filtering, but package management Install-Package will still accomplish the job for you. The basic core. We do the discovery, go find stuff based on the sources that are registered on your local box, then install or save it locally, and allow you to do some kind of inventory so that when you say what are all the things that I've installed from that type of installer, we can get a list of it for you. We have a set of package management providers already. Windows Server apps, that's kind of like an appx package type, alright, kind of. It's something that we need some differences from that in order to support Windows Server components, PowerShellGet, Windows containers, NuGet, there's a bunch of them that are actually available for you. And the package sources. Every one of those package types can support more than one package source and I'll show you that in a minute. So PowerShellGet. Is there anyone in here who has not used PowerShellGet? Okay. We have developed the PowerShell Gallery in order to support people sharing stable code. That's the goal behind it. It's not stuff where you put test stuff out there. When you've got something that you're actually willing to share with the community, it's relatively stable, we want it out in the gallery, but you don't actually download something directly from the gallery. You use the gallery to go find things, to find out who did it, to find out information about the stuff that's there. You use PowerShellGet and the cmdlets that are part of PowerShellGet to acquire that thing, you can also do find, and we'll show you that in just a second, but you're going to acquire the package, you're going to put it on your local box, save, take a quick look at it, always do a save first, investigate, in God we trust, nobody else, then when you are comfortable that this is something that you want on your box, then you do an install module. When you do an install module, we'll keep a list of the things that you've installed and we'll get rid of it for you. This is all the PowerShellGet specific versions of package management, and as a part of delivering this in Windows WMF5 and these previews, we made sure that we supported side-by-side module versioning and dependencies, and those two go hand in hand. Part of that is when I write a module and put it out on the PowerShell Gallery, and this gentleman here takes a dependency on my module, he can build that into his module and say I want to be able to know that I can get that version of that module to go with my code. And anybody see the article about MPM about two weeks ago? When you put something out on the PowerShell Gallery, we don't let you delete it today and if you want to have access to the article about MPM broke the internet, you can look that up. People ask us about this. The point being, I can take a dependency and he can take a dependency on a specific version of a module. But then what happens if I want the latest version of the module. Well I have to be able to install the latest version side by side. It's why we have support for both elements. We've added script support, we've added support for internal galleries, which is what Raghu is going to show you the ability to say I want to get all of this stuff from something I've tested and we've also made some other changes. There was this comment out in the group that when you used PowerShellGet, we had a -version alias. -Version works fine for install-package. Install-Package -version gets you at least the version you specified it mapped to minimum version. The problem was that when you did the uninstall version, it mapped to that as the minimum version, so it would also get rid of everything newer than that. So we've made some changes, we've gotten rid of that alias, and now you get to specify minimum version, maximum version, required version again. You guys told us this thing's evil, we fixed it. We've added credential support, we've improved proxy support. People got to us, told us these things aren't working, so we've make some changes in the system. Commands that are available in PowerShellGet, well mostly it's about how you manipulate modules, how you manipulate repositories, and then also how you manipulate scripts. There's a lot of concern about well we're going to open this door and everybody is going to put scripts out there instead of modules. We're being very explicit, folks. We want you to put modules out there. Why? Because modules include documentation, they include tests, they include examples, these are the best practices for things that you need to put out into the PowerShell Gallery. There is a place for putting scripts out there, we have support for it, but we prefer that you use modules. Alright. So---

PowerShell Gallery

Little bit of commentary on how we've been doing with PowerShell Gallery, I said you all have been working with it and adopting it fairly well. As of, I guess it was Mid-February, we passed 1 million downloads. We're at 1.4 million now. It's growing rapidly. The statistics page shows this hockey stick of growth and if you are familiar with it, that actually says we're very stable. So as of about Mid-February, we realized we are stable enough plus we had implemented the support for antivirus scanning and also we've built in script analyzer scanning to make sure that the code that's coming into the gallery meets a basic bar, so people can trust it. A lot of what we focus on is ensuring that the features that we're adding are about trust. The people who publish, we have about 700 people who've registered to publish, okay? We have about 700,000 people who are downloading. You guys can do the math. We will focus our energies on ensuring that the people who are requiring content that experience is optimized and so there will be times occasionally when you're going to come in say well that seems like you're putting a little extra burden on the publisher that's because we want the things that are out on the gallery to be something that somebody can look at and make a trust decision. Yes, I trust it, no, I don't. Some of the lightening demos that you saw yesterday including the viewing scripts, including some of the other things that we show here, those are being put into place so that people know I can trust this content. We're also adding in integration with Azure automation and some improved filtering and I'll show you that UI in just a little bit. So real quick, let's do some demo because otherwise you guys will kill me.

Using PowerShellGet and PowerShell Gallery

So basic PowerShellGet functions, and boy that's just in a bad spot. Sorry, folks, I've got to figure out how to use the mouse with this thing sitting here. So Get-Command, this is all the stuff that's available in PowerShellGet and I've already done this. Now one of the things that you will see here is you can do some filtering in PowerShellGet in order to find the things that you're looking for and a lot of people don't know how to go about this, so I thought I would spend just a minute and show you how to do that. Right now, we have a number of things that are out in the gallery that talk about the ISE, right. So which one is actually the one that came from Microsoft? Well, looks like that one is, PowerShell ISE preview. So if you go ahead and you have to do the filtering the way that you guys are familiar with, it's Where-Object and author, and I'll publish all this stuff. You're going to see a lot more of our documentation increasing here, but this is one of the most commonly asked questions, how do I find things in the gallery based on the author? Okay. How do I find something in the gallery where the author is Amazon? So the point being trust, you can go ahead and you can locate the things that you care about in the gallery based on who it is and whether or not you can trust them. One of the other elements of trust is, okay, what are the things that this depends on? I already told you that I can take a dependency on your module. Well one of the things that this HistoryPX module, which by the way, seems like a pretty good module. I skimmed through the code the other day. Did anybody in here write that? It's Kirk right? It's Kirk's. I didn't see Kirk so alright. So he's got a set of dependencies that are available on that module, right, and what we talk about is you need to know what it is that you are installing on your system. I can do a save module and all of those dependencies will get installed onto my box and you can read through that here as the verbose commands go by, you see SnippetPX right there is one of the elements that was a dependency, and we will install those dependencies, just like I said I can take a definite dependency on something somebody else wrote and it'll get installed using PowerShellGet. So again, these are all about what can I trust and that's the main focus that we have for the gallery, alright. So now let's take a look at what's actually in the gallery. One of the things that you saw a little bit of UI about is we've had a number of questions in the last little bit about where, how do I find all of the modules that don't have DSC in them. Did you all see the lightening demos yesterday? (Audience comment) Okay, well this is one of my favorite pieces of UI because it responds to, I've got 181 items in this test gallery, and know I'm going to try hard not to show you that URL because that's our test gallery. Yes, it's live. Don't use it if you discover it. It's there and we will blow up your stuff if you put it there. But if I want to see just the cmdlets and just the functions, the things that contain just the cmdlets and functions, this new filtering UI, which will be out in the next few weeks is something that you can go ahead and take advantage of. And again, it's trying to make sure that you can find what you're after, alright. The only piece of searching that you can do, or the only way you can do searching for owner, and owner is the actual person who published a package, is by using this UI today, okay. you can't find it in the search functions in and you'll find module or find DSC resource, so this is a quick tip. It's the only one of the demos that I'm going to show here now in the interest of time. But if you want to find out everything from PowerShell team, or from Kirk, or from anybody else, the best way to do that is in the gallery, you search for owner and then the owner name, alright.

Package Management Improvements

And yes, I'm running through this kind of quick and my apologies. Package management improvements. Main things that we've added. Support for containers. Nano Server optional packages are coming in TP5. Windows Server packages, again, coming in TP5. The ability to deploy something onto the Nano box when TP5 ships in the next couple of months, you should be able to try this out. We are very much looking forward to your feedback on how well that works. Multiple package provider versions side-by-side. This is something that will start to count more as you write your own package providers. A package provider is I'm going to take a blob of goo that is going to be put onto my box, unpacked somehow, and run through some process at the end of which it's installed, right. Blob of goo gets put on my box package, blob of goo. Alright, fine. I can have multiple versions of a package provider that I use for different things and I do this with my own PowerShellGet package provider because I have the internal gallery and I have the external gallery and I have both of them registered and both of them tested locally so that I can work back and forth between them, between the dev environment and the production environment. Event Log Support. Well if you're going to do something on a machine, it would be nice to know that you can actually see what happened and we've also again we've gotten rid of this nasty -version thing. We've created the ability to install the NuGet provider locally and we've added credential support. So let's here's the set of package providers that I've got on my local box and you can see that I've got MSI, MSU, NuGet, a bunch of things that are present there and different package sources and this is where you start to see I've got two package sources, don't write that URL, but the point is, those package sources are there and this is one of the tools you can take advantage of. A lot of people are asking when I create my internal gallery, how do I control and make sure that the systems only use that internal gallery. Well the way that I do that is I register the package source for PowerShellGet to be the internal repository and I remove the default PowerShellGet registration for PowerShell Gallery. Now when I use PowerShellGet, all those cmdlets will only go to your internal gallery. So package source, Set-PackageSource, and Get-PackageSource, this allows you to manage where you're getting things from. One of the other things that you need to be aware of is we're not the only people who are publishing package management providers. It turns out that there's a few other people who are putting things on the gallery as well. You'll notice that the GistProvider is out there, and we've got an example that I'll show you here in just a minute, that'll walk you through how to create your package management providers. JQuery is one of the NuGet packages that's available. The problem with jQuery is that it's available on more than one location. Okay, so there's something I want to show you here and I'm going to back this up just a little bit. If I do a Find-Package jQuery, the first thing that happens is I check all of the package locations that are available that are registered on my box, okay. If I want that to run a little bit faster, I either use the source or the provider name in order to find the one that I'm after. Then, I can go ahead and save it, install it, and manipulate it. So one of the other elements that I can do is I can take advantage of the fact that I can handle all of the installed versions of a module, so I installed two versions of Desired State Configuration from the gallery, alright. I want to be able to say make sure that I only have the right version of my module present-- Oh that's right, version's gone. Gee. We got rid of version. Alright, you guys are all going to make the same mistake I just did. You're going to try to use version. The reason we got rid of version is because I've got two versions of X Desired State Configuration there. Right. I don't want to have version, treat it like minimum version, and delete everything off my box. Use required version when you want to use that parameter. Okay. Now one other thing that people have been asking us for is the ability, can you see that font there or do I need to zoom it in. It needs to be a bit bigger. Okay. Visible? Yeah. Alright. So get the PackageProviders available on this box. Well it turns out this box isn't connected to the internet. It's not connected to anything. It' a little VM that's local. One of the things that folks have asked us for is the ability to say how do I deal with packages and with a NuGet installation. Every time you use PowerShellGet, the first thing that happens is I get this you need to install NuGet warning. You guys seen this? Yes. Yes, you have. Okay. So I have a local copy of this thing available right here. As of the latest version of the package management, I can do Install-PackageProvider and it will go ahead and update it and put it on the local machine. You'll be able to get rid of those warnings forever. So in your sequences for your installations, you're going to be able to get the NuGet provider that you want, put it on a local share, treat that as a local source, install it, you get rid of that nasty message so that now I have 1, no more funny warning messages such as you saw before, right, all those funny warning messages are gone, and 2, I have the package, the NuGet package available. Okay.

Authoring a Package Management Provider

So there's only one more thing that I really want to show you and that's authoring a package management provider. Jennion, who's sitting over here, wrote this example back in November or thereabouts, October. If you go out to github.com/OneGet, which is also oneget.org, you'll be able to find the MyAlbum-Sample-Provider. If you take a look at that, what it implements is the ability to copy png files as a package, which is it's cute, but what it does is it's a very good example of the things that you need to implement in order to implement your own provider and why should you care. We talked a lot about giving you control. Let's say you take the PowerShellGet package provider and you want it to be just a little bit different or you take the NuGet package provider and you want it to be just a little bit different. You want it to work with your own local repository. You want it to use credentials that are available on your own local repository and required. You would make a modification to that base code and update it to fit your needs. So going back to my other window, the thing that I'm going to point out here is there's a couple of required things that you have to implement, and unfortunately, these things don't really do anything. These are the registration elements that are required. You've got to implement them because otherwise package management doesn't know your package provider is there, but that's not where you do the work. Okay. What you really need to take a look at in this example is this set of optional elements that actually do all the work. The Get feature Get-DynamicOptions. DynamicOptions are the things that are the optional parameters, the things that you can supply. Everybody who is going to write their own package management provider will take advantage of this. This is where you would maybe include something like credentials. Most likely, if you're going to be building something around credentials, it wouldn't be optional, but you can see where that fits. The other thing that you're going to want to handle is dealing with a package source, so the add package source and resolve package source are critically important so that you can know where to go from this package management provider to get something. So you stand up your own internal repository. It's a NuGet-based repository. You're storing stuff out there and you're going to require credentials. You'll set that up in your own version of the NuGet package provider, you'll release it as MyNugetPackageProvider, you'll resolve the package source to your internal NuGet gallery, and require the credentials, okay.

Concepts Relating to CI/CD

So take a look at this thing and I'm going to recommend that you go through this code. It's actually pretty simple, straightforward, and easy to follow. If I can, you can. Trust me. Okay. So, last element, how these concepts relate to a CI/CD pipeline. And I've been running through this stuff pretty fast. This whole week is primarily about how does this fit within DevOps. Package management is here so that what you can do is you can say I know where to get the things that belong on my box. You can set up your internal repository, you can register as long as you understand what the package source type is and you're not modifying it, you can take the existing package type, and say the source for all packages of this type are here. Everything that I've validated is here. All of my servers are only going to go here to fetch something. Then your release pipeline can make sure that everything gets published there. Your DSC resources when they run will only get things from there. Alright. So this is all about ensuring that you have the control to drive the system the way that you want to. So now, what we're really going to do is we're going to dance here. Raghu and I are going to swap out machines real quick. And I'm going to let Raghu show you how he put it all together for the internal repository of private PowerShell Gallery repositories.

Private PowerShell Gallery Repositories

Hi. So I'm going to talk about a couple of things about how do you set up your own private gallery in your enterprise, for example, and how do you populate your gallery with the things you need, and I know how we can make sure all the machines in your enterprise can point to this gallery and get all the work done. So this will be mostly a technical deep dive of the configuration and the DSC resources and all the things which make up the system. So if people start want to I mean look at the code that's on PowerShell GitHub page, so if you go to PSPrivateGallery, you can take a look at the modules of the configuration. So there are a couple of high-level configurations, which are self-customer facing. One is the ability to deploy our own private gallery, which is this configuration. So essentially, what it's doing is it's setting up your front-end, which is IS, in this case, it's setting up the database and it sets up all the Entity Framework later aspects. So you could take a NuGet gallery code base and plug it in here under and it should setup a NuGet-based repository. So it's got four high-level steps. So what I'm going to do is kick this off first, and then explain as to the different components of the configuration itself. So let's go ahead and run this on the machine. So if I look at my IS, I don't have any websites set up. And I'm going to run this configuration to set up my gallery infrastructure. Does ISE need to be installed first? This does all that, so you can take a brand new VM auto machine and just run this configuration. It's going to install all the dependencies, it's going to add on the Windows features, it's going to do everything for you. (Audience Comment) Yes. So wherever you got IS, a set of magic cmdlets, and DSC configuration. Then our composite resources some are class-based resources, so I'm going to go through the code. So I just wanted to show you how quick and easy it is to deploy your own private gallery. This is a just a deployment phase. We haven't gotten into the populating phase yet. So let me go back, yeah, so in about 60 seconds, which is about a minute, I was able to deploy a private gallery. So let me go back and refresh and yeah, it's set up your own private gallery and it's running under a user, which is low privileged, in this case, it's a non-admin user called GalleryAdmin. That's app that running on there, so let me go back to my browser and try to browse this gallery. I set it up on Port 8080. So the first time it has to start all the IS servers and it takes a couple of seconds. Yeah, so there it is. So I just changed the branding information to be more generic. You could put your own branding. Can we add certain communication to this? Yeah, we can do that. It probably needs a couple of other steps in the configuration, but I think as we evolve the project on the GitHub page, we will add all that capability. So yeah, this was just a proof of concept to show you that you could set up a private gallery. The only things I have enabled on this page is the ability to discover modules and publish modules and that changed the branding information and the ability to set up local users, so this is the deployment phase. I was able to set up a big blown gallery in about a minute and you should be able to use package management cmdlets, the PowerShellGet cmdlets to actually interact with this gallery. So let me go ahead and do that. So let's do all the part one first where I talk about how the gallery is actually deployed. So this configuration is controlled by this configuration data and all we are doing here is we are consuming a couple of credential files to set up the gallery itself for the app pool and you can also control the name of the website and where you want to deploy it and where to pick up the branding from, and also the database itself. In this case, it's a one box scenario. I'm installing SQLExpress on the same box, but you could change this to use full SQL on a different machine and you can use a connection string to point to the box. So part one is setting up of the web server itself and that involves a couple of things. One is installing all the dependencies and installing the website itself and this is actually a composite configuration. (Typing) And all of them are shipped in the PSGallery module and we make this available on the gallery soon. And the configuration I want to show is the configuration to set up the WebServer, which is this. And let's actually go and open that. So this is a composite configuration that sets up the front-end, and as you can see, it actually does a bunch of things under the hood, things like installing all the dependencies, setting up the users, and setting up the app pool, doing all your final copies for the branding and the gallery content itself, setting up the website and all that. So all the complexity is collapsed into this composite configuration and what you see on the front is just one single configuration to just do the WebServers. It's similar for database. So I'll quickly go over the database resource, which is this. So this is actually installing your SQLExpress package itself. So we can take an empty VM and kick this off and it's going to do everything for you. It's going to set up your instance, it's going to set up your database, and it's going to use the credentials you supplied in the manifest of the configuration data. So these are the primary couple of resources to set up the front-end and the database. There's also the migration part. It's called migration because the NuGet uses Entity Framework and we are actually creating this key-minded database. We are populating with empty tables and all of that there. And some of this is specific to SQLExpress and you could plug in your own migration logic if you want to deploy a different kind of repository and those pieces are separated out. Are we going to be able to figure out how to do that credential piece? The credential in the database? Yeah. Is there a way that you can do that, Clixml and that stuff? I just use a credential object and that's where the Clixml… Exactly, so before you kick this off, you can take a credential object, something like this, we can say $cred =, so $cred will have your credentials and you can actually export this into a file basically serializing it to a file. So the password is a secure string, and the username will be recorded as this. And actually, in the configuration, we actually leave that file here. We actually import it and then use it. This is one we are supplying configuration if you wanted to make it more secure, you can avoid using the, you can use actually source. Yeah, so let's go ahead with the rest of the things. So that's our database resource and not the migration part. Let me quickly go over the migration part where we actually populate it with the schema. And it uses actual connection string to the database. This is actually a MOF-based resource. So this is part one where we bring up an empty instance of the gallery, which is this. So now we'll kick off part two, which is populating the gallery with certain modules and then we'll talk about what that's made up of. So in part two, what you're actually doing is you're populating your local instance or your private instance. Okay, I don't have internet, that's why it's failing. You're populating your private instance with the modules from the source gallery. In this case, I'm actually seeing that my source gallery is the public gallery and my destination gallery is one I created privately, and I also need to know what modules I want to put in the local instance. So let's say you need in an enterprise you have dependencies on certain modules and certain _____. You can call up all that here. Just an example. And this publishing configuration is actually very interesting. It does a bunch of things. It actually just starts your source and private galleries first so that it knows where to pick up things from and where to put it to, and it sets up a user for your gallery, so you can do all your user management using a DSC configuration. The user means this user here. So you can manage local users, so we will add that domain resource later. And the last part is the publishing pieces where the configuration actually takes the specified modules from the source and publishes it to the destination. And we're actually using the latest and greatest features from WMF5 and Windows 10 to do this, so things like RunAsCredential. So all this is running out of the context stuff, the app pool user, which is low-privileged user. So none of these run as high-privileged or a system context. So we are very careful as to run it under. And let's quickly go over the code for the publishing pieces. So for this, it's actually a class-based resource and there are two resources. One is the gallery module, which does the publishing and one is the user management resource, which is right here. So you can create a local user and you can manage it. And for the publishing piece, we are actually using the public interfaces of the private gallery we just set up, so they're actually using the package management cmdlets and PowerShellGet's cmdlets to accomplish that. So we are using things like Find-Module to discover what you needed from the public gallery and actually Save-Module to actually take the module and put it on the local machine. We don't install anything on this machine. We save it in a temporary location and then we publish it using the public interfaces of the private gallery, so we go to the publish interfaces. Yeah, if I had internet connection, I would have showed you when you populate that, you can actually see these counters going up, but I don't know if you have time for that. (Audience comment) Yeah, so all of this is DSC-based, check it out. The whole code is on GitHub on PSPrivateGallery. I already see a few issues being opened for requests, so we will take a look at them. And this is actually a community-driven project, so if you guys have ideas, if you want to contribute, please do so. We're taking full requests then. Thank you all very much.

PowerShell as a Service: Managing with Microsoft Operations Management Suite with Eamon O'Reilly

Introduction

Alright, so we're going to talk about Windows PowerShell as a Service today, managing clouds, Windows, Linux, on-premise, or public by using Microsoft Operations Management Suite or OMS for short. I am Ed Wilson, and I'm sometimes referred to as the Microsoft scripting guy. I also write a blog called Hey Scripting Guy that's read by quite a few people and I've started, as of January, the OSM team blog and I'll show you a link to that at the end. If you haven't been reading that, that is the absolute place to go to find out everything about OMS. And I am up here with my friend from the OMS team, Eamon O'Reilly. Hi everyone. My name's Eamon O'Reilly. Thanks, Ed. I work on the automation of the service inside of Operations Management Suite, and so we'll kind of walk you through what that looks like today mostly focused on the PowerShell capabilities. Okay, so what are we going to talk about today? We need to have some objections, not objections, objectives perhaps. Alright, so our session objectives and our takeaways, we're going to talk about why you want to bring what does PowerShell automation give to the cloud and we're going to talk about what the core capabilities are that our automation services provide. We'll show you a nice slide about that. It is Operations Management Suite and the idea there should think of well hmm, it's a suite of product and it really is, and so what we'll talk about that how all that works. We're also going to yeah everybody in this room is probably familiar with the very famous PowerShell manifesto. I was actually talking with a couple people from the PowerShell team right before I came in here and PowerShell has been around for 10 years. It's amazing. What's really kind of trips me out a little bit is when I'm reading some blog from somebody, oh well we're using this new thing called PowerShell. It's like dude, yeah, but yeah so 10 years it's been around. Well one of the things that's really cool and Jeffrey Snover actually republished the PowerShell Manifesto, I think, last year just kind of as a retrospective, but one of the things that he laid out in that Manifesto is that we honor your investment in PowerShell and they've continued to do that so that yeah stuff that you wrote back in what is PowerShell 1.0, it still works today. Yeah, there may very well be some better ways of doing it, but it's still works today. It's an amazing level of background compatibility back over 10 years, but not only we're talking about that you can leverage or that we respect your investment in the code because to be honest, like oh man I don't want to look at some of that code I wrote 10 years ago, it's embarrassing yeah. But certainly, I mean, we always learn and I've written more than 5,000 PowerShell scripts and stuff I wrote even last week, I think of new ways of writing it better today. But so what am I talking about leveraging that investment. When we came out with your functions, and then we came out with workflows, we respected what you had learned from writing functions and writing workflows. Then when we came out with DSC, we respected what you had learned from working with workflows and from functions and modules and all of that stuff kind of respects each other. Well with automation and with moving PowerShell into the cloud, we still respect that and allow you to leverage what you've done. So as a very, very simple example, all that time you spent learning workflows when we introduced it a couple of years ago, dude, it applies, and a matter a fact, dude, it applies big time, so you did not waste your effort learning workflows. But anyway, I'm sorry, we're still doing objectives and takeaways. So integrating automation in OMS, Azure, yeah on-premises environments, so we're going to talk about how we tie all this stuff together. We're going to talk about how we solve some very, very real problems. Some of the problems that as a person that's been scripting for more than 25 years, questions that I've heard over and over and over again for the last 25 years, we finally solved them. So we've got some very, very real solutions yet to some of the problems, and so we're going to talk about that. The key takeaway. We are deeply invested in PowerShell, in DSC, and all that effort that you have spent in learning this technology, we're going to allow you to apply it. And as we all know, PowerShell automation, we're heavily dependent upon the community. That's why we're here. That's why we can have great people like Eamon take time. On the OMS team, we ship about every three weeks. So he's in the middle of a release cycle, but. There's always one now. He's always in the middle of a release cycle. But yeah, so he's here to talk to you. Definitely we want to talk to you, find out. I think that we've hit a home run with this stuff or at least a good double, but a stand up double. But let us know. We're here, we want to hear your feedback.

Automation Trend

Okay, I'll take this slide. So one thing I wanted to talk a little bit about is why we went down this path of bringing kind of PowerShell into the cloud and what were the reasons we thought about as we went down this path. And I think the first one was-- we've seen a lot of customers, probably a lot of you are starting to look at adopting cloud technologies and it might be that most of your stuff is on-premises today, but you're also trying to say how do I take advantage as I move some of my services and applications to the cloud, how can I also think about moving my management? And so there's kind of core capabilities that the cloud gives you and one of those is you need to be able to integrate into all these different clouds now because you're not going to get rid of all your on-premises stuff as you start to bring things into Azure or into AWS or even into your own service provider. So you want to be able to like talk to all those systems and still Integrate them today just like you might do on-premises. The other thing that's been going around for a long time in the automation space is how do you deliver self-service? Most of us are involved in writing PowerShell so that we can kind of complete some process, either a small process on our own server or large processes that go across an entire organization and we think about that and one of the main goals we have around that is so that we can offer out that automation to our end users so that they can actually accomplish their tasks when they need it. And so as you think about the cloud, it just drives the self-service capabilities. That's really what cloud is about is I want on-demand virtual machines, I want on-demand storage, SQL, it's really about that self-service capabilities and how do we deliver self-service around PowerShell as well as we go forward. And then the last thing that I think if you look at the conference and where everyone is focused right now is a lot on DevOps. How do we really enable the kind of continuous deployment of our services and applications, working with the development teams, with the operations team, and allow that to go seamlessly back and forth as continuous development happens. A lot of people call this like infrastructure as code or some other name around it, but the idea is how do we have the common tooling so that our development teams and our operations teams can actually talk the same language, use the same tools, and gain the benefits of kind of that end-to-end development and management. And then the other trend I talk a little bit about is as people start to think about okay I want to take advantage of the cloud because I gain all the benefits of scale out there, I don't have to worry about the infrastructure, how does management also play into that? You know, why would you want to move your management to cloud and it's really the same reasons you might want to move anything to the cloud is you don't really want to have to manage the infrastructure anymore. You want to take advantage of the capabilities that cloud gives you and focus more on those scripts you're writing and the process you're trying to do and not managing infrastructures. The other big trend we've seen and I don't know if you guys are seeing it as well is that the world is getting more and more heterogeneous and we think about heterogeneous it's mostly around, we live in a lot of a Windows space, but most of the customers we talk with and probably a lot of you have other systems in your environments. You probably have Linux in your environments, but for sure you have other management solutions. You're probably not running all of the management stack. And so, how do you talk to all of those systems as they start to grow and grow, and as you move to the cloud, they just grow more and more. And so we really want to think about how do we go about solving that communication, that integration across Windows, Linux, as well as all the different systems you have to talk to complete that process. And then the last one is I think everyone here is committed that kind of automation in PowerShell is the way to actually automate your end-to-end process, but it also starts to put a lot more pressure on the systems because now we're seeing automation at scale. Whereas before we might do some automation to complete some tasks, we now see it as a pillar for most organizations as they start to develop more and more integrated management solutions across the environment, and so we wanted to build something that says okay we can actually solve that problem and give you automation at scale, so it can go from your small needs all the way up to the largest needs you might ever need because we're able to bring it from the cloud. So that's kind of a lot of the reasons why actually started thinking about okay all the PowerShell stuff that works today, how can we actually bring that to the cloud, solve a lot of the problems that you're seeing today, but also set you up for success in the future as you start to think about moving more and more assets towards the cloud.

Microsoft Operations Management Suite

Ed, do you want to talk about Operations Management Suite? Absolutely. So this is showing Microsoft's IT management solution and if you look down here at the bottom, we got System Center, and so system center has been around, it's very mature technology, and primarily it's been used and focused for managing the on-premises servers, all those servers that are over their systems there and talks to it and it helps to manage. And then a few years ago, Microsoft started getting into the cloud, and so we've got Azure up there, and initially, the idea was hey we need to be able to provide the same kind of management capabilities for the cloud that we've been able to have for our on-premises servers. As we try to shift more of the load to the cloud, the need to manage it grows proportionally as well. And so, that's kind of where Operations Management Suite, the idea for it came from is that we need to be able to manage this up there on the top, but we're not in a total cloud world. We're not a total on-prem world. It's a hybrid world, and so the need really is there for hybrid management. We need to be able to manage both. And by the way, in case you don't know, Azure isn't the only cloud service. There's also this little thing that's called AWS or something that some people use. And so, we need to be able to manage that too. And not only that, as a lot of times, people have their stuff in both places. They've got Azure, they've got AWS. So there are four pillars for Microsoft Operations Management Suite and there's a suite, so you can think of these four things. As we were putting the product together, we drew from different resources and we brought stuff together. So a lot of people who have heard about OMS, they think it's all log analytics, analytics, monitoring, and stuff like that, and that is a piece, but or people sometimes think yeah well it's automation and there is a configuration and automation piece as well. On Monday, we talked about using the OMS automation in DCS to solve a number of very real problems that we have with Desired State Configuration. One of the simple ones to highlight out of that is setting up a pull server. Setting up a pull server is kind of a pain. Setting up a pull server correctly is almost a shot in the dark. We give that to you for free. As soon as you start doing OMS and automation, you've got that. There's a backup in disaster recovery component. There is also a security and compliance component. Some of the stuff that we have, we provide deep insights into your environment that allow you to pull this information out and expose stuff that you might not have even known was there. And so, all of this, so OMS sits right there in the middle where we're doing the cloud, we're also doing the on-prem, and because of that, as we're initially looking at a lot of our customers or looking at stuff, we've got system center and OMS and they actually wind up being better together, but OMS is not a cloud-only solution. We can do cloud, we can also do the on-prem, we can do Linux, yeah we can do the other things, it's not necessarily a lay on top of system center. You can have OMS by itself or you can have system center with OMS on top. So it is pretty much the kind of management that you need for your environment that allows you to take care of these capabilities. Great. Yeah, so I think the area we're going to focus on today obviously is a configuration automation. It's most tied with all the PowerShell capabilities around all your scripts belong to your DSC. But the last slide before we kind of give you an overview of what's going on is I think everybody starts to realize when they look at the kind of lifecycle of the infrastructure and the application that there's a bunch of capabilities required when you're actually managing not just infrastructure in the application and it goes all the way up from how do you do the bills to how do you configure that and all the way through the monitoring backup and recovery, how do you make sure it's secure, the governance, all of that is going to be required as you start to think about especially in an enterprise world, what does it mean to manage the infrastructure and the application. And it's been a real challenge. I think we've had a lot of good tooling in System Center to actually help with that with PowerShell and all the other management tools, but as you start to expand that to the cloud, how do you still have the same processes as your stuff moves to the cloud? And I think at the core of this, a lot of core services will offer their own kind of built-in automation. Like when you do backup, it backs it up for you, and you can say that's automation because it backs it up for me. But a lot of times, you really need to connect systems together to really provide a solution and that's where the configuration and automation servers can kind of he uses a core throughout your entire lifecycle, not just the infrastructure and the application to make sure that the end-to-end works and at the core of that is the PowerShell because of its ability to kind of connect into everything and deliver that service. And so you'll see, as you start thinking about the whole lifecycle how automation configuration is actually central to all of that. Obviously, when you do the configuration side and other things we have like built-in capabilities itself like DSC that is required for that, when we think about the whole lifecycle, I would also say at the root of it all, it is configuration and automation.

Automation Service Overview

Okay, so let me jump into a demo. So we got through kind of the core slides, but how many people have actually seen the automation service? Good. We see two or three, so I'm going to jump in just spend five minutes, try to level set everybody what it's about and then hopefully as we go forward, you'll kind of get an idea of why we built it out and how you can start taking advantage of it yourself. So if you go into the portal up in Azure, there's actually called automation accounts over here, so you can go and create a brand new automation account. I think we showed that on Monday with Joe, so I just created one and I might jump into some of the capabilities around that, but when you first create an automation account, here's what you're going to see. So you'll see this basically the home page and this is kind of the overview of what's going on inside the service and it has a set of things. So what we call Runbooks are basically PowerShell scripts, PowerShell workflows, or graphical runbooks, so we kind of have three types of language we support today. We may add more in the future, but all of those are runbooks to us because they basically allow you to deliver a process. From our assets perspective, I'll jump in here because it's kind of a set underneath here and this is really where we try to solve a lot of the problems that PowerShell has today and some of them are the first thing is we allow scheduling. So you don't have to kind of set up your own scheduler using task scheduler or something else and manage that, we do all of that in the cloud. So if you have things you want to run on schedule, it's available from the service. The other thing is all of your modules, so as some of you know, when you're trying like to automate and do PowerShell, you kind of have to make sure the modules are the right system when they're actually going to get deployed and actually execute. And so what we do with our modules is you can centralize them all into our service and then we can distribute those out to wherever you need them. They can run from in our service, but you'll see that we're starting to bring it so that you saw with the DSC probably on Monday we download all the modules needed for that configuration so that you can actually run it. And so you don't have to worry about the distribution of all of your modules going forward because you can centrally manage them here. From a certificate thing, it all really depends on what you do in certificates, but a lot of times, certificates themselves to get deployed and managed, and so we allow you to do that. it might be something as simple as setting up like a certificate auth between servers or it might be deploying your certificate to actually use for authentication and security over in IS, so it really depends without all of that inside of our service as well. The connections I'll talk a little bit about later, but these are really an object that allows you to kind of separate what system you're talking to into a connection object so that you can reuse that inside your PowerShell going forward and you don't have to hardcode that into the system. Obviously, variables are kind of the catch all, so again, all of this is trying to allow you to say I don't have to put stuff into the scripts. I can put stuff into a central location service and then I can reuse those across all of my PowerShell as I go forward and it's kind of the goal of like we've seen it for years and most of you probably do it, but don't hardcode anything into the PowerShell because then it doesn't really become reusable, and so if we can allow you to pull all of that domain knowledge into a central store and then you just use it at runtime, it really allows you to have a powerful, flexible, and shareable PowerShell scripts. The most obvious one at that is like your credentials. You probably use credentials to connect to a bunch of different systems, but when the password changes, you don't want to figure out do I have to go back to the scripts or some other place. You can just set it once here and then all of the PowerShell you've written just inherits that as it goes forward. And then, I mentioned variables and here's our credential stores. So I won't go into DSC configuration because I think you guys saw this on Monday, but obviously we have full management of all configurations inside our service, so you can upload your configurations, you can compile them into the MOFs, you can deploy them out to any server either in Azure, on-premises in AWS, and it's a pull services, so they get all their configuration and all the reporting comes back to our service. Jobs, again is how do you troubleshoot? One of the challenges when you run scripts on-premises today is how do you know what happened if something goes wrong? Where is all your logs being stored, are you writing them to some share, are you trying to put them in some folder? We'll centralize every single action that's taken from the automation into our jobs. So if you need to go back and troubleshoot, you get the full log of what happened, but you also get which PowerShell script was used at the time, so you can actually say well it's only changed my PowerShell script and that's why it's failing or is it because the script just started to fail now and move the error? All of that is centrally managed inside the service, so you never have to worry about that going forward. These are the nodes in management, again, I think you saw most of these on Monday, and here's the node configurations that were actually available. Question, yes. Hey if I'm doing DSC pull server on here and it's contacting my nodes local on-premise, am I going to be able to tell from here what's going on? Yeah, so there's full reporting. I won't, so you can, like one of these… (Audience comment) Yeah, you'll know exactly what's going on from inside of the OS, all of the reporting shows up here. So because the pull service you'll see that, you know, I'll just jump in here to one that's compliant because this one's running, but you'll see I have a configuration that basically installs the Windows feature, sets up a website, probably some stuff from Azure storage, I do some file work, I get some stuff from remote file, and I install the package, standard stuff you'd see in DSC. But now, all of this is reporting back, so if one of these things fail like you couldn't set the website up correctly, I can drill in and you'll see exactly what's going on. So you can see here I took action against the default website, as well as I created a new website inside of my configuration, so you can basically do troubleshooting all the way from the top level down to the actual resource that might be failing inside DSC. So yeah, that's not, that's one of the major advantages you'll get to it's kind of the central management of everything. Okay and I'll jump out of here because I know we have other stuff to show, but the last thing to show here and obviously you get the overview of your jobs, plus we have source control integration directly to GitHub if you're starting to go down that path. You can just pull all of your scripts directly from there and it allows for kind of that source control. Okay.

Cloud Configuration & Automation

So today it's only GitHub that we directly integrate with. I have some sample PowerShell out there that allows you to basically connect to local.get, to TFS online, and you could basically use any kind of source control you want and it basically just runs and I can share that later if you like. Yeah, I've been on the online system, TFS is already showing up with coming soon. I know. It's coming I would assume is the keyword there. It is coming, but we don't have a date on it yet, but there are some ways to do it today. I actually have a little script that just runs and does that automatically. There's lots of ways to do it, especially with our web host. But yeah, we're going to try to make the integration tighter. (Audience comment) Yeah. (Audience question) So today we store them and it's kind of a similar type of thing to the key vault, but we have our own store, a secure store in the back-end that we use, and then we put them out as needed at runtime. You could use key vault cmdlets if you want to store them there and then at runtime pull them into your script if you didn't want us to store them, but it's a very similar back-end security model we have on key vault. (Audience comment) No you won't. So the question was if you wanted to sync with your local.get or your local source TFS, does that mean we have to open up a bunch of ports so that we can connect to it to pull out those committed runbooks and scripts. And you don't and the one way you don't have to do it is I kind of showed it, but I didn't talk about it while I was. We have a hybrid worker in our service, and so what the hybrid worker does, you can deploy this into your own environment on-premises and then it reaches out for work, so it's only outbound ports that it has to go, so there's no holes being punched into your firewall, it's just an outbound port you have to open and then that's how you can actually sync everything up and then send it up to there. Okay, so let me just kind of, I know I flew by a little bit on that, but I just wanted to level set everyone. Hopefully, you got a view for what we offer, but it really comes down to these major areas. I won't go through each one, but obviously everything's through the browser, so you can access it anywhere so all of the user experience, all the authoring you can do up there in the browser as well. I might show that later. We have full role-based access control, so if you want to say only certain parts of your organization have access and certain automation accounts, you can actually assign an operator role and they can just execute stuff in there. They can't actually modify, look at any of the credentials, do any of that. Similarly, you saw some of the source control versioning work that we we've done. From the authoring, we have two ways, this is the PowerShell summit, but we also offer graphical authoring if you want to use that, and you can use graphical authoring with PowerShell and vice versa. But obviously, we have an authoring experience for PowerShell directly in the browser as well. So if you want to do authoring in there and you're on the road and you need to fix something up, you can directly do it inside the browser, and test it, and then deploy it. I didn't jump on the gallery. I think we may show that later, but we have a gallery experience that we can integrate with. This allows you to pull all of your modules from PowerShell Gallery. Everything's going to PowerShell Gallery, so if stuff isn't up there that you need, either please right it yourself and upload it or ask us and we'll try to upload it as well, but everything we're doing is building upon PowerShell Gallery as the kind of the source and the community content that's up there, so we don't want to trade another one because there's such a huge ecosystem around it already. And he says modules on the PowerShell Gallery, but we also have scripts up on the PowerShell Gallery as well, and so if you've just written kind of a quick script or workflow or something like that, you'll be able to pull that down from the gallery as well. Yeah, it just got added the script read to recently, so it's another way in. We're going to kind of do first class integration with that very soon. I won't go through the runbook engine. You can probably imagine, but everything is built, so we run all of this security and isolate it in the back-end for you. So we basically create a host that you can run all your PowerShell in the service and it can scale as many as you want, it's resilient, it can failover, you can specify which regions, all of that kind of capability, you just get for free by moving things to the service. And it has a because it's a service, you can integrate it very easy. I'll show you some of the ways you can extend and integrate into the service because it has REST APIs that you can integrate with, as well as obviously all of the cmdlets, which were the Azure cmdlets that you can manage it. The integration I talked a bit about, so I won't spend any time with that, but I mentioned earlier that through the key value property of PowerShell is not just a great language, but it's because everybody and a lot of our partners and competitors have really on boarded with PowerShell as the way to manage systems, and not just Windows systems, but actually management systems and even clouds. I don't know if any of you guys have tried the AWS module that's out there, but it's like it's pretty awesome. They have like I think 700 different cmdlets for managing every single thing you could do against AWS and all of that can be run inside of our service, so it really kind of shows the power of using PowerShell, but the ecosystem around it, it really does allow the integration to take place.

Moving Your Investments to the Cloud

Okay, so I'm going to hand over to Ed and he'll talk a little bit about if you want to, how do you bring it over. Okay, so I started off by talking about how we honor your investment in PowerShell. Well then that makes us say well okay, well dude, so obviously the language by being able to use the PowerShell language and stuff, but you've also got scripts that you've already written and a lot of those scripts you can bring directly into our service to be able to run immediately. Some of them, you're going to want to change a little bit. So how many of you ever remember that Don Jones said don't use Write-Host that every time you use Write-Host, you kill a kitten right. Okay, well not only that, it's going to kill your automation script. Okay, you may not like kittens. I actually saw a picture of a dog sitting there typing Write-Host, Write-Host, Write-Host. But so use Write-Output, it's simple. Write a PowerShell script that goes through every single one of your PowerShell scripts and changes Write-Host to Write-Output. Actually hmm, that might be a good scripting guy article. It'd be easy to do it. And you're already set. Now the nice thing about Write-Output is not so much from when you're actually doing your automation because, as Eamon said, we have great logging, but when you're testing, when you're testing your automation to make sure that it wants to work, yeah it's nice if you have something that kind of comes out. This is kind of like using Set-Debug preference or something like that where you use your Write-Debug, but we just use Write-Output and I don't know what would happen if I used Write-Debug. It may work, it might not. I just figured I wouldn't. No, we suggest Write-Verbose. Yeah, Write-Verbose. Okay, so that would be better. But yeah, or Write-Output just to throw you some little status points up, as you're writing this stuff for the very first time. And no Read-Host. Yeah, absolutely. Don't Read-Host. Obviously, that's not going to work. Don't use Get-Credential. A lot of stuff like that. I've actually got an article that's kind of in the works right now that I'm going to have on the OMS team blog where I'm going to go into some much more detail about the stuff that you want to clean out of your PowerShell script before you stuff them into automation. Another thing you want to do is you're going to want to modularize. That's actually a word I made up back for my step-by-step 2.0 book where you take your script and can kind of break them into component pieces, okay. So you want to do that because if you break your scripts up into component pieces, rather than having these great big old monolithic, I call them Frankenscripts, break it up into small discreet pieces, that way you can reuse them. So for instance, if you've got a script that sends, that calls SMTP mail because you want to send out an email or something, that makes a nice little piece. You can just kind of store it up there and you can just simply call it, bring it in whenever you want to. Although, we've got some stuff that's coming, you're not going to want to do that because we're building in the email notification features, so you'll want to be able to try to take advantage of that as well. We have, we've got a scheduler that's really cool and so you can create multiple schedules and then just attach that schedule to your script however you want. It's going to just run. You're going to get all of the notification. Anybody that's ever tried to use even the PowerShell scheduler, or the App command, or the Windows Vista scheduler, we've got like three or four different schedule engines built-in to Windows, none of which is perfect, yet each of which has major problems and issues around reporting credentials and all of that stuff and then when you start trying to test these things, it takes forever because you have to schedule it to run, you go to test it and you don't know if it was a credential issue, an access issue, a resource issue, or what that caused, or just I forgot to close a curly brace. You don't know what caused that script to fail. Well with our resources and logging and stuff, you're going to be able to figure that out. Workflow. Workflow is the way to go around here and I'm going to show you that in a little bit. Now this last thing here. Authoring in the ISE. I use the PowerShell ISE all the time. I love the fact that it's expandable and we can do the add-ons and stuff. I love the fact that we've got it up on GitHub now, PowerShell Gallery you can download new versions of it, yeah, all that stuff. Joe Levy wrote an ISE add-on, matter of fact, that's going to be in tomorrow's OMS blog. I'm going to talk about that. So he wrote an ISE add-on that connects automatically to automation, allows you to pull in your runbooks, you've got IntelliSense right there in the ISE, it creates a local store right in your profile, we use JSON to keep it in sync with the runbooks and stuff that's available in your automation, and you can test it right there. That's what I say, so I can edit, I can save a draft to the cloud, yeah, I can publish it to the cloud, yeah, I can test it from the cloud, I can run it local, however I want. It is a very, very, very cool thing and it's only on version .2. Yeah, can you imagine what it's going to be like when it finally gets to version 1.0? It is open source, by the way. It is published in the PowerShell Gallery, which of course also means that it's back in GitHub. And so if you want to contribute, if you want to fork it and add some stuff to it yourself, you're more than welcome to do that. Yep. I can show later on too. I'll show you a link to it if you want to start the preview.

Bringing Your Investments to the Automation Service

Okay, so let me show you this. So bringing your investments into the automation service. So now I want to tell you a story. So Sunday night, we were there at the hotel, the scripting life is like downstairs talking to about 20 or 40 different people that were like hanging around the bar area down there, yeah, and I went up and I was having like some issues, so I sent Teresa an email and I said hey, I need you to come up to the room. I need you to help me with my demo. And she goes, huh? I said yeah. So why did I need her to help me with her demo? Well because she is an MVP, which means that she has a free Azure subscription. So I wanted her to activate her Azure subscription, I wanted her to set up an automation account, and all of that. So she did it and I didn't help here because I was like busy working on something else. And she did all of this and then of course, then she made the mistake after she put her credit card in and made the mistake of giving me her credentials, so I said wahoo! So I'm lined up. So that's why this actually, it is the scripting watch portal and Teresa is the automation account name. Now one of the things that's really, really cool and we just introduced this recently, and so and I've got, I'm going to be writing some OMST blog articles about this. So if you go back and you're looking at Azure automation and some of the scripts and tried to automate some stuff, you'll see that there's a lot of stuff out there. A lot of this stuff is kind of difficult to word I mean because it's using Azure cmdlets, it's using Azure RM cmdlets, there's talks of different, talks about trying to get access to your particular subscription and all of this stuff and all of that is pretty much a nightmare, especially if you're just trying to learn this stuff. So what we did is we created a run as account and Teresa did this automatically. I didn't even tell her to do this because the GUI, when you're setting stuff up, it's smart enough to say hey do you want to create a run as account and she said yeah. Well once you create the run as account, then it does three things for you. It creates a run as certificate, it takes your account that you use and actually creates an account for that, it also saves your credentials as a credential object, and it makes a connection to your default subscription. All of these things were issues before. Okay. It definitely solves a lot of that. I think if you tried this out, depends on your organization, a lot of times you can't really use your user account because you might have to factor auth enabled, which won't work inside the service. Whereas now, we basically take advantage of service principles inside of Azure ID and that's what we create behind the scenes for you and then we do cert auth against those, so you can manage your certs and then we add you as a contributor to the subscription, but you could go in and manage and change that and do anything you want, but it just does all of this by default just by checking the box during creation right now. Okay, so this is a little bit of code that Teresa did, yeah, and it's pretty. So once you've done that, then you use Get-AutomationConnection and the name is the AzureRunAsConnection, and then you add your Azure RM account, specify your service principle, you've got Tenant, the TenantID, the ApplicationId, blah, blah, blah. Now I say she wrote it. She wrote this probably the way that lots of you write code. She found it and cut and pasted it, right. So the other thing that's really cool is that once you've set up this run as account, it automatically does a demo test script--- It's a tutorial, yep--- to the tutorial and that's where this code came from. Okay, but now you just simply past this into all of your other stuff. But you can see how easy it is to come over here. So there's the code. She kind of cleaned it up a little bit, but so add your RM account, blah, blah, blah, and that's all you really need. And then at that point, your scripts, you add this in and then your scripts will just simply work. So let's go back over here and I want to show you two runbooks and these runbooks are using this service principle and this one here. So this is it. It's a workflow, okay. Workflows start VMs. Now the one thing is you're naming your workflow and the name of your runbook, those names need to match, okay. You'll get an error message that'll be very obvious if they don't work. So there's that code right there that Teresa wrote to make my connection and then the rest of it, this is standard PowerShell. Basically, it's three lines of code 7, 8, and 9. My VMs get the AzureRmVMs. Now these are your new Azure VMs, not the classic mode ones. Okay. If you're creating new VMs in Azure, you want to be using the new ones, not the classic ones. The classic ones are stuff you created a long time ago. So we get all of those stored in a variable and just simply use foreach, walk through the loop, VM, and then Start-AzureRmVM, that's a tongue twister, and then give it the name and the resource group. Okay, so Teresa created all of her resource servers in a resource group called server. Make sense, it's servers. So this starts them up and then the one that goes exactly with this is called Stop-VMs basically. It helps, if I can tell you this, it helps if you create within your company a naming convention and kind of stick to it. You see this is a stupid name. But because I was wanting to figure out how to stop them because they were already running. And as they're running, they're tick, tick, tick, tick, tick, tick, tick, just taking money out of your credit card. So I wanted to figure out how to stop them really quick and I didn't want to do a bunch of clicky, clicky things because I'm a PowerShell dude, right. So workflow stop all of these. Now that obviously if I have more than a few, I'll be using foreach parallel because this is a workflow, okay, but I didn't want to get things too complicated here. If you know workflows, obviously you want it to know that one of the advantages is doing this stuff in parallel. Same code, exact same code, cut and paste, but instead of Start-AzureRmVM, it was Stop-AzureRmVM. So there's your two runbooks. Schedule them. This will help you when you start or you're playing with this stuff doing your development and everything else, these two are going to save you some money guaranteed. Start them up, shut them down. Now not only that, let me show you this because I really and truly mean it when I say I really don't like mices. Okay, so look what I did. Stop. So this is the schedule. I stopped these at, okay, so this is stop on April the 7th, I'm going to stop these at what is it 7 am, is that what it says? Yep. Ah, different time zone. Okay. Different time zone. Okay. So we're going to start these things at one time, we're going to stop them at another time. Okay and so they start up, I work, they shut down.

Integrating Into Other OMS/Azure Services

Cool. I know we're running out of time, so I'm going to jump just into a last couple of slides. One of the things that obviously there's real power in the service itself and that's where you can do all of your automation, all your PowerShell, all your configurations inside of service, but you'll start to see we've actually integrated it into a lot of the other OMS services as well. And so as you're starting to do that monitoring and all that logging, it means you can actually now trigger your automation so that if this action happens, which you're monitoring, maybe you want to escalate it to a service disk or maybe you want to restart the VM. We're all depending on that process, you can now tie the insides with the action together inside the services. Similarly, with site recovery, you can do pre-automation scripts to run, some post automation scripts to make sure that failover actually happens and that's first class integrated into the service. And then there's a bunch of integration into Azure as well. So if you create Azure alert today, you can actually say when this alert fires, I want to call a webhook. We didn't go into the webhooks, but we have webhooks available on our service as well. That basically allows you to call them. But then if you do anything on Azure VMs, we have rich integration where it says run this runbook. It's not just a webhook, so we've kind of first classed that experience as well. Obviously I mentioned earlier, it is because we're a service, it really makes it easy for you to extend it as well, so you can write your own modules, they're going to work in the service, you can call our SDK, you can leverage Azure resource manager, all of those capabilities are built into the service and allows you to build on top of that and extend. So if you haven't tried it out, definitely do try out our SDK. For the ISE add-on, I gave a link to GitHub here, so you can actually see where the source code lives if you want to use it. I'll see if I can open that up. But while I do, let me just jump in real quick and show you what the ISE looks like because I think it's useful to see what it does. So you can see here I've actually logged in to the service, I can pick my subscription if you had multiple ones and this, if you remember my RM account is the one I'm actually pointing to here. So that's my automation account and it just enumerates all of the accounts inside the service. But inside if your now all those runbooks are available to you, and so you can just open up one and just start to reuse it. Let's see if I can. So this is a very simple one. I was testing some output, but these could be any PowerShell, so you get all of the benefits of this, and so you can author inside of here, but then you can also throw in some key vault stuff. There's almost anything you want to do you would just normally do in PowerShell. Author it here and you can test it, but then you can upload it to the draft, so you can see this is updated locally, so I can just upload it directly into the service. And now it's in the service, but I can test it here obviously just running it, but then I can also test it directly up in Azure because we put it into the service, and so the identical script you author and tested here, you can push it to the service and just test it directly inside of here and just off it goes. One thing I mentioned is you can test it either in Azure or if you have a hybrid, you can cut it there as well. Alright, so let me jump out of here because I know we want to just finish up.

Review and Wrap Up

So here's kind of the last slide. It's really about just what were the session objectives. Hopefully, you learned about what we're doing, kind of where we're heading, how you can bring a lot of your PowerShell forward. But I did want to mention as well that we have some customer days going on. Ed maybe can talk about those. So we've got customer days where people can come in. We actually have two of them scheduled and if you're within the US, yeah just send me or Eamon an email and we'll see if we can get you into it. It's for US only, but if you're internationally and you're willing to pay for your travel, then we'll put you up for hearing that as well. The last thing is the OMS blog, so blogs.technet msoms and that's where a lot of this stuff is and where a lot of it's going to be going forward. And so, thank you. And with that, I've got some stickers, if you guys want to slide by, grab these as we clear out the room for the next people. Alright, thanks guys.

PowerShell Module Development Lifecycle

Introduction

So I'm Adam Driscoll. You can talk to me on Twitter if you have ever questions after this or whenever. Email me. I'm a Senior Application Developer at Concurrency. We're a small little consulting shop out of the Milwaukee area. I do a lot of like azure and System Center stuff. I was also the developer for the PowerShell tools for Visual Studio, so that's kind of my open source baby. And today we're going to be talking about PowerShell Module Development Lifecycle. So what does that actually mean? So what we're going to do during this talk is cover these kind of main points. First of all, I'm just going to kind of talk about what a development lifecycle is and why you want to use something like this for your PowerShell modules. Then we're going to look at some tooling that you can use for your development lifecycle. So we're going to start with GitHub issues and milestones. So I'm going to kind of go over the basics of what that means, how to track requirements, how to do some sort of design using GitHub. Then we're going to talk about GitHub for source control. So there's a lot of different source control systems out there. I just picked GitHub for the purpose of this demo because it's free and available on the internet. So we're going to look at the basics of that, how to get your code up in there. I'm not going to get into super detailed description of all the things you can do with GitHub source control because it gets pretty complex. That can be a talk in itself. From there, we're actually going to talk about AppVeyor. AppVeyor is a continuous integration system that you can actually hook up to your GitHub repository and it integrates really, really nicely and we're going to use that for running automated Pester tests and actually yesterday, I added here for deploying to the PowerShell Gallery. So we're going to use AppVeyor to deploy to the Gallery, so then our module is then available for download from the gallery. So you can kind of think of this as the entire lifecycle of your module.

Application Development Lifecycle - Process and Tools

So why do we need an application, the development lifecycle, or module of the development lifecycle? This is kind of like a classic software development cartoon that you always see around the place and it's like kind of a hit on communication. The customer describes their swing as this like weird three-tiered swing that doesn't really make too much sense, but it kind of gets the point across. And my favorite one is how the programmer wrote it because it's this crappy little string swing hanging on the ground, pretty much like all the code I write. And then business consultant obviously is this grand chair hanging in the tree. I love how it's documented, it's just missing. The customer was built like it was a rollercoaster. But in the end, what they really needed was a tire swing. So because of that, we kind of have this iterative development process and it's been kind of a term that's been around for a while in the application world and it applies directly to module development because if you think about it, a module really just is a different type of application and it's circular in nature. So you hear things like agile development processes and iterative development and everything. This is a case for iterative development of an application. So you kind of start with requirements gathering and this is where you go and talk to your key stakeholders, and depending on what kind of module you're making, it might be yourself. You might be the key stakeholder for that module, but it might be people in your company or people out on the internet might find interest in this, so you want to go out and kind of pull those people, gather requirements, and then get those requirements kind of documented, prioritized, and figure out what you need to get done first, that kind of thing. From there, you're going to kind of move on to a design step. A lot of people may think that you don't actually do too much design. I don't do too many Visios or anything like that, but there's a lot of design conversations that at least take place. You might be whiteboarding something or just having a conversation on GitHub issues where you're dropping pictures or seeing why did you do it this way, that kind of thing, so there's at least a conversation around design. From there, we get to the fun part, which is development. So everybody loves writing code, turning it out in your favorite editor, but there's a lot of nuances that come with that. You want to be able to track your changes, you want to integrate with other people developing on your module. You want to be able to roll back in case something happens, that kind of thing. And kind of integrated with development is testing. So there's a whole concept that you should be testing while you're developing, so test-driven development or behavior-driven development. Well there's kind of testing that takes place after the development has completed like user-acceptance testing. So there's this tooling out there to help us manage that. From there, we have to think about release. So it's great and all that we have all this code up in GitHub, but historically, it's been kind of hard to get access to PowerShell modules. You had to download a zip, you had to make sure the files were unblocked before it would load it and that kind of thing, but nowadays, we have the PowerShell Gallery, so we can manage our releases a little bit easier. We can get it out to the general populace a lot quicker. But we have to think about things like breaking changes, we have to think about version numbers and that kind of thing, so that's where the release management comes into play. And as you can see, then we move back into requirements gathering because you know after you release it, the first thing that someone's going to respond to you is like this doesn't work, or why did you do it this way, or I need this feature, so you just continue to work through this application lifecycle. So when you see terms like ALM, that's Application Lifecycle Management and that's kind of what we're thinking about in terms of module design. So there's a bunch of tooling available out there for this kind of stuff, so you can use a lot of the same tools that other software developers are using, so C# and .NET developers are using GitHub. We can use GitHub for PowerShell modules, like I said, it's still just software development. So you can use tools for requirements, design and gathering like GitHub issues, JIRA for tracking work items or even like Visual Studio team services. So a lot of these things are free for open source developers or small projects and stuff like that. And then when you move into like the development phase, you can use things like your favorite editor, you can check things into GitHub or Bitbucket or GitLab or whatever you want to use for that. Testing in the PowerShell world, we have Pester, which is great for both kind of unit and integration testing. It's really straightforward and it actually integrates really well with AppVeyor, which I'll show you later. Finally, when we get to release management, you're can use things like GitHub where you can put your releases up there, they have a release concept. But pretty much for PowerShell modules, you're going to want to start using PowerShell Gallery, so people can easily install your module. And then kind of managing the orchestration of all of this is our build system. So what we're going to look at today is using AppVeyor. So AppVeyor, free for open source projects. You can also get a paid tier. It gives you a few more bells and whistles. And then there's things like Jenkins. If you go up to powershell.org, they actually have free continuous integration systems running on Jenkins I think for open source PowerShell projects. I think it's TeamCity. Is it TeamCity? Okay, alright. But yeah, that'll be another build system that can be in this list.

GitHub Setup and Basics

Alright, so I'm going to do the most dangerous demo of all time right now and I'm going to rely on the conference internet to communicate with three different web services, so nothing is scripted here. Alright and what's funny is I was actually working on this presentation yesterday and GitHub had an outage for about 20 minutes in the middle of my practice. I'm like oh no, what did I do? But it looks like it's actually back up and running today, so hopefully it works. But so sign up for a GitHub account. It's super easy just like anything else. When you sign in, yours is going to look a little different, like I follow a bunch of stuff, so that's why I'm seeing all these notifications and I have all these reposts on the right-hand side. What you're going to want to look at is a New Repository button, so you click that and you can create a new repository. Let's say I want to create my module and my module is going to be a LoremIpsum generator, so it's just kind of a random text generator. So just name your repository and then you can put a description in there. It's telling me that this repository is available. Public repositories are free, so you can just sign up and get that for free. Private repositories are not, but they're relatively cheap, but for the purpose of this demo, we want to make a public repository for people to communicate on this particular module. So the first thing you want to do is initialize with a readme. A readme is just that page that you see when you first load up a GitHub page where it'll have like a description, it might have some images, a how to, that kind of thing, like what is this thing. So you can initialize it with a readme and then you always want to select a license. So I've actually had modules that I've published that didn't have a license specified and I've had people comment and say like I can't even use your code because you don't have a license and my company won't allow it, so you have to be careful which license you select and you want to make sure that you have one because it kind of covers your butt, as well as whoever is using your code. If you're ever curious about what a license means, you can head over to tldrlegal.com and you can just search the different licenses and it has like a super easy-to-understand breakdown of what that license actually means and what's covered there. So there's all kinds of cool stuff like what's a GPL license why shouldn't I use it, that kind of thing. Alright, so now we're ready to create our repository and luckily that was pretty quick. So now we have a repository where I can check in code, I can create issues, I can set up my wiki, all that kind of stuff. But if we're going to follow the kind of development lifecycle, the first thing we want to do is start tracking what requirements we have for our particular module. Well a LoremIpsum module really needs to be able to, I've already typed in here, really needs to be able to generate text, so generate random text. So the markdown language that you can put in these GitHub issues is really rich, so you can do all kinds of things like put quoted text, you can have code, you can do links, you can have images, you can actually reference commits and other users so they get notifications, all that kind of stuff, but we're going to just leave that blank for now and look at some of the other options on the right-hand side here. So labels allow you to kind of categorize your different issues. You can have multiple labels on an item, in this case, as an enhancement, so we don't have any code here yet. You can create your own labels and all that kind of stuff. There is also the concept of milestones. So milestone is kind of like a container for a bunch of different issues. So for example, we'll create this issue and we'll go back to the Issues tab and you can actually create a milestone here. So you can actually group issues and pull requests into a particular milestone and milestones can be time boxed, so you can say, I want this milestone done by the 7th, and then it's great for things like releases like 2.0 should be released on the 23rd, or it's good for features where you might want to say all these issues relate to this particular feature, but we're going to say I want all this completed by the end of this session. Alright, so now I have my issues set up and you can see it maintains like a history of things that are either referencing it or have been added to it. So now if I set like the milestone, you can say, oh it's been added to this milestone. I can add a comment like this is a great idea because all my ideas are great ideas. And then, my favorite new feature is that you can upvote things, so that's pretty awesome, which is actually kind of helpful when you go and look at an issue and you see how many people have actually said like yeah, you should probably implement this even though the Issue pane is kind of organized by when it was submitted sort of thing. So I mean you can reorganize it if you need to, but yeah. So now that we have an issue, let's actually go out and try to get some code up here. So you're going to go back to the code tab and there's a couple of buttons here that are helpful. So right here, this is the actual URL that you can use for any git client, so there is a million git clients out there. My favorite one is called GitKraken because it's a really cool name. Then there's this handy button here though that you can click that actually will trigger one of these built-in tools directly from the website. GitHub desktop is kind of GitHub's official client and then you can use Visual Studio. I actually have GitHub desktop on here already, so I'll just click that. It'll ask me launch this application, I'll click that, and GitHub Desktop will pop-up and ask me where I want to clone this repository. So I'll click OK. So now it's actually cloning the repository out to my local box. When it does that, I get the entire repository local. So if you haven't used Git before, Git is like a distributed source control system, so when you clone a repository, you're actually getting an entire copy of that entire source control system. And it has some links back to where it came from and that's how you sync back up to your upstream repository. So now I have my license and readme file available. You can actually go look in my GitHub folder, I have those files locally here. So I can go ahead and edit that stuff, but let's actually implement our module. But luckily enough, I already have done that. So I have a module sitting here that all it does is generate some LoremIpsum text. It has one function in it, it accepts min/max words, min/max sentences, it has some LoremIpsum text, then outputs that to the command-line when it's done or the output string when it's done. And as any good developer, I also have some Pester tests I wrote. So the Pester tests test to make sure that certain things are happening when I am generating this LoremIpsum text. So what we're going to do is we're actually going to copy these to that folder and if we go look in that folder now, you'll see that I have these two files, and if we go back to the GitHub client, you'll see here that now I have these two files as changes in the Changes tab here. So anything that's marked in green on this right-hand side is an addition. Anything that would be marked in red is a subtraction, so you can see the changes in the code. Since I'm just adding two files, the entire thing is green. So first, well what you can actually do is this is kind of neat is from here you can say Fixes #1. So it's actually going out to GitHub and when you use the the pound sign, it's looking for that issue number and if you say Fixes #1 because I have now generated this code, it's going to reference that issue in this commit. So now I've committed it and what you'll notice is you'll go back out to GitHub and refresh and it's not here. That's because since it's a distributed repository, you actually need to sync it back up to the upstream repository and there's a handy Sync button right here that you need to make sure to click after you've done commits. So you can do a bunch of commits and then sync at a later date. So sync does two things. It pushes up to the repository and it pulls down from the repository. So in this case, we don't have any changes that were different from what I had locally. I am only pushing changes up, so hopefully that happens pretty quickly. And then up on GitHub, what we should see is now we've updated or added these two files up here. So once these files are up, you can check on the different commits, you can see all the history of those files, you can see what was included in each one of these commits, you can actually make comments on each one of the lines and say like why did you do this or like this was dumb and then write a comment on the entire conversation or on the entire commit if you wanted to.

Using AppVeyor for Continuous Delivery

Alright, so now let's say I go in here and I am actually editing my module and for whatever reason, I'm just looking at this and I'm like oh dude, why did you put less than there, that should be greater than. And I don't have anyone code review this. I don't run any of the tests. I just change this and I'm like I'm super smart, I'm just going to fixing the module. So I fix the module, commit it, sync it, everything's happy. I'm a good developer, I'm using source control, I am done, but the problem is now that I've checked this in, obviously the test is going to fail because all my tests are going to fail because I've changed this logic that is incorrect, but without any kind of auditing in place, there's no way for me to know that happened. So what we can do is we can actually integrate with a continuous integration system. So one available continuous integration system is AppVeyor. AppVeyor is like it says a continuous delivery service for Windows and it's used by a lot of projects. It's actually, I'm starting to see a lots and lots of projects using this and it's really cool because it's free for open source projects. You can pay for plans. I think the big thing with unpaid subscriptions is you get in a queue and sometimes it takes longer for your build to actually happen. So once you sign in, you can actually sign in with your GitHub credentials and it's automatically linked to your GitHub account. And you can see here, I have a couple different builds that I've set up, but what you're going to want to click is the New Project button. So the New Project button, since it's linked to my GitHub account, it just lists all the repositories that I have. So now I have my LoremIpsum account and I can just click this handy dandy Add button here. Alright, so now I have my build set up. So what you can see here is there's no latest build. It automatically tracks changes in the GitHub repo, so anytime I commit a new change to my repo, it will automatically start a build, but you can modify a lot of those settings here on the right-hand side. There's a bunch of settings for AppVeyor. I'm not going to go through all these, but there's certain things like only building certain branches, building only with certain tags, that kind of thing, building on a schedule. You can also set things like environment, so they have different images that you can work with. You can set environment variables. You can have it run tests, you can have it run build scripts, you can have it deploy certain places, so we could automatically have it deployed to GitHub releases or something like that. And then there's all kinds of other stuff. What's really important is you've got to make sure you get your cool badge on GitHub repo that the build is passing. And then one recommendation that I always have with AppVeyor is that you don't actually set all these settings in here and use this because it's really hard to audit changes inside this GUI. What you want to do is actually make the changes in there, save it, and then grab the appveyor.yml file. So YAML is Yet Another Markup Language is what it stands for and it's actually a really simple syntax and it just the appveyor.yml file can contain all your settings that you would put in this UI. So then you can check it into your repo and when you put it in the base folder of your repo, what it'll actually do is read that instead of the settings here and then it'll set up your AppVeyor build based on that, rather than this. So if we went and actually went and built this, so what you're going to notice is it's going to start a build and because I'm cheap, I don't actually have a paid subscription for this, so you're going to notice it queues it, so that's what this status is marked here. It's queued and it's going to flash this little blue thing until it starts. And then eventually it will finish the build. Luckily, I took some screen shots of this, so we don't have to wait the entire time for this to happen, but I'll go back and show you guys as it finishes sort of thing. So what you're going to notice the first time you build a PowerShell module with AppVeyor without setting any settings is you're going to get an error. So the error here is that, oh man, error here is that you need to specify a project or solution file. The directory does not contain a project or solution file. That's because AppVeyor is kind of set up for .NET code. It's looking for a Visual Studio solution or project in the root directory that it can build, and since we're building a PowerShell module, we don't have that. So what we need to do is actually change the way that AppVeyor builds. So there's a couple ways that you can do that. If you look in the settings, you can actually just turn off builds and then you won't get that error anymore, but it won't do anything. You could also just run a script and what's nice with AppVeyor is you can select PowerShell script and then and use whatever cmdlets you want to actually build your PowerShell module. In this case, you might have noticed that when I was checking, I was creating my module, I don't have a module manifest here. So one nice thing that we can do then is during our build, actually generate that module manifest on the fly, And since the module manifest has a version and AppVeyor has a version, what we can do is we can get that version dynamically and set it into our module manifest. So what I did is I actually created a appveyor.yml file and let's take a look at that. Actually what I'm going to do is copy that into output directory and then open it from there. Alright, so here's a YML file. It looks very similar to the one that I showed that was up on there. It sets the version syntax or the form that you want for your version, in this case, it's 1.0, and then this will increment every build. Then I'm using the build_script keyword and specifying that it's PowerShell script to call New-ModuleManifest, and in there, I can actually set the module version using the ModuleVersion parameter and use an environment variable that AppVeyor provides. So AppVeyor has a bunch of different module or environment variables that you can specify, one of them being the current build version, so it'll set those before it actually executes anything for your build. And then I set just a bunch of other properties for my module and what I can do is I can actually go and check that in, so Adding my appveyor.yml. One thing I noticed that you need to be careful of is make sure that appveyor.yml is all lowercase. For whatever reason if it's camelcase, it will not work. Alright, so now go back, okay, so yeah we finally got that error actually in AppVeyor. What we'll notice here is once this syncs, it actually picked up on the fact that I committed a new change up to AppVeyor, so now it's actually running a new build up there. So as that goes, what you'll see is it's going to generate the module manifest and it's going to succeed and it's going to look a lot like this. So I created a new module manifest, here I'll zoom in on that a little, go, or I won't zoom in on it a little. Okay. And you can see the whole command-line, which is kind of cool. So it cloned it, it checked it out, it called New-ModuleManifest, and then the build is success. It didn't find any tests or anything like that and we're good to go. But---

Running Build Tests

Remember that we have a broken project right now, we have no tests that are running during the build. So this would be a great opportunity to actually run some tests as part of our build to make sure that whenever a developer checks something in, it's not broken. So what we can do is update our yml file, so let me do that. We come back over here, you can see that I have added a few more steps to our yml file. The first step is that we need to install Pester. The AppVeyor image that's running, I think it was like 2012 R2 without Pester installed, but what's really nice is it comes prepackaged with a bunch of other software including Chocolatey. So since it has Chocolatey installed, we can actually just say send this to Pester, it'll go up to the Chocolatey package repository, download Pester, install it, and it's available for us to run tests against. And then the other section that I added was this test script section. So test scripts run after the build steps, so if you go look at the AppVeyor documentation, there's all these different steps that the build process actually takes and the last one that it'll take before deployment is a test, I guess test up. So in this case, we're specifying that we want to run a test script because by default what it's trying to do is it's trying to find .NET assemblies that contain either NUnit or MS Tests in them and then it'll run those automatically. Well in this case, we kind of have to do some special scripting to get this to work. So the first line here is, actually stole this from Warren Cookie Monster to do this because it works really well. And the first step here is to invoke Pester, so we just call Invoke-Pester on the current path and then we want to output it as an NUnitXml file. So NUnit, like I said, is a .NET-based unit testing project, but it has kind of like a standard format that lots of systems integrate with including AppVeyor. So it's really nice that Pester has that because it can integrate with things like AppVeyor and TFS and all that kind of stuff because they know how to consume that particular format. I also want to pass it through into a variable so that I can check the actual status later in this script. The next thing that I want to do is create a new WebClient and upload this file to AppVeyor and they have a really nice like API, like a REST API that you can deal with for doing all kinds of stuff like setting different things that appear in the build and uploading tests like this. So this actually, you go out to the URL, it's /testresult/nunit and then use another one of those built-in variables, environment variables for the job id. So then AppVeyor, yep go ahead. I have a stupid question. Where is this stuff running from? This is running, I think AppVeyor is hosted in Azure. Alright, yeah so now we're going to find this test result file, upload it up to AppVeyor. We use the JOB_ID to associate this file back to our job. So once it's uploaded, then it actually will parse that XML file, discover all the tests listed in there, and then you can kind of see a nice little GUI in AppVeyor for the tests. The last line is we want to actually fail the build if the tests fail. So in this case, we're checking to see what the fail count is and if it's greater than 0, we throw an exception and that'll actually cause the AppVeyor build to fail and you'll get like a red build up in AppVeyor. So if we save this, check this in, adding tests, commit that, sync that, and then okay you can see that this was a previous build that I showed that doesn't do any tests and it succeeded, so we're green right now, but like we said, it is a broken build. So now we added the tests to the yml file, and it's actually going to run this build and run the tests. What's going to happen there is you're actually going to see something like this where you get the output from Pester, and it's kind of cool because it actually formats it like exactly like you'd see in the PowerShell console and you can see all the tests failed. In this case, I must have taken this screenshot when I had three tests instead of two, but you can see that up in the top left where it says broken that it's red. So you'll actually be able to look at the build, see that it's red, if you had one of those little badges out on the front of your GitHub repository, it would be red, so it's really obvious when it's broken sort of thing. So these tests are going to run, they're going to fail, and then what we can do is we can actually go out and figure out how it broke. So that's where you go back into source control, you can look at the commits, you can say oh, this guy was fixing the module, what does that mean? It's like oh. Adam! You're dumb! And then comment on it. And from there, what we can do is we can go and fix out build again. So I'll go back to our module, bring that guy up, change it back to less than to fix the tests and hopefully that would fixing the tests. And sync. Alright, so now that commits back up to master and we go. So that's kind of how you can automate things during the build. So there's lots of cool stuff that you could do here, not just running tests. You could run things like script analyzer to have it validate particular things. You could have it sign your script so that you don't have to worry about doing that locally. Anything that you could think that you'd have to do to like publish your module before you publish your module, so that's a great opportunity for the continuous integration system. What's also really nice is, it's nice for you as a single developer, once you start working with a project with a lot of developers, that's where it really, really starts to pay off because you have merges happening, people don't always know exactly what everybody is working on. So when you merge it all together in a continuous integration system, it's super nice. The other thing you can do is set up different builds for different branches, have it run on schedules, that kind of thing, so you can do more complex things maybe once a day or something like that where you run integration tests and that kind of thing.

Publishing to PowerShell Gallery and Adding Secure Variables

Alright, so now we have our module, it's fixed, it's tested, it's running. I guess I could have showed what a green test looks like. So if it's successful, same thing, Pester output looks green rather than red so, but now we have this test module that we want to have people use. So what you're going to want to do is go to the PowerShell Gallery. So if you haven't gone to the PowerShell Gallery yet, it's powershellgallery.com and what it is it's based on a technology called NuGet, same thing that Chocolatey is based on and same thing that NuGet's based on and it's a package management system, so you can upload packages, in this case modules and scripts, and then use the built-in WMF5 cmdlets and I think they're WMF4 now as well, install module, install script, find module, and it will actually go out to these repositories and find the modules that are available. So it'd be great if our module was up here and available for people to easily use when they are just on the command-line, so they could easily install our LoremIpsum module. So what you're going to look at is this Publishing tab. So you'll notice that if you want to publish a module, the thing you want to specify is and you're going to use the Publish-Module cmdlet and you can specify the name of the module and then a NuGet API key. So a NuGet API key is more or less your identifier for your account. So if I were to click on my account, you would actually see my NuGet API key, which I'm not going to do because you've got to treat it like a password because if anyone gets that API key, technically, they could publish modules as you. So I'm not going to click that, but that's where you'd put that. So now let's actually go and update our appveyor.yml file to do this. So if we go back here, run that, and go back here, you can see there's a lot more stuff going on. The first thing that I'm doing is I'm changing the clone folder. So I might just not be, I haven't used Publish-Module all that much, so I'm going to be doing something wrong, but I actually had to mess with it a little bit to get it to work correctly with a path rather than a name. So if you look at what I'm actually doing with Publish-Module down here, I'm specifying a path and that's the clone folder that I'm specifying up here. So it'll automatically look in there. That's what will have my psm1 and my psd1, so that'll all be in there. The other thing that I added obviously was this deploy script. So the deploy script, the first thing that it does is it installs a package provider for NuGet, which will also update it to the most recent version, so it's a step that's required because we are using an image that does not have that in there automatically. I guess I glossed over the fact that I did change the image, so by default the image is called Visual Studio 2015. That does not have WMF5 on it. So we'd only actually have the Publish-Module cmdlet in that case. So what I can do is switch over to the AppVeyor image called WMF5 and it has that available to us. So then I'm calling Publish-Module. I'm specifying my NuGetApiKey, which is this ApiKey environment variable, and then the path to my module. So it's going to go out, it's going to discover that module manifest, it's going to look in there, upload all that metadata to the gallery, and then make it available for people to install. And what you're seeing down here is the actual definition of this environment variable. So what you can do is you can specify an environment tag, the ApiKey, and in this case, I'm using a secure variable. So this is not my ApiKey. This is an encrypted version of my ApiKey because if you remember, when I was running my AppVeyor key or AppVeyor builds, you can see all the command-line output, so we don't want to just dump our key on there for anyone to get access to. What we want to do is actually use a secure variable from AppVeyor. So the way that this works is this environment variable when it's actually running inside AppVeyor, is decrypted. So technically, if I were to just dump that to the command-line and it printed it out, it would be an unencrypted value. But it only unencrypts it for builds that are not pull requests. So if someone forked your repo, changed this script to dump out your API key, submitted a pull request, which caused an AppVeyor build to run, it won't actually decrypt that key, so you can't have anyone maliciously trying to get your key out sort of thing. So the only time that it would run would be when you triggered a build either through a commit directly to master or pulling in that pull request because you've validated the fact that someone didn't do something malicious with that key. So to create secure variables, I'll just show you how to do that real quick. Oh, our build failed. See there our tests failed. I wonder if it fixed them. Oh yeah, fixing the test did not fix the test, so there's still something wrong there. And to create an encrypted variable, you can just click on your name, click encrypted data, and just put it wherever you want in here, click Encrypt, and it'll actually give you the value and what you need to put in your AppVeyor YAML. So that's really important. I've actually, especially for deployment purposes, there's lots of reasons that you'd want to do that. For PoshTools, I actually need to do that for the code signing cert password and stuff like that, so you've got to be really careful because you can shoot yourself in the foot. I had to rekey my code signing cert like four times because I kept messing this up and dumping my public key to the AppVeyor log. I'm like ah. Alright, so now that we have that, we can actually check in our publish, which isn't going to work because our tests are broken, but that's okay. Publish module. Alright, so submit that. So now that's going to kick off a new build and it's going to take that extra step of actually publishing it. So I don't know if I actually took a screenshot of that, but that's okay because I don't want to wait for it to happen anyways. But once you actually publish something to a gallery, what you'll see is something like this. So here's our LoremIpsum module that gets published. You can see it got all that metadata from that module that I generated automatically, so it generates the LoremIpsum text and you can see that now they have functions, you can click on those, find out more information about that kind of stuff, and it actually has the ability to copy and paste this and run it, install it, that kind of thing. One thing you'll note about the PowerShell Gallery is that this module is unlisted. So when I was making this demo, I came to the realization that once I published this module, I couldn't delete it. I was like why is that case. Do you guys remember that MPM thing that happened, or yeah, MPM thing that happened where that guy unlisted his module from MPM. Did you hear about that? Yeah, so that's why they do this. So that's why they do this. So you can't actually delete your module. You can only unlist it, so people can't find it and install it manually. So I can relist it, so now that makes it publicly available for people to download again. So yeah, if you didn't hear about that MPM thing, some guy had like a 10-line MPM package that like thousands of packages---what's that? (Audience comment) Seven, yeah. So he got like some guy he had some naming right, he had the naming rights to it pretty much and some of other company wanted naming rights and what he pretty much said well yeah, if you give me $10,000, I will let you have the name, and they're like no, and they took away the name from him or whatever, and so he decided to unlist all his packages, which just broke the world of Linux for a while there. So that's what we're trying to prevent, I guess, with this. So you can't actually unlist your module once it's or delete it once it's published.

Considerations and Wrap Up

Yeah, so that is the module and now it's available, you can go download it, you could go comment on my GitHub. One thing I wanted to note is I took a very kind of simple approach to this and there's a couple things that you might want to consider before actually doing it exactly this way. Like this is a great way to kind of get up and running, but for one example is anytime I commit anything, it's now publishing to the gallery, so it's updating my module and really it's just running automated tests, no one is really smoke testing it by hand or anything like that, not the best idea. What you could do is create two different branches, one a development branch where you do all your development in testing, and then once you're satisfied that something is complete, then you merge it into master. So the development branch still runs your AppVeyor tests, it just skips the publishing step. And then when you pull request into master, that's when you can decide to publish. The other you could do is have a manual build on AppVeyor to actually say I want to publish this module now, so over on AppVeyor, publish, and complete that, and publish it to the gallery, so it's a little more manual steps, so you can decide when to release rather than doing it automatically. Yeah, so that is kind of what I wanted to cover. If you guys have any questions, I'd be happy to take them now. Otherwise, feel free to get a hold of me. Cool. Alright, well thank you very much.

PowerShell 5.0 Advanced Debugging

Introduction

My name is Paul Higinbotham. I'm here to talk about PowerShell Advanced Debugging. Welcome. I'm a software developer on the Microsoft PowerShell core team, so there's my contact information. Feel free to contact me. I've been working on the PowerShell team for almost six years now, which interestingly makes me still kind of a newbie. PowerShell has been around for a long time. A lot of people on the team have been there for 10 years or more. One of my areas that I work on in PowerShell responsibility for is remoting, some of the debugging stuff, workflow, actually a lot of different areas, but mostly PowerShell remoting and remoting related systems. So what I want to talk about today is more advanced scenarios in PowerShell script debugging. This slide is really just about motivation. When you have script that's just not working, there's a lot of ways you can diagnose it and try to figure out what's going on and fix it. A lot of times you get away with just looking at script output, verbose output, trying to figure out what's going on with your script and sometimes you can fix it at that point. But really, a lot of the times, what you really would like to do is just get a debugger on it, a script debugger and PowerShell actually has a pretty nice script debugger that you can use to break into script, step through script, set breakpoints, that sort of thing. But traditionally, it only works for interactive scripts, scripts that you run in the ISE or in the console. But PowerShell is a pretty capable platform and allows you to create and run scripts that are not being run interactively. They can be run as background scripts, they can be run as part of an application that hosts PowerShell and these are more complex scenarios where you can have complex script. I think we'd like to debug, like have a debugger on it and be able to step through the script, but previously, that just was not possible. You can only debug the script if you were running with an ISE. So anyway, this talk about how to use some of the new PowerShell Version 5 features for debugging these more difficult scenarios, and of course, one of the great examples is DSC script resources, so that'll be one of the demos. So live debugging goal. What you want to do when you want to put a debugger on script. Basically the script is not working, you want to find the issue that's breaking your script code, fix a problem, and move on, and usually the first step is trying to get debugger onto your script. And again, as I mentioned before, debugging interactive scripts is really easy and PowerShell, you open the script file in ISE, set line breakpoints, you run the script, you step through it, you can see what's going on, so that's really nice. But debugging non-interactive scripts is more difficult, actually it was impossible until version 5, and these kind of scripts are like background running scripts. PowerShell has a rich set of APIs that let you create scripts that can run concurrently, that can run in the background very easily. It's not running in the ISE or in the console, so how do you debug something like that. Some examples of that are you can have like a custom management tool that's based on PowerShell and PowerShell scripts, WinRM remote host when you do PowerShell remoting, WinRM creates a process that hosts PowerShell, and within that, runs the script remotely on a remote machine for you, so that's just an example of how PowerShell can be hosted in a process. And then, of course, the most complex scenario is the DSC configuration scripts when they run. They run on a, usually on a different computer in some kind of a hosting process inside a runspace, and if you have DSC configuration script that's not working correctly, how do you debug that? Preferably like bugger in the context on which it runs. I mean a lot of times you're developing probably your configuration scripts in some kind of a mock up on your dev machine works fine, but then when you deploy it onto another machine, it's not working. So what we would really like to do is be able to put a debugger on that live session and debug that script in that context. So the main concept for advanced debugging, I think, is redirection. The whole idea is putting a debugger on the script that's running is basically you have ISE on your dev computer and you want to point that or redirect that to where the script is running and debug it. And there's like three cmdlets that conform to this redirection paradigm, one is Enter-PSSession, that's actually a cmdlet that's been around since version 2 and that's how you do remoting, interactive remoting. You would Enter-PSSession and you're basically redirecting the command-line to a remote machine. So when you type in the command-line, it's being executed on the remote machine and then of course, output's going back into the console. But what's new for v5 is two new cmdlets. One is called Enter-PSHostProcess and what that does is that allows you to redirect the command-line to a local process on your machine and that can work within a remote session, so you can connect to a remote machine, and then on that remote machine, you can connect to any process that's hosting PowerShell, and then when you execute commands, it's actually executing in that environment. And then the third thing is, third cmdlet, is called Debug-Runspace and what Debug-Runspace does is basically redirect the debugger. It points the debugger at a particular runspace that's executing the script, and so then you can debug that script in that context.

Runspaces in PowerShell

So just take a second here. What are runspaces? And this basically says that every PowerShell script that runs within a context called a runspace, as we call them internally, and what the runspace is basically the context in which a script runs. It has the variables that you've defined, it has the functions that you've defined, modules that you import, breakpoints that you set, everything is all kind of contained in something called a runspace. So when you talk about debugging script, it's kind of synonymous to say you're talking about debugging at runspace. It's a runspace that's basically running the script and providing the context for that script to run. So when you hear things like point a debugger to a runspace, what that means is it's really the framework in which script is being run. And I just want to provide a diagram, just to kind of see it, kind of get your head around it because it's fairly complex. You know over on one side of town you have your computer, you have the PowerShell ISE open, but you want to debug script that's running on the machine someplace else, and so the first thing you do is use Enter-PSSession so can connect to that machine across town, and then within that machine, there can be multiple processes that are hosting PowerShell. It could be a PowerShell console, it could be some sort of application process that you're interested in, maybe like a DSC configuration, local configuration manager that's running script. And so the next thing is you want to then point to that particular process that you're interested in and that's when you use Enter-PSHostProcess cmdlet. Then within that process on that remote machine, there can be multiple runspaces that are running script. And then so one of those runspaces you're interested in is a script that you want to debug and then so you want to use debug runspace to point to that particular runspace and debug that script. So it's fairly complex. Either these cmdlets are not something that you're going to use every day because this is kind of a complex debugging scenario, but when you need it, they're very powerful and it's something that can really save you if you're just facing a problem. Scripts that aren't running and you're trying to figure out why. It's just really nice to be able to debug it in a live session. So from here, I want to pivot to the first demo. Basically, what I want to demonstrate here is the process of redirecting to a host process that's running script that you're interested in, and within that process, finding the runspace that's running the script that you want to debug and then attaching the debugger. And actually what this demo is going to do is simulate a process that's running four simultaneous scripts. One is going to be, actually I only have three here now, but one's going to be just a script that's running in a continuous loop that's you're going to attach the debugger to. Another one is called copy file script. It runs very quickly, and so this one's going to be set up so that as soon as it starts running, it pauses and waits for a debugger attach so that you can debug it because it runs in a few seconds and you want to debug the script. Third one is just a simple algorithm that extracts the sub array from array, and in this case, it's going to use a new cmdlet called Wait-Debugger and what this does is a cmdlet you put in the script and when it executes it puts the execution engine into a wait mode. It stops execution and waits for a debugger attach. And the nice thing about wait debugger is that you can put it anywhere you want inside the script, and so the debugger will stop and the script execution will stop and wait for a debugger at a particular point, which is kind of nice when you're debugging. And the fourth one I don't have listed here is set up a runspace, and again, it's going to run the copy file script, but in this case, you're going to give it a line breakpoint and say I want it to stop at this particular line and this script file, and then wait for a debugger attach. So I've created a brand new console and pretending it's not an interactive console, but it is, and in it, I have a script that's going to simulate what a PowerShell application, PowerShell hosting application would do, which is basically create runspaces and then run script files. I won't spend too much time on this, but I do want to show a little bit how this works because it's kind of the same pattern over and over again on how to create a runspace and then execute a script inside it. And then once this is running, and I'll show how you can attach to the process and then debug each of these scripts that are running. And you can see it, okay good. So first one is basically a pattern where you create a runspace and give it a name, creates a call to PowerShell object, you associate it with that runspace, and then you say okay run the script file. And this is one that just runs continuously, so it's just going to run in an infinite loop and it's going to have runspace and then call LogQueryRS. The second one is the copy file. It runs very quickly, so again, we create the runspace, but this is a little bit different. After we create the runspace, we use a cmdlet called Enable-RunspaceDebug. What this does it sets up the runspace in debug mode and also sets a source called BreakAll. So what it's telling runspace is as soon as you start running script, stop at the very first execution point in the script, and wait for a debugger attach. This is a little bit dangerous, something you definitely don't want to have in a production environment because the script will look like it's hung, it's not running, and that's because it's waiting for a debugger attach, so this is something you only want to do in investigations. And the third one, and again, I kind of want to go through these because they're slightly different, this one is the Max SubArray, again it's creating a runspace, adding in variables, and executing it. The only difference is that inside the script, there's a Wait-Debugger command. And so, it is also going to stop and wait for a debugger attach, but there's nothing we did special to the runspace. We actually had to modify the source code. So the Wait-Debugger is a very powerful, again something you don't want in your production environment because it looks like your script is hung and the downside is that in order to use it, you have to modify the script file, but it's very powerful for just saying I want the debugger to just stop at this point, so I can investigate what's going on. And then finally, the last one, again I'm doing a copy file. Again, the only difference is I am setting a breakpoint in the runspace at this particular line in the file and one thing to keep in mind is again we have to tell the runspace to be in debug mode because normally when runspace is executing script and a breakpoint event fires, if there's no ISE debugger available to handle that, then it just ignores it, it just continues running, but in this case, we don't want it to. We want it to stop and wait for the debugger attach. Okay. So over here on my clean process, I'm just going to go ahead and run that script, get the process id 4112, and then go back to here, and then I want to show a new cmdlet called Get-PSHostProcessInfo. What this does is that a list all of the processes on your local machine that is hosting PowerShell and is available for a connection and we see a bunch of them here because I have a few open, we see the ISE obviously, the one I already forgot, (Audience comment) 4112, thank you, is one that we're interested in, these are other consoles that are running. So now we can use the new redirection cmdlet called Enter-PSHostProcess 4112 and we can attach to that process. One thing I kind of wanted to show here and I think I'll just go ahead and do that now, just to back up a little bit, that this Enter-PSHostProcess will work in a remote session. So with just one laptop, it's hard to simulate all this, but I'm just going to pretend that this is running on a remote machine, so I'm going to do an Enter-PSSession, actually it's a loop back to this computer, and of course, for some reason my poor laptop seems to be really, really slow. And if this is going to take too long, I can, okay there we go. So now we're, and again, we can do a Get-PSHostProcess and we kind of pretend that okay we're remoting into a remote machine, we do get PSHostProcess, we want to enter PSHostProcess 4112 and the thing to kind of look at is the command-line because it tells you what's going on. It tells you that you're connected to the remote machine, localhost, in this case, and within that, we're connected to the process 4112. And then from here, we do a get, this is another important cmdlet, Get-Runspace. What this does now is within this process, it'll list all of the runspaces after runspaces in this process. And now finally, we see kind of what we expect to see. If we look at the first one, this is because this process that we're in is actually a console host process and it always creates a default runspace, so there's always a number 1 Runspace1 and that's been created so that when you type in the console, that's what executes your command in your script. But then we see the runspaces that we've created and this is what we expect, and of course, the first one the LogQueryRS, the availability is busy. That's because it's running in continuous loops. Every time I look at it's going to show busy, but on the CopyFileRS or these other three all show availability in breakpoint. What that means is that they're not running script. It's stopped and it's waiting for a debugger attach. Again, you have to be a little bit careful with that because if you're not aware of it, you just might think that script is taken forever to run, it's not actually running. And then the other one here is a little bitty string. This is called RemoteHost. This is a temporary runspace that has been created when you run the Enter-PSHostProcess. What that does is that executes the commands that you type on the command-line, so here like if you type in your command, this is running in that RemoteHost process that its connected to. When you exit the attachment to that host process, then that temporary runspace goes away. So then I just show real quickly how then to debug these runspaces. Each one is just a little bit different. So you do Debug-Runspace logqueryrs, that's the first one, and what's happening is when you do a Debug-Runspace on script that's already running, it attaches debugger and then it sets the debugger into step mode immediately, so it just stops at the very next execution, and at this point, you can step through the code and do all the debugging you want and you have the command-line here, so you can query state and machine state and variable and all the things that you can debugging, so you have a nice live debugging session there. What I'm going to do is a detach where you type detach at the debugger prompt, it detaches the debugger, and so now if you do a Get-Runspace, we see that it's busy again, so it's busily running in that infinite loop. Now if we do a Debug-Runspace on the first copyfilers and it did this before. I'm not sure what's going on there, so we're going to skip that. Oh, that's weird. So yeah, don't look at that. We're going to move to subarray. I'll take a look at that later. This next one, this one is what we expect. This is the code I was talking about. For the Wait-Debugger, what I did was if a global variable is defined as true, then I go ahead and execute the Wait-Debugger, which means that it puts it in step mode so that the very next line to execute is stopped and waiting for the debugger attach. And now you can go through and do debugging as before until you're done. And then if you want, what I kind of like about this is that it's conditional, so if you're done debugging, you can do it, and this is in a loop, so if you don't want to do it in a loop anymore you can just, whoops, set the variable to false. (Audience comment) I'm sorry? You're missing a G on the debug. Oh, thank you. It's these bifocals I can't see very well. And then you can just do an F5 to continue and then it continues to run this very fast. One thing to keep in mind is that the debugger attach remains, so you get this little warning that says command or script is completed, but the debugger is still attached. So like if your new script started running, you can debug that if you want, but if you're done debugging, then you can detach a debugger with just Ctrl+C then it takes you back to the command-line. And let's go one more Debug-Runspace copyfilers2 and this is one where we set the breakpoint. One thing to kind of keep in mind is that this one is still busy, this one has gone to available because it's completed running the script, and so the runspace is available to run a new script, which is kind of interesting to note. And this one is still waiting for a debugger attach. And here is kind of what we expect, it's on Line 21, which is what we set, and so it can debug from that point. And again, I'll just do a detach by typing detach at the debugger prompt. So when you're done, you can just do an Exit-PSHostProcess Exit-PSSession and I don't know why that's taking so long. I've been getting these weird pauses. Let's see if I can get back to the slides.

Q&A

So yeah, so any questions? Feel free to interject. There's time at the end also for questions. What runspace are these in? Are these under a user runspace or a system runspace? The question is what runspace they're in, the user runspace or a system runspace. Are you talking about the credential context of which they are running? So I send the same context that the host process was created, so it's running in that context. So if it was created with admin credentials, it's that, or any other user credentials, it's that. So as you, the way it actually works is when you create, you saw the runspaces and then you execute script in that runspace, PowerShell spins up a thread and is the thread based on the process context. Any other questions? Ah, yes. If the runspace that you were attaching to was multithreaded using the .NET classes to create a multithread call, will you be able to attach to the separate threads? The question is if the runspace that you created was multithreaded, could you attach to each of the threads. And the answer is that each runspace can only run in a single thread, so you can't have multiple threads. And so the way that PowerShell does concurrency is per runspace. So multiple threads in script is really not supported. It's just a single thread. Are you talking about just creating a runspace tool rather than just using Visual runspace? So currently one of the pieces of middleware that I have running uses the .NET class to create a thread pool and then the value is group memberships in parallel for each one in the thread pool. Gotcha, gotcha. I understand. So yeah, so what you're saying is that inside your thread using .NET API to create multiple threads and then execute matters code on each of those threads, and in order to, so now you're outside of this script debugging. So in order to debug that, then you have to use a native or manage debugger to debug those threads. PowerShell script debug is only for debugging script execution. Any other questions? Oh. But in that case, if you have a runspace pool, you would see all the runspaces in the pool and theoretically be able to debug those? That's a good question. The question is if you had a runspace pool, which is a construct that collects and manages multiple runspaces that you would be able to see all of those runspaces and debug script in each one of those and the answer is yes. When you do a get-runspace, all runspaces are created whether it's within a pool or separately is shown and it's shown in the list until all those runspaces, those objects are disposed. And as well, on jobs because jobs usually have a child job that's actually doing the work, so you have a double set into those runspaces? So the question is, how would you debug jobs. And I think the assumption is that jobs are running in multiple runspaces. Actually, PowerShell jobs run in child processes and inside those child processes are created runspaces, so it's done that for extra isolation, it's fairly heavyweight. And so, in order to debug, there's actually two ways. In order to debug a job like that is that you could actually attach to that process and then find the runspace and debug it, but there's also a way from the original client console to do a debug job. So when you get a job object back, there's actually Debug-Job cmdlet. I'm not talking about it in this talk, but you can debug the jobs through that way as well and it actually goes through the PowerShell remoting and then talks to the child process. Interestingly, actually I've created and I think other people have created thread jobs and what they are lighter weight PowerShell jobs that don't run in a separate process, but run in a separate runspace and if you use my thread jobs, then yes, when you run those scripts what it does is it creates new runspaces in which to run those scripts and you will be able to see those and you can debug them using this technique. Paul, is that available in your thread jobs? Is that in the gallery? The question is that thread job available. I've been meaning to put it up on the gallery and I can't even remember if I did or not. I think I did a few months ago, but if not, I will do it. There are other people have done, I mean it's a common idea, other people have done the same thing. What I did was make them as similar to other jobs, the only difference is that instead of running it in a child process, it runs in a thread. The nice thing about that too is that the objects that you're dealing with are live to that process. You don't have that serialization layer going from one process to another, and so the objects are live and plus it's much faster too. You don't have that overhead of creating the child process. So if that's in the gallery, is it called thread jobs? I think it's called thread jobs, yeah. Look it up in the gallery. I just can't remember. It's something I've been meaning to do and then you forget about it and then a month or two later, it's like oh I should do that, and then I forget about it, but I think I did, but I'll make a mental note to check that. Okay.

Debugging DSC Class Resources

So, oh good, so we have some time. I have 15 minutes. What I'd like to do is go from here to more, actually a real debugging scenario, which with DSC resources. Let me just get back to the, if this is going to work. So DSC class resource. The way DSC class resources execute, you see that all these things that we just talked about can be used to debug those because it follows actually the same pattern. DSC configuration is usually run on a remote machine, it's run in a host process, some obscure WmiPrvSE host process, and inside of that host process, there's a runspace that's running your script, and so it would be really great if you could attach a debugger to it when your script is not working correctly. And just again, for a little bit of visualization, see how kind of complex it is, you have your machine on one side of town, you want to connect to another machine or maybe a virtual machine, and inside that, there is some process that DSC has created and is running your script and you want to attach to that and debug that script. I kind of wanted to take a quick second here and show something that's actually kind of cool. In order to demo this, what I did was I've created a couple of virtual machines on this laptop, and on that machine, I have a very simple DSC configuration that I want to run and that I want to debug. As it turns out, when I was doing this in my office, all of the machines are domain joined, and so when I was at home trying it, I'm no longer connected to that domain, and so I can't connect to it, so I can't do the full demo that I would like to do, but I can do enough to show how to debug basically a DSC resource and just kind of how that pattern works. One thing I just wanted to show real quickly though, which is kind of neat is that even though I can't connect to my VM through WinRM because I'm not domain joined anymore, there is something new called PowerShell Direct and we worked with the, or actually I worked with the Hyper-V team and it's not possible to connect PowerShell to a VM on your machine without running it all, and in fact, there's not a network enable at all because it goes through a new socket layer that the Hyper-V team has created. So you can do something like these are the VMs that I have and you can do Enter-PSSession now and give it a name and then just try not to type quite so much. You missed the R. Did I miss the R? Okay, thanks. (Typing) So and then so now you can manage this VM what we would just kind of need without having to go through WinRM remoting. And then on this VM here, I have a DSC class just for testing. Just make sure that everything is set up. Yeah, just the way I want it to. And so, how do you go about debugging this DSC resource class? So you start out and what the DSC team created was Enable-DscDebug and you set it to BreakAll and what that means is that now when it runs, it's kind of a big hammer. When you run DSC resource scripts, it's going to be in BreakAll mode, it's going to be into the debugger and wait for a debugger attach, but to give you some information about that, so now you can say Start-DscConfiguration, give it the class, you want to wait, Verbose. So it runs and then it gets to a point and it tells you with a warning, the resource, which is test, is waiting for a PowerShell script debugger to attach. Use the following commands to begin debugging this resource script. And then it gives you a series of commands that you can run, and of course, they look familiar because they're exact same things that we were just talking about and I'll just put it up here to see. And again, you can do an Enter-PSSession if you need to connect to the remote computer. A lot of times you don't need to because you might just have a remote desktop, and in fact, that's how I'm going to do it, so you can skip that step. And again, this is something that we've seen before in our PSHostProcess and the nice thing is that it tells you which process because there's probably a ton of WMI processes on there, so it gives you the process. Another interesting thing is that it actually runs in a non-default AppDomain, so when that's the case, you have to specify the AppDomain name, and then finally it gives you the runspace that you want to debug. And of course, everybody sees this like well why do I have to execute three commands and that's a good point and actually I have a prototype not released, not fully tested yet, but I will release at some point where instead of executing all three commands, you just execute Debug-runspace with basically a kind of a connection string. It's just a hash table. That has all the information, so you don't have to type in three or four commands or whatever, you can just do one. Something, again that I can't demo here because right now it's just based on connecting via WinRM and that's not working right now on my laptop, but it's something that we'll get out at some point. So okay, so anyway so the DSC configuration is stopped, waiting for a debugger attach. What I'm going to do, since I can't remote in with PowerShell, I'm just going to go directly to, (Clicking) why won't that maximized, there it goes, and what you can do is you can just copy and paste those commands or it's pretty easy also to just do Get-PSHostProcessInfo and if we look, we can see like well there's really only one WMI process that's attachable that's running a DSC worker, so we're pretty sure that this is the one that we want. You do Post-Process, get same pattern, and then we can do a Get-Runspace and we see that oh there's just one that's in breakpoint mode, so that's the one that we want to do. So then we can do Debug-Runspace number 3. And now this is kind of interesting and the blog, actually I created a blog about how to debug DSC resource classes. This is a little bit confusing because what's happening is that we don't see any file, right, we kind of expect it to go into the DSC resource implementation, but really what happened is when you say, when you set the runspace to BreakAll mode, it just says stop at the very first script execution point and what happens is that each of these methods in the DSC resource class is executed within a helper script, and so the debugger doesn't know that, it just stops in that helper script, so it's a little bit confusing. So what it's doing is it's getting ready to call the Test method on the class, but it hasn't yet, so in this case, what you need to do is you just need to do F11 and just step in until it finally is executing the actual script and then ISE will bring up the file. Now the good news about this, oh question. (Audience comment) So oh there was a demo yesterday about this? Yes, that this problem was fixed. Oh, I was just going to say that. Yeah, so I wanted to show this because it hasn't been released yet, but the problem is fixed and I was going to demo it on my other VM that has the fix in it. So yeah, a person in the DSC team very cleverly fixed it by before they execute the Class method, they get the class file, parse it, and find out the line number, so it actually said breakpoints on the line numbers. So the nice thing is that in the next release you won't have this problem, but I just wanted to mention it so that when people see this, they're not too confused about what's going on. So yeah, this has been fixed. And so yeah, we're just about done. So I think this is probably about as far as I want to go right now, we can go, oh there is one thing we could talk about real quickly. And again, we have some time for questions if you like, so I'll let you decide. So we're going to talk about some of the gotchas and one of them we just talked about now and that is that the Enable-DSC BreakAll stops in class in the calling script and that's been fixed, that's not true anymore. Another kind of a downside to DSC debugging is that every time a new Class method is called, they reset the runspace, but when you reset the runspace, also removes all the breakpoints, so all the breakpoints that you put in are gone, so you have to put them in again and that's going to be a little bit confusing. There's a bug on that and I think that can be addressed for the DSC team. The LCM hosting process that you attach to, that can go away. That's not always a problem, but sometimes when you have your ISE attached to it, you can do things like edit files through the remoting system, but underneath it, they recycle the process regularly, so if you do something like try to save the file that you edited, sometimes the underlying process goes away and then you get an error and that can be really annoying, that's just something to keep in mind. And then the last thing is that just BreakAll is painful. If you're running a number of classes, more than one class, it's going to stop in every one of those Class methods each time, and if you're running like 10 and it's the 7th one that you want to debug, you have to go through each one. And really, ideally, what you would like to do is set a breakpoint, so I really want to debug this class file at this line and that's not supported in DSC now, but I'm pushing the DSC team to add that. So it'd be really cool where you say Enable-DscDebug, instead of BreakAll, you say give it maybe like a hash table of this is the file and these are the line numbers and I want it to break at that point.

Wrap Up

So that's really all I have. If there are any more questions, let me know. Oh. Instead of breakpoints changing every Class method call, if my Test method calls my Get method, does that reset the breakpoint as well? All the breakpoints that are set within that specific method call remain. So like if you're calling helper functions or helper methods within like say the test or the set, those all work fine, but the next time like if it goes from Get to, so like say… (Audience comment) Yeah, so I get, yeah, so the next time it's a new method call then I'll reset at that point. Any other questions? And that's actually because Get, Set, Test through the DSC Get call is there to send up, gets one call and one of them it creates a new runspace, that's why these don't affect us. Actually sometimes it creates a new runspace. So the question is or the comment is that there's a new runspace created for each of the method calls. That's sometimes true. I know that they do create new runspaces, but mostly they recycle, and so there's actually a reset function that you can call and part of that reset is to reset when you set reset runspaces, everything is cleared including the breakpoints. I think it can be modified so that you say oh reset this runspace, but keep the breakpoints because I'm running this in a context where it's the same class or something, so you know that you want to keep the breakpoints. So I think that can be addressed and again there's a bug tracking it right now, so I think it will be at some point. Okay, well I think that's all I have then. Oh, contact information. Feel free to contact me through MVP, or GitHub, or whatever. We try to keep tabs on all kinds of different areas. I'm happy to talk to you, or after the meeting as well, or take any comments that you have. Thank you very much.

The Release Pipeline Model: Applied to Managing Windows Server and Microsoft Cloud

Introduction

So we've been working on this since about December. Yeah. We were talking on the phone and said here's all the---we like to just chat about these are the things we're seeing going on with all the different customers we visit and some patterns that are working and patterns that are not working and I'm going to cover this a little bit, but we really kind of decided we should write a paper that explains all of this in terms that IT pros will understand because there's all this chatter about DevOps and the amount of into is just flowing so quickly now that no one has taken pause to actually reflect on if you're coming from a background that's Microsoft Core infrastructure, what does all this mean to you and lay it out in a language that makes sense and that's comprehensive as to why it should matter to your organization, so that's what we decided to tackle. So this is a presentation, but we're releasing a whitepaper today, which I'll show you, I'll give a link to you, that's about 50 pages long. So for 20 minutes now, you guys just go ahead and read that and then we'll listen to your comments, no. But no, hopefully, it's valuable to you. I suspect that this will end up being the first edition and that we'll come back and make changes and I'll put up my information in front here. So before you get too far, who the heck are you? Well let me pull up the next slide with a picture of my jeep. So I'm Michael Greene. I'm a principal program manager here in Enterprise Cloud Group. I'm actually in the CAT team and I'm the CAT PM for PowerShell and DevOps. CAT is Customer Architectures and Technologies. It has historically been our job to drive customer feedback. So we run the TAP program, that kind of stuff, go out and meet with customers. I'm kind of the hunter in many ways now, so if you think about as Microsoft has become very customer focused, the CAT team ends up, previously we were driving customer feedback, well now it's like an even more aggressive role. It's go meet with as many people as you can and every time you find something interesting that we need to know about, make sure that you're just funneling all of that back to the PM team and we're continuing to evolve, so we did an Azure CAT team and an Office CAT team and more and more you see CAT team sort of working on the very latest cutting edge of what has come out and going to customers who are doing it first and being that on-site, I'm here from engineering so that when you run into things that we didn't expect, there's a direct conduit for those first couple of deployments to help make adoption easier. So it's like I'm from Microsoft and I'm here to help. Yeah. I should get a van with a logo on the side. I just wanted to put up, oh so I've been with Microsoft for 12 years, out of that, I spent from 2010 to late in 2012 in operations in what was called BPOS, now called Office 365. So Don Jones taught me PowerShell in version 1 and in 2010, I spent 3 years in operations PowerShelling everything, so I've lived a lot of the experiences that have resulted in things like what we're talking about today, so I've got a lot of hands-on experience of PowerShell. If you want to contact me, you're better off sending me a letter than sending me an email this week, so if you want a response, but you can hit me up on Twitter. I don't mind doing a little dance where you have to follow each other back and forth before we can DM each other or you're welcome to just tweet at me and we'll get in touch with each other. And my GitHub repo is Mgreenegit and we'll be referencing probably some stuff there. But Twitter is a good way to get a hold of me. When I come to this event and I walk around, for me, it's like walking amongst celebrities because I'm not really like into Hollywood celebrities, but when I look at Twitter and Slack channels and blogs and stuff like that, it's all of you guys that I'm reading every day, so coming here and looking around at all the badges and seeing those names, it's like whoa, that was that guy that I see every day and I've talked to him every week for the past 20, but I've never met him in person, so it's really my pleasure to be here. Great, and I'm Steve Morawski. I'm a principle engineer at Chef on the community engineering team. I do a bit of DSC in PowerShell and stuff for a few years and Michael and I just kept bumping into each other a lot at community events and at customers and talking about these same challenges, and so he's like hey here's eight pages I just kind of scrawled out on my thoughts on this topic and let's go and do stuff, and so we started collaborating back and forth and I just kind of rode along on his coat tails to help get some feedback into the paper and all. And for me, this concept is one of the most critical things as you start to move towards more DevOps environment and workflow. If you do not seriously look at addressing some of the concerns that we're going to walk through here, you will not realize all of the benefits from working with things like Desired State Configuration or Chef, being able to, you will not approach some of the big wins that you see in reports like the state of DevOps and things like that where you see these faster times to recovery and reduced failure rates and things like that. And so, for me, this is a---

Shift

This is a super personal topic because this is what's going to help make your lives better. There's a lot of learning to kind of get our heads around, a lot of different terminology, so I'm going to get out of the way and let's dive in because we've got lots to cover. Yeah. So I'm going to start with this slide because this is how big of a mind-boggling change this is. So I was talking about this subject with a friend and he reached up on his shelf and handed me a book. I believe it's the Theory of Scientific Revolution. It's by Thomas Kuhn. But then he spoiled it for me and he told me when you read this, all you need to get out of it is the following. And so, I searched for that and read that page and I didn't read the rest of the book, but I'm sure it's good. The point is in order for a scientific revolution to be an actual revolution, to have a paradigm shift, you have to satisfy two things. There's a new set of problems. So you're thing came out and it solved those problems. Great. You also have to solve all the existing problems. So when a new thing comes out in the industry and we're looking at it and saying is this something that's going to be huge impact, it's great, yep, all these new cloud adoption and deployment type stuff, it really helps out with that, but if it doesn't help solve that, by the way, I'm going to have servers that are not real, I've been supporting these things or that model for 10 years and I can't just do continuous deployments and knock these things over once a day. I'm going to support them like pets even though we want to support everything like cattle and that's going to happen for a while, it has to support both, right. The paradigm shift has to make your life better for all of the scenarios in order for it to actually be a revolution, otherwise, it's just more tools, right. So when I go out and the only like universal truth I've learned in my life to this point is that there's usually not an extreme, right. Usually, things sort of land somewhere more at a moderate place. So everything starts looking like a spectrum. If I go meet with 10 customers, I'm going to find some that are really on the cutting edge. I'm like oh the whole world must be doing this and then I'll go to the next customer, it's like oh, no they haven't changed at all. So as we met with people, I really found two voices. The first was, yeah, yeah, we've been doing DevOps, so thanks for showing up and having an interest in what we're doing because that's neat. What we've been trying to figure out is how do we bring Windows Server into what we're doing, so when we go meet with those customers and help figure out what their model looks like and how Windows Server can be part of that, and obviously, PowerShell takes a huge role for something like that. On the other side of the spectrum, you meet with customers and you explain what you just learned from the first customer, I don't know what just happened, there we go, and they say yeah, yeah, that sounds like you're speaking a developer language and that's not really an IT pro thing. So 80 or 90% of the people are in the middle and they're just trying to keep things from falling over, right. So you can walk up and talk about hey there's this other way people are doing things now and it's really interesting and I think it's better and they're like look, I will look at making my life better after I'm done with all these things that make my life hard. So I love that animation. There's like three or four different versions of it. So we wrote this paper and it occurred to me just a second ago that I should prescribe a hashtag. If anybody wants to tweet about it, you're more than welcome to. It's live as of this morning. That short code should work. It goes directly to the pdf, and if you use that hashtag on Twitter, then I can search for it and I can tell my boss that lots of people were interested.

Configuration as Code

I'm really excited to get that out and not have to think about it anymore. I'll leave that up if anybody else wants pictures, but it's just aka.ms/thereleasepipelinemodel. So this slide, I really put in for future presentations. We actually are going to make these slides public and then continue iterating on them so that anyone from any organization can use them because I feel like I need to weaponize everybody here to go back to your organization and be able to have this conversation. So I want to make sure that this slide contains the information to do that. So you probably are all familiar with configuration as code when I say that I'm talking about things like DSC. The way that I talk about that with people who are new to the idea is that if you've been managing Windows Servers or anything in IT, there's lots and lots and lots of APIs. So the way that you work with Active Directory is different than the way that you work with WMI and the registry and the file system and DNS and then you've got third-party software, you've got hardware devices and they're all different. So the idea of configuration as code in the most simple concept is that I'm going to bring everything back to just property value. And what's happening under the hood is all being performed by scripts written by people with subject matter expertise. They are trusted and have given me good tools. But at the end of the day, that file becomes my living documentation of how stuff is built. So we all hate documentation, but whenever your documentation is functional, then it takes on a very different role. It's something that I had to create in order to get things where I wanted them and now I don't have to go write the paper with the tables that explains how the server was built. So you're all familiar with DSC. I will mention that if you didn't go to other sessions that relate to this Azure Resource Manager, I also look at that as configuration as code. In fact, most of the examples I'm going to go through today are Azure Resource Manager deployment templates, but this same model, and we'll discuss this more at the end, can apply to lots and lots of things. So any time that you see an opportunity to configure something as code and put it through a release pipeline, look at that as an opportunity, do an investigation, is this a way that I can make this work better. And the very last bullet, I'll just let you take ownership of the last bullet. Yeah, so there's a lot of great tooling that's being implemented in the DSC space and the ARM space, but at the end of the day, none of that makes you a subject matter expert in the things that you have to operate, so I can learn Chef, I can learn DSC, I can learn ARM, that lets me spin up a whole lot of things, but I still need to know how to operate them, I still need to know what the settings mean, what the things are. So nothing in this is going to replace knowledge about how these systems work. If you have in-depth Active Directory experience and knowledge, that is a major plus moving into these things. Even though we might not be clicking around in some of the familiar UIs, being able to deploy a resilient Active Directory infrastructure as configuration as code is going to lean heavily on your experience. Being able to understand how Windows services behave, especially in like Cross-platform environments. You've got Linux guys come over and say hey I want to run this thing as a service. No you don't. That might be what you wanted to do on Linux, but really you want to get this thing in Task Scheduler or you need to create a new service that knows how to register itself and do some things. You still need to have that platform knowledge, so none is that is being obviated by the configuration as code space, though it may seem like it because it's kind of a simplified interface. It's key value and whether it's ARM, whether it's DSC, whether it's Chef, it's oh I declare a resource and then give it some properties. But yeah nothing will really kind of step---

The Release Pipeline

Step away from and take away that core knowledge of the systems, the expert knowledge that you all have. It actually becomes more important. So these are the four things. In writing the whitepaper, we had to boil it down and every one of these has eight things, but we wanted to get down to what are the core concepts if I'm looking at this for the first time that I can categorize this information to pieces I can digest one at a time. So we're going to go through each of these and the pattern in the paper is written in a similar fashion is what is this and then what does this mean to me if I'm working in operations, right, because these things all, I mean, especially if you have in-house developers, these are things that they talk about right, but these are not things that we normally talk about in operations. I'm not expecting you to read this slide, like I said, the slides will be available and this is obviously in the paper as well, but we tried to capture what are the problems that this actually is trying to solve. So we're going to dig into these a lot, but I hate just reading off a slide, so I'm going to dig in deep to those. So if we talk about source first, that's the first big bullet. What did we do before? Right, so I'm going to describe change management, even though we're talking about source control. So I wanted to introduce a change, right, so I spin up a VM, I'm going to prototype the change there, and then I went and I submitted the change form and then I went to my change advisory board and I present it. I've tested this, I'm going to introduce a new policy setting, I've created a VM that looks like that server, I created a sub OU, I moved the machine object into that sub OU, I applied the policy there, if all goes well, then I'm going to also introduce that policy to the OU, to the policy that applies to the whole OU and cross my fingers that things don't explode because I don't really understand what most of these servers look like, right, because who knows if yesterday somebody RDPd into it and made a change. I have no idea. So I need a rabbit's foot and a change form and I'm going to see how this goes. When we get to change advisory board, we're going to argue because somebody's going to say why do you need to make that change. It's probably going to break my stuff. We're going to try to catch, if two of us are going to implement something that's similar and we might bump into each other even possibly in the same maintenance window, so that's a little scary, and then, of course, we're going to go to the build documentation for that server and update it because we always do that. So the first that happens is we don't prototype in a VM and we don't update our docs. We just show up, well we might submit the change form, right, and then we show up and present. The next thing that happens is we're the cool kid because we're the expert, so we don't ever even submit the change form. We just walk in, throw down our laptop, and say Saturday I'm going to make this change, deal with it, and that's kind of change goes and then arguments happen. And then at the end, we just present an argue, and then lastly, we just argue, and it's pretty much how change management actually happens. That's what we did before. So what do we do now? We start with configuration as code. So instead of doing documentation at the end, which we never did, we just update the configuration file so that is the documentation for how this thing is built. It's also how the machine, it's how the work is performed, so if we didn't get it right, it didn't get built that way, so we know that this is right as we go. Included in that source control, so when I say source control, I'm talking about things like TFS, talked about subversion, git was the one that for some reason what not landing in my head and that's the most popular one. Not visual source safe. Not SharePoint versioning. It could be a file share, in fact, I talked to--- no, no, no--- it should not be a file share. It is not a file share. But I will relate it this way because it makes it a little bit easier to digest. a lot of people in this room probably have a background where at some point you worked on desktops and you probably had a file share full of apps that you deployed using something to those desktops. Think about it like that, right, you had a folder structure and you had versions of those apps and you put them there, and with those apps, you probably had a batch file that did a silent install. We just want to start thinking about it that way, right, so we're going to take the way we're configuring these servers, we're going to pair it up with a script that automatically deploys that file, the Pester scripts that evaluate the quality of that and we're putting all that stuff together. And then your release of it could be a web server, it could be Active Directory domain controller, it could be an Exchange box, whatever, you're defining a version that's yours. So that's you're, like you might be working on DNS Server version 7.4, and 7.4 doesn't mean that it's DNS version 7.4, it means for you, this is your 7.4 release of that part of your infrastructure and that's kind of how we can relate source control back into this. There's a version associated with how that infrastructure is configured right now. So we make the change. We're going to talk about testing in a lot more depth, but you're actually going to run tests locally on your machine to say yep, it looks like I've done this correctly, and then you're actually going to do a push. So if you're familiar with git, and even if you're using TFS, git.exe will be your front-end in most cases. You're going to do a clone to your machine, you can pull changes that other people have made, you can push your changes back, and if two teams are working on this at the same time, that somebody owns that repo, they can do a pull request. It means that they can take DNS 7.4 offline, maybe they wanted to submit a change to you, they can submit change .7.5, you review it, it'll show you line by line what exactly changed. We're going to take a look at this. Here's what's most important, as those things get checked back in, some people call it check in, some people call it a push, there's metadata that happens. Who made the change? When did it happen? There's comments to explain why did that happen and you can literally hold these things side by side and see I can see right here that now we were static IP, now we're dynamic IP. Like all of these things are side by side in a window and you can see exactly when that happened. The state of DevOps survey that's been going on for the past three years, the fourth year of the survey is going on now, Dr. Nicole Forsgren has been doing research on the statistics based off that survey. One of the findings that they had was that change argument change control board thing there is way less effective than source control and peer review of changes as they go out into your infrastructure, and so there's a paper that's out there. I will be blogging the link to that probably later today if you want to dive into kind of the science of it.

Managing Changes

But, there is actually, there is good statistical information that shows that peer review of changes in source control is way more effective than any change control board that's out there. So I'm going to demo this first with screen shots and then I'll walk you through it, but when you go to repeat this presentation, and I hope you do, I can share with you how to recreate the environment, but in case you don't have time, I want to make sure that you're armed with showing people what's it look like to manage change if you're using source control. So this is using TFS, this is TFS Express, so you can go to microsoft.com, look for Visual Studio under the bottom of it, I think you can go to visualstudio.com, just go to the bottom, and there's a link for TFS Express, which for five users or less is free. So the environment that I'm connecting to is this environment and it's just a TFS Express instance running in a VM on Azure. Like you said, GitHub is a perfectly great place to do this kind of thing or if it's personal information or private information for your organization, you could use a private repo there, you could use GitLab, GitHub Enterprise, there's just a variety of different source control platforms out there. TFS is nice because for everything we're going to cover today, it offers it all, so it's a one-stop shop. So this is just what it looks like. That's a project that's checked in. You can see the files, you can see when the last change occurred, you can see the comments on what changed. If I go look at this in Visual Studio code, so this again is free download. You probably used it for things like DSC and JSON and other projects, but you can see I'm looking at making a commit, so I'm saying I'm going to retire 2008-R2 because I'm only going to support 2012 and 2012-R2 now, so I'm just removing that from my allowed values for the incoming parameters and I'm going to check in that change. So you can see it's showing me exactly what changed. Now that's kind of an easy one because it's just changing parameter values. It could be adding a network adapter. It could be changing the way that the network is configured. It could be adding a new virtual network. The point is that you can go see this is what I had, but I removed the line and this is what I have now and you can do that commit. You can actually also do your push from within Visual Studio code and a variety of different editors. You can also go out to the command-line. So in this case, I made my change, but before I submit it, I'm going to run tests on my workstation because I don't want to embarrass myself by checking in a change that I know is going to fail. So I just invoke Pester. This is the weakest set of test scripts ever, but it's just a basic deserialize the JSON, make sure that everything I expected is there. So I'm just going to run Pester and it's going to make sure that what I'm checking in is not junk, right, that I didn't make a silly mistake along the way. And then I do a git push, in this case, it just says, yep, you modified some files, great. You own the repos, so no problem push them back up there. At that point, I can go look at my source history. In the browser, I can see yep, I retired 2008-R2, and so if anybody ever wants to find that, it's easy to search for and if I drill into that commit, I can see once again in the browser that yep this is now stored forever. I can always go back and see exactly what changed when. I was logged in as TFS1, so that's who, it was Tuesday, April 5 at 5:54 PM, so I can see when, and I can see any comments I wanted to add. The next thing happens is build. So what did we do before? We waited. After that change control meeting, right, and we came in on Saturday because we hate our personal lives. We made sure that all of the servers that we were going to work with are docked in RDCMan so that we can remote desktop into every machine where we're going to manually make change. That was always my prep for going into the change windows. Do I have my RDCMan file all set up with the password stored and stuff like that. So I connect to all the servers, I make changes, and then I have to do a reboot, which is where I sweat it out for 30 seconds or so or 4 minutes if I'm waiting for BIOS to go through all the checks. Hopefully I get everything in order, and then after I do a little dance for good luck, then I go through and manually make sure that everything still works the way that I think it should. Can you show us that dance? What's that? Can you show us that dance? Maybe at the pub crawl tonight. Okay. I'll break out the change window tribal dance. So that last one is the scariest part, right, validating that we didn't break something. And then if you did, that's probably a six hour or an eight-hour change window, so now the clock is running and I've got to get it fixed, but if I don't know, maybe I'm not the expert on this, I was just told that there's a 0 day, so I've got to make the change. At that point, now the clock is ticking until Monday morning and users are going to come in and it's going to work or it's not going to work and that's how it goes. Alright, that's the life that we put ourselves into week after week or month after month, so that's what we've always been doing. What do we do now? As we check in that change, there's a variety of ways that this can happen, but it's going to run the scripts, the supporting scripts that we put in there. So it's going to start by just evaluating quality using the tests that we wrote and checked in with that configuration as code. That can happen as a web hook. If it's TFS, it's all one box, so it's all part of the same system, and I'll show what that looked like to turn it on. Other platforms have agents that monitor a source control and look for changes, you can also schedule it to happen, you can manually key you a new build, but when it says build, I mean especially like if you took Computer Science courses in college, you think of build as I'm taking my source and creating an executable, like that kind of a build. Yes, that's true, but in this case, the build service is just running a PowerShell script to do these things. So build really just refers to there's a server somewhere that's going to take care of running these supporting scripts to make things happen for me. So all that gets stored in source and we're going to, go ahead. And the reason it's called a build is at the end of that process, we're going to have some artifact. It may still just be a PowerShell script, it may be a module, it may be MOF files, but at the end of the day, there's going to be a result of that build process that we're going to move along to do some enhance testing on or just to move into deploying in the environment. So it does map really well to the concept of build because there will be some output or some result of that process. Yeah. An interesting thing that I have learned from customers is that yes, you could go create your build server and that's the central build server for all your things, but a lot of people when a team creates a new project, they have their own build server and all these teams have their own build servers. They actually have configuration as code to go build a build server and the reason for that is once everybody's doing this, why have down time and releasing change just because somebody else's machine is down and that kind of stuff, right. It's just performing a function. You can all have one place that you go look for source. You actually want to get to the point, and we'll talk about this in test, where that build server is doing stuff almost all the time and we'll talk about that more in test, but think about that as you can really design this for yourself, which means you don't have to change the whole world to make this work, right. You could have this as an instance that handles just one project that you're working on. This is also a really good opportunity to look at JEA because now you're performing everything automated, so why not have a JEA. If, number one, you're building a machine using configuration as code, well that's how you put a JEA endpoint on something. And number two, why give that machine total access? Why not have a set of commands or a constrained endpoint that only allows build to do what it needs to do and use that. And at that point, you're delivering change using build, you could have constrained endpoints for doing maintenance activities, so it's not to say, I don't want to confuse this with I'm only going to do this and now if there's an emergency, I'm out of luck, right, so you do want to get to the point where you're not making random changes because then it didn't have an opportunity to go through test, but there's still going to be maintenance activities. There's going to be weird things that come up. You might have to go get information from boxes or go check the state of things, so that's an opportunity to use things like Azure automation, use JEA on the machine and build out this platform. And since I don't mention it later and I don't think Joe mentioned earlier in the week, Azure automation can be managed using ARM. So you could actually check into source an ARM template that includes a script that should run and when it should run using hybrid runbook worker in your environment. So you can coordinate all of these things together. We have a question. Oh, sorry. I was just going to say what you described it kind of fits in well with my experience in that this is the model of what it looks like at the endpoint, right, where probably somewhere on the continuum or maybe at the very beginning of this, so we're not going to have all these pieces. So as we start to implement one thing, right, and then we get that working right and then we get, oh why don't we add Pester or why don't we add JEA, and so over time you're evolving towards that this end goal. That is exactly it and you start with these things pretty much in the order in which Michael is describing them. You start with source control---

Triggering a Build

---because that ends up being your single source of truth. Then you start adding your build process and that's where you can start plugging in a bunch of other stuff like what he's going to continue talking about here so. I'll show you what it looks like and then we'll get to the big one. So when you go to demo this, what we're really going to demo is and I'll do a comprehensive live demo at the end, but trigger a build, verify that things happened, look at the report, right, so that's basically what we're going to do. So in this case, this is actually the script that we'd run. This is formatted using PSake. In the live environment, I'm using a new version of PSDeploy to replace this big code block, and so my live demo it's a little bit cleaner. But if you're familiar again back with Computer Science type concepts, you've heard the command Make. On a Linux environment, you might be spinning up a project from source and Run-Make, so PSake, pronounced Saki, but the concept is to use a DSL that's similar to make or rake for Ruby and have that in PowerShell. So PSake is an awesome project to look at and it allows you to chain together a lot of really complex tasks. It's a lot to just a glance, but it's definitely a project to look at. For my build definition, which just means what are the things that are going to happen whenever the build runs, I just have a PowerShell script that I'm literally, just go run that, pass it arguments, and the arguments are just coming from stored variables. So just like in Azure automation, you can store assets, store variables, store credentials, things like that, same thing. So my build literally is just run a PowerShell script. I have two steps because TFS understands how to collect published test results, so I have Pester output to XML and then this will go pick it up and make it in a nice reports for me and that's like a free bonus. What we'd want to get to and Mark Grey and I have been talking about this, we want to publish a project, you should really be able to sanitize this to the point where you could take that project and just make it work in any build system. It could be AppVeyor, it could be Jenkins, it could be Travis, it could TeamCity, it could be Visual Studio, TFS, it could be locally on your machine, you should be able to just run build that PS1 and not have to worry about it, right. You might have incoming parameters, but that's about as complex as it should need to be. I just wanted to show quickly for this particular project, I've got a whole bunch of stored variables. That's just so that my scripts can become as generic as I can make them and then I really want that to be scaffolding that I can just if I want to go create new project, I shouldn't have to reinvent the wheel when it comes to what does that look like to check it in and have build run it and have tests run it and run a deployment. We should pretty much just be able to pass it different parameters and it should just handle it on its own. And TFS is a good example where you can encrypt information as its being stored. This is where for triggers I can run it by clicking Q. If I check that box for Continuous Integration, then whenever I do a push, it just runs. And if I click scheduled, of course, then I can set a day and a time. Yeah. (Audience question) Steve might know that better than I do. Very. So just from a standpoint of flexibility or---so the question was what's the difference between PSake and MSBuild and the answer is a lot and not very much. They're both build descriptor languages, but because it's PowerShell and not XML, it tends to be a lot more readable, it's a lot more lightweight to write and kind of dig through, but out of the box it doesn't have all the same steps and things as MSBuild. MSBuild is really optimized for building .NET or .NET DLLs, executables, web projects, stuff like that where PSake is a little more open-ended and just like Rake is going to be really good at building Ruby projects, and Make is going to be very good for your C projects. PSake is a really nice easy addition to not only .NET projects because you can call MSBuild tasks through it, but you can also easily do a lot of your PowerShell-type testing, easily get into PS Script analyzer and Pester and because you're in PowerShell at that point. I should call out for PSake, so you see task depends, so task test depends on clean, which just means run as script to make sure. Because this is always running over and over again in the same box, clean up anything that would have been there from the last build. And then deploy depends on test and then test depends on clean. So when this runs, default depends on deploy. So if we follow this backwards, it means if your tests pass, go ahead and deploy and that's why it's interesting to say if you check that box for continuous integration, it means you edit the file on your machine, you hit Save, type in your commit, hit Push, it's going to go to build. If all of you tests pass, you're going live, right, so you could literally just from your workstation go there. Or you go into your dev environment or you're going, wherever that target is. Exactly. I'm like that's the most extreme that I've ever seen it, in fact, I don't know of anybody that I've seen go that quickly into production. There probably are for some projects. Yeah, I think we've got some customers that do that. Yeah, nice. In most cases, you get to the output, so in this case, for DSC maybe you've got the MOF and then somebody eyeballs it. What's interesting, so we'll look at PSDeploy in the last section, actually I'll come back to that section. So I just want to come back to that point you said hey that's kind of extreme and I don't know a lot of people that go all that route, but I'll bet every single one of you know somebody who's downloaded a script off like TechNet or something and gone and run it in production and that's got even fewer quality gates than what we've just demoed here. That's true. Right, so this is more rigor than what a lot of us are currently doing today in our environments. Very true. So it's not all that crazy. Sure. So I noticed that you have the test in the beginning, right, so would you consider like you talked about the fact that after I do this deployment, right, now I've got to test maybe my apps and different things like that afterwards and I'm not really seeing that as a step. Is that part of my code in terms of my configuration code? I don't know what's going on there, but it's hanging. Does it need to have like okay here's the five applications test that you want to do? Exactly. That's the next section. But that's exactly right. Your head is in exactly the right place. In fact, that's the most important thing. I just wanted to show real quick. I just love this that as you run the build when you click on Console, there's a PowerShell window right there in your browser showing the scripts that came from source control and everything is just running right there in your browser and you can watch it and it's like oh, this work that I did is happening somewhere and I can do that all day long. So you might have a build fail, no problem, you can see the output, so you can go back and look and you might have it send you a notification, you might just go log in the website and see it, but now you can keep track of okay this worked on my machine, why isn't it working somewhere else and help troubleshoot those types of things. Let's see. You also obviously can have build succeed. What's nice is when you come back and look at this later, you'll see each of the steps where it went and got from source, it ran PSake, and then it published test results, so you can double-click on those and actually see the full log output in the history of these builds.

Testing

I was going to say I think I got a screenshot of that, yep, so if I go look at there's the history of my builds and then we get the test, so everything that has happened in this environment, you can go back and see what it was. So testing, what do we do before? Create our VM, install the OS, probably installed an app or built a service of some kind, we tried the change, it could be manually, it could be, yep sure. Sorry, I've got a question on that last part. I kind of use AppVeyor a lot for my CI and one of my biggest issues is trying to get AppVeyor to work, so I don't do any 48 builds just trying to get the AppVeyor configuration to work. Yes. Is there a way to in this product, I guess I want this product to just delete that build and be able to keep rebuilding because I'm not really incrementing my code, I'm incrementing my test code just to get it to, you know what I mean, there's really the environment building, you know what I mean all of a sudden like I can't get modules to install from the gallery because they're NuGet versions and not up-to-date, something like that, can I delete a build and just re-run the same version in this? Most source control or most build-type environments have a way to expire out history because it's a database somewhere and you may not care about it forever, in this case, I think I've got it set to like 10 days or something because I noticed when I logged in last night that these builds from last week when I was playing around, which I had like an embarrassing number of failed builds because I was just banging on it and banging on it, but it does, you can do that or you can keep them forever. It's configurable. But I do exactly what you just described, which is I think I got everything right and then I have to figure out how to make everything work in this other environment that is generic. But it actually leads to me having better quality in the end and solving more interesting problems, but there's a lot of lessons learned as you go there. Like with AppVeyor, like I don't have a way to get my hands into like their, you know what I mean, their apps are spinning up, so I mean my last recent one was all of a sudden, yeah, I just couldn't install modules from the gallery, so I was like yeah their NuGet whatever wasn't up-to-date and even though I spent that much time trying to troubleshoot that, the only way to test that is to build. There is a way, by the way, for AppVeyor that you can inject a stop and then remote desktop into their, yeah, but we can talk about that. We had another question coming up over here too. So as part of build paths, one of the steps that we do before we do any deployments is to separate the artifact, scan with a colon if you see any part where the applications that are open that are _____. So we use some conference part, I would like to get some feedback on you guys any do this before any specific tool feasible as part of the objective's plugin and would you do a ---validation? So yeah, you can definitely inject all sorts of security testing into this pipeline. This is just giving you different points as which you can inject various test phases, right. So one of the things is with build we end up with an artifact and build is usually on like a per project basis, so it might be how I manage my AB servers, it might a particular application, but at some point, these things come together in like a test environment to where we can prove these things out and that's where our configuration management in our applications kind of all land together and that's the environment that we want to do a lot of that security testing in things. In the Chef ecosystem, we actually push that way earlier with Chef compliance and inspect where you can do that security and compliance testing as part of the whole dev process all the way up through the cycle, but where you get into a lot of the, we're going to test how this looks deployed on a test server, maybe do some fuzzing of URLs and all that kind of fun stuff, that's going to happen kind of after you get your artifact built and deployed into some kind of staging environment. And so that's where we start coming into right after Michael gets through kind of just. There you go. We should hit the accelerator too. Yeah, we're death five. Yeah, let's not go there. That's not going to happen. So to shortcut this, your build is going to run all the tests that you stored in source control. We're lucky that we're running PowerShell because script analyzer is out there. It's a whole just volume of rules that you can get for free and it runs from the command-line and you can automate it, so that works out really, really well and then you go all through Pester. In the next, you're over there for the next time log. Yeah, I think 2:00. Okay, because he can show you what Kitchen looks like, so you can think about test Kitchen from Chef working together in this environment with PowerShell, with Pester, but what if for this change you actually get to test it against 2008 R2, 2012, and 2012 R2, not that anybody would have diversity of machines across their environment and have to deal with something like that, but that would make doing that again back to just a property value, what platforms, these three, okay cool. And then it's just going to go run those tests, spin up a VM on whatever you want to spin up a VM on, run these tests, save the results, go to the next thing. So all of these tools are all about making complicated things simple. And then definitely go look at OVF, Operational Validation, it's under github.com/powershell/operationalvalidationframework, I think, github.com/powershell and you'll find it. But it's a project that's being spun up around how do you know that after you've deployed the change that everything still works. Go ahead. I was just going to say, and unfortunately this is kind of what, we're getting down to the wire timewise, but this test phase, source control and this test phase when they did the statistical analysis of all the parts of continuous delivery and what adds the most value, it's source control and the integrated testing and automated testing. So the more effort you put into focusing on source control and all of the automated testing capabilities, those are going to deliver your biggest bang for your buck as you start to adopt more continuous delivery-type practices. And that's been proven out. That's definitely been a thing in the Chef ecosystem. We are super heavy on the testing side of things because it delivers such great value, so definitely spend the time looking at all the different test tools. Yeah, I always tell people if there's one skill in 2016 that I would go learn, it's Pester because you're going to be equipped for everything that comes at you after that. So if you only have time to go learn one new thing, Pester is just hugely valuable. TFS is nice because it goes and picks up those results. It works because Pester can output into an XML format that's an industry standard for NUnit. So it just picks up those and brings them in. I didn't have to do anything to make this reporting work, so it can say yep, for each of those builds, here's your test history. It all worked. You can obviously see if I had 1,000 test instead of 4 that this would be really of value. There's lots of charts down that side--- I can drill in.

Release

I can see the results of any one test and then I can create bugs. So I can say, Steve you checked in junk, here's a bug back to you that this doesn't work right. That never happens. So I'm skimming fast. Release is interesting because it's the most exciting thing to me, but it's the least interesting part of the demo, so it's where I've been putting all of my thoughts lately, but as far as demo is concerned, if you're running this, something got released, so it's not that exciting. But basically, something is going to go create the environment if needed. If it doesn't already exist, it's going to manage any changes. So as part of build, you might have in that script, go out and configure a load balancer or go create a new volume on the SAN, something like that, right, so any environmental changes happen there. What we did before obviously was go dig through all these things manually and that would include now that I've built this machine, I'm going to go to Windows update and get it fully patched. I'm going to make sure that antimalware is there. I'm going to onboard into our backup and monitoring services, all of those things. It's why it took us three weeks instead of three minutes because we did all of those manually. And then we announce a Go Live, right, which is again where we get gray hair. So what do we do now? Build again. Just that's another thing that it takes care of. So everything that we need to do to make that release happen is stored in our deployment script in that source control and it's tested using Pester, we've tested it locally from our workstation. As part of the build server doing things, you might have multiple environments, so you might actually do a release to a quality assurance environment and then that's it for the automated build and then you go back and trigger something else that goes into production. I've seen a variety of things go through this and that's what we talked about promoting through stages. You might have cut over scenario where you have a green/blue and you use a load balancer, so we're going to deploy to green and then flip from blue to green, things like that. Just to do a quick demo since we're now out of time. I'm huge fan of PSDeploy. So Warren is here. I think this project is absolutely awesome, and so we've been working together on, he's just a champion with PowerShell and has been making some interesting changes to make it work like Pester and PSake so that all of this is just one cohesive story, but I contributed an ARM plugin so that it could work with this, which I think I broke last night, so my demo fails at this step, but you can see it's the same concept, right, Deploy using ARM from that template to that resource group. You can do tagging, so your tag might be production versus QA, which means it would just be a variable that you could use from your deploy to say I'm going just set this for QA and then tell PSDeploy go run all the deployments that are tagged for QA or that are tagged for dev or they're tagged for project name or whatever. One of the things I can't wait to try is use this because you can have these things depend on each other and have Azure and Azure Stack deployments tied together, so I'm going to release a service onto Azure Stack, but I'm also going to have it go create an Azure automation account, put my modules and configurations there, and then Bootstrap those machines to Azure, and do a one-stop hybrid deployment. I can't wait to prototype that. All of these tools make those really complex, mindboggling, new things very easy. And I can now take the lessons I've learned in the past and bring them forward.

Wrap Up

Just to start wrapping up, these are the, since we're at PowerShell Summit, learn Pester, learn PSake, learn PSDeploy, go take a look at OVF. As you get further down the line, there are so many DevOps tools, so as we update this slide, we'll continue growing this list, and in the whitepaper, we actually call out a wide variety. Yeah, and one thing, I have a session on essential skills for DevOps world or something to that effect later this afternoon. Michael is going to come join as well, so one of the things we're going to talk about is build pipelines and the importance of continuous delivery and testing and all that, so we can definitely have more time to dive into some of these concepts if you still wanted to. We will make this probably my last slide. Yep. This is a huge point, so I just want to land this before we leave. Coming from an infrastructure background, it's in our nature to say I found a new thing, this is my thing now, and I'm going to focus on that and there's a couple of things that make this type of platform work. One is you've got to get buy-in, right, from others in your org. You can't do this alone because it only works if everyone agrees that the tests have to pass before we move forward. The other is, keep your options open. So in the paper, we talk about things like how do you use this together with ConfigMan and what's it look like to use this together with Group Policy because you're not going to cut this over overnight and it's your job as you are the people, right, you are the people in your organization that they look to as the lead architect that says how do we do this smart guy, right, and the answer is it's not going to be a okay, well I stopped using this tool and now I'm using this tool and now we're a lot better, right. That's just a tool change. It's the process change that makes this so valuable. So we don't want to look like this with your precious. And now I've got my build server and now I'm just linked to this stuff, it's all I care about now. Nope, you've still got all the tools you had before. It's a process change and you can bring all of these things forward and that's how you be the hero for your organization and help them get into this new world. So yeah, we'll talk about this applying this same model to everything and making it visible and everybody should be on success. So we'll continue onto this such thing then. Thank you. Great.

Building Unconventional SQL Server Tools

Introduction

Okay, so today we're going to be talking about Building Unconventional SQL Server Tools in PowerShell. And I speak at a lot of SQL Saturday events so and I've actually been working with SQL Server longer than PowerShell and I've been using PowerShell since version 1.0, but I started with SQL Server in the 6.5 days. Also, notice at the bottom of this slide you can download my demo code if you want to follow along in the presentation. There's just a couple of minor tweaks I made to my code. I had somebody, an attendee in my last session recommend that because the code is not always so easy to see on the projector. Anyway, so I've been working with SQL Server 6.5 in the mid-90s and I guess they originally designed that product from Sybase and it was back in the crazy days when you could actually stick moldable databases in the same device, which meant you could take databases that had nothing to do with one another and stick them in the same physical file, which yeah, was insanity, not a good idea. So the product has gotten a lot better since then. I've done certification since 6.5. I did like the MCDBA on 7.0 2000 and MCITP on 2005. I can't remember if I did the 2008. I do more PowerShell nowadays, so I kind of figured that I'm not worried about SQL Server certifications. But I am the DBA for the company that I work for. I'm also the Exchange guy, the AD guy, the SAN guy, the firewall guy, you name it. I'm the only engineer. But today we're going to building some unconventional SQL Server tools in PowerShell and you'll see what I mean by that. We'll go through this slide, but I do have one question for the audience first. Who was in my session on Monday? Okay, I'll try not to repeat myself too much. If it's really important for the attendees that weren't there, I'll repeat myself. But if you didn't see that session, I would definitely recommend taking a look at it even if you're not working with DSC. It was DSC related, but there's some good tips in there if you're just building any kind of PowerShell tools. So my name is Mike F Robbins. I'm a Microsoft MVP on Windows PowerShell, SAPIEN Technologies MVP, leader and co-founder of the Mississippi PowerShell user group, co-author of Windows PowerShell TRM 4th edition. I wrote a chapter in the PowerShell Deep Dives book. I was winner of the advanced category in the 2013 Scripting Games. And if you want to learn more about me, just see my blog site. Okay, I've got some questions for the audience. How many IT pros do we have in the room? Okay, good. How many developers? And you may be both. Okay, what about DBAs? You may be like me. You're the reluctant DBA. Okay, and who's working with SQL Server today? Okay, good. I'm glad. The session is going to start out where it's not too advanced, but by the end of the session, it's going to be deep because I didn't want to start out by talking over anybody's head. Okay, so who's writing Transact SQL today? Okay. And who's working with the .NET Framework? Okay, and who's using some type of source control system for their PowerShell code? Okay. And who's using some sort of unit testing system such as Pester? Okay, and the source control and unit testing, that's something that you definitely want to learn more about. And who's using the PS Script Analyzer? Okay, that's something that you can use to test your code with for best practices and if you find that your code is just horrible based on the PS Script Analyzer, there's actually some reading material I've got for you on the resources slide because what you want to try to do is write your code so that you don't have a lot of problems with it and it's really a thought process and that's what my sessions this week have been about is I'm not going to give you a fish or a toolkit, but you're welcome to my toolkit because they're on GitHub. If you want to start with it, just support the repository and I do take pull request, but I want to teach you to fish. I want to teach you how to write your own toolkit. I want to teach you the mindset. So the information we're going to be covering is we'll talk about the SQL PowerShell module and snap-in. We'll talk about the SQL PowerShell provider, talk about SQL Management Objects, the .NET Framework, Transact SQL versus using cmdlets, functions, modules, toolmaking, automation, and we're going to use PowerShell to write some dynamic T-SQL code because why not use the tool instead of doing things manually, even writing your code manually, write PowerShell code. I've actually used PowerShell code to write PowerShell code for me. And I've got a good blog article on doing that with DSC configurations. And I presented for the Omaha PowerShell user group. That session is recorded and I demonstrated that as well. So how do I get the SQL PowerShell module? So this is from SQL 2014. It's installed as part of the management tools. Now in some previous versions, I was actually able to install just the SDK and get it that way, but I found in 2014, I actually do need the basic management tools to get the PowerShell functionality. Now that's going to change. On Friday, they actually released a preview version of SQL Management Studios, so it looks like what they're starting to do is break out the SQL Management Studio from the like install disk and the good thing about this, this is a preview build for March, the good thing about this, there's several things in the PowerShell SQL module that are fixed. It no longer changes your current location. It no longer uses unapproved verbs. It also loads much faster and I'm going to show you the code they're using today why it loads so slow. And the more modules you have on your machine with SQL 2014 or prior, the slower it loads because of the way they wrote their PowerShell commands.

Exploring the SQL Environment, Cmdlets, and Client Tools

So that means it's time for the demo. We'll come back for a few more slides toward the end of the session. Okay, I want to show you the environment we're working with here. So this machine is running Windows 10 Enterprise edition. We've got Hyper-V. We've got a domain controller running and if all these are running Windows Server 2012 R2 Server Core, so we a domain controller. We have a SQL 2014 box, a SQL 2008 R2, and by the way, they say you can't run 2008 R2 on Server Core, it's not supported, so don't try this at home, but it is running on Server Core. And then this SQL03 box is a Windows 2008 non-R2 with no PowerShell and it has SQL 2005 with no PowerShell. When I go to SQL Saturday events, I hear from a lot of DBAs that hey I can't write one PowerShell command to query all these different versions of SQL and Windows and so on. Yeah, you can. You just have to write it for the lowest common denominator. It's kind of like if you still have PowerShell 2, 3, and 4 in your environment, you can write one command and use it on all those machines, but unfortunately, you've got to write it using v2 syntax. So just to prove to you SQL03, I've tried to run PowerShell, hey it doesn't exist. If I look at the Windows features, no PowerShell. And the reason I chose this version, it's going to be out of, SQL 2005 I think is going out of support here in a week or so, but I wanted to get the newest version of Windows Server and SQL that didn't require PowerShell. And actually, what I'm going to show you will work on even older versions. So we'll switch back and forth between a couple of virtual machines. I've downloaded a prep for this demo of made the text size 130, changed into the demo folder, I've set my error messages to yellow because the red ones are kind of hard to read on the projector. So we're going to query the servers and actually the machine I'm on just to make sure we've got connectivity and also to show you the PowerShellVersion. So we've got version 4 on one of the servers, version 5 on the others, and the one that doesn't have PowerShell is going to fail. And I've got detailed notes in here as well. So we'll get into the entry part of this. I'm just going to enter a 1 to 1 remoting sessions on SQL02 since it's SQL Server 2008 R2. I want to show you the few cmdlets. I want to show you where we were and where we are as far as how many cmdlets the SQL team has added. (Working) And actually only two of these, three of these are part of the, so we've got two here. We've got two that are part of the SQL cmdlets and the other ones are part of the SQL Server provider. So in SQL 2008 R2, you've got 5 cmdlets total. That's it. That's all you get. And I don't know about you, I've still got SQL 2008 boxes running in production today and I would imagine that a lot of people do.

SQL 2014 Client Tools

Okay, so we'll just exit out of that remoting session. Something else I want to show you is I've installed a SQL 2014 client tools on this PC. And I want to show you that what it does it actually modifies the PS module path. So there's some DLLs and other things that have to be installed to make SQL management objects work. With SQL 2014 when you import the SQLPS module, all that happens magically. It imports the DLLs and everything. Now if you have SQL 2008 R2 and you want to use SQL management objects, you have to actually manually import the DLLs and there's plenty of documentation on the web for that. So if you want to know the details of that, just do a Google or a Bing search on it. Okay. I'm going to import the SQLPS module. It's going to take a couple minutes. Do we have any questions at this point? I'm actually going to mess up our cameraman. I'm going to walk off the stage here. But this is what I like about PowerShell is that SQL gives me that little four square box to operate in and a lot of other products too. And if I did something like called Microsoft and said hey I edited the registry on my server, they'd sorry that's not supported, but with PowerShell I could probably call them and say yeah I modified the registry at PowerShell, oh, that's okay. So I don't have to operate inside the box that people give me to operate in. So you'll notice, we do have two cmdlets in here. I could use the verbose parameter to see the details of this, but I have an encode and decode SQL name and I know why this occurred because back in the snap-in days, they had those cmdlets and the snap-ins actually don't do the checking for the approved verbs, so when they moved it to a module, they just moved their cmdlets, well then they started getting this warning. So what they've done in the new version I said this was fixed in is they've been doing what I've been telling them all along. They renamed the cmdlets and aliased the old names to the new ones, so that way it's not a breaking change. Okay and if you noticed, it actually changed my current location. I was in the Demo folder and it changed me to the SQL Server PS drive. I want to show you some of their code. So this is one of the things that it runs when you load the SQLPS module. So they do a Get-Module -ListAvailable which is going to get every module on your machine and pipe it to where object. So that's where my comment comes in that the more modules that you have on your machine, the slower it's going to be to import the SQL module, but that is also fixed in the new SQL management installation. And of course, you see that it's signed, so it's not like I can just go hack this file and fix it you know. Okay, so let's change back to the Demo folder. Let's take a look at all the cmdlets we get with SQL 2014. We get lots of cmdlets. We actually get some analysis services cmdlets and there are lots in the SQLPS module, but when you import the SQLPS module, it imports both of those. So I stored it in a variable just because it's not so easy to see how many. So we actually have 57 cmdlets. So between 2008 R2 and 2014, we went from 5 cmdlets to 57. I think that's a huge improvement. Okay, so what we can do, we can actually run Transact SQL code directly from PowerShell using the invoke SQL command cmdlet that exists in SQL 2008 R2. So I actually just queried the list of databases on SQL01. Pretty simple.

Querying SQL Server

Now what I want you to think about is I'm sure everybody here is it's a beginner concept filtering left. I mean everybody should know that you want to filter as early as possible in the pipeline. So but when you're working with SQL Server, it may not be a 100% clear what you're doing. So what I can do is actually query a SQL Server database and I'm actually going to do a select * from the person table. I'm going to bring it back to PowerShell and filter it down to the people with the last name of Browning. So I have a list of the users with the last name of Browning. Well I can actually do that same thing inside my SQL code, which is a lot more efficient. It's kind of like some of the Active Directory cmdlets. I mean I think we all know they leave a lot to be desired sometimes depending on what you're doing and depending on the size of your organization and what you're doing, a lot of times you're better off using ADSI. But if you don't know ADSI, then you've got to use the PowerShell cmdlets. So anyway now I execute this other command and you'll notice it was instant. So what I'm going to do I'm just going to run the first one again and then I'll run the second one as well. Well the first one took 1167 ms. So the second option took 8.1 ms and this is all running on this machine, so we're not having to take into account network latency and all that sort of stuff. So what's the difference? The second command was 21 times faster than the first 1. So what I actually try to do, I try to write all my code in Transact SQL because I want to filter as early as possible. I don't want to bring anything I don't need back to PowerShell and a lot of my knowledge with SQL Server is actually, when I started working with PowerShell, it just made sense because you don't want to query all these table and pull every column and every row back only to filter it later and it's the same concept as filtering left with PowerShell. So what I can also do, I can also run store procedures from PowerShell. So that runs fine. Well there's kind of a caveat here. Not all store procedures will run in PowerShell and not all T-SQL code will run in PowerShell. There's a store procedure called sp_who2 and it's basically the processes. And you'll notice I got an error message. It says that duplicate columns are not permitted because it's trying to pull back two columns with the same name and we can't have two objects with the same name. So what I'll do, let's just copy this. So if I put that on my SQL Server and this is SQL Server on query1. If I execute that, it executes fine. Well the problem is you've got a SPD here and you've got a SPD there, and the reason you've got two SPD columns is they're doing a join, so they're joining on that column because you have to have the same, it may not necessarily be the same name in SQL Server always, but the problem with this, we can't just go modify system store procedures. It's probably not a good idea to start with and even if it works well for a while, guess what, when you apply a service pack, your changes are probably going to go bye-bye. Okay, so what's the solution to that if I really want to run sp_who2? I'll just write my own.

SQL Management Objects

Okay, so there's something else called SQL management objects. So I can use SQL management objects and I want you to remember that SQL03 that I'm about to query does not have any PowerShell installed on it. So I can create a new object and then I can use that object. Guess what? I just got the same output as space_who2 store procedure with PowerShell from a machine that doesn't have any PowerShell installed. Pretty cool, huh. How do you connect to that machine? I just created a new object and this actually uses DLLs behind the scene that are on the client, on the machine you're querying from. So using SQL management objects has the dependency of having the SQL management tools installed on the client you're accessing the server from, but it does not have the dependency of having anything like PowerShell installed on the server end. It does require SQL Server, of course, and this is one of the ways you'll see developers actually access SQL Server. And SQL management objects, I'm going to show you something here in a few minutes that it's what the PowerShell provider uses, it's also what the cmdlets that they, all they're doing is wrapping their 57 cmdlets around SMO. So when do you find yourself using let's say a connection string, instead of loading all of the objects in (Inaudible)? I recommend using the .NET Framework like you're talking about instead of SMO if you can because it eliminates the dependency of having the SQL management studio or at least the PowerShell module installed or the DLL specifically. Can you give examples of use cases for instance? Well the use cases would be if I want to deploy something on a 1000 machines, I don't want to put the SQL management studio on all those machines, I just want to run the command, and I've got some examples of that as well. I've actually got a data reader that I've created and if you don't write .NET, then there's actually some really good stuff because I don't know that Warren Frame is in here, but I stole a command from his GitHub repository and you can find it in my repository now and I don't feel bad about stealing it because he stole it as well, but everybody who's changed that code is actually noted in the comments. Justin Dearing, he's another PowerShell and SQL guy, if you know him, he's noted in there. And I don't mind getting commands off the internet if it's all text based, but if it's a DLL and it's a black box, then hey that's another story. But even if you're not allowed to download commands from the internet and run it, you're allowed to create your own, so you can just go get Invoke-SqlCmd2 and take a look at what they're doing and then write your own version of it. Or if you're allowed to copy and paste, then you can just copy and paste and create your own command. Let's just do the same. Did that answer your question? Yeah. And there's really different use cases. I know a lot of DBAs and they actually prefer SQL management objects, they prefer to work with those because they're not developers and they really don't feel comfortable working the .NET Framework, so that would be one reason is sometimes like me, I'm not a developer either, but I was able to figure out how to write a data reader and then I found the SQL command, Invoke-SqlCmd2 and I was like hey I'm kind of wasting my time. I'll use this one because it's much more efficient and well-written than what I had written. We'll run the same command just to show you that the same command would work on a 2014 server. And we're kind of covering these rapidly. I just want to show you some of the options that we have and then we'll get into a few more advanced things. Okay, so one thing I want you to notice here is we now have a SQLPS provider. It's right here at the very bottom. So that's part of the SQL management or the SQL PowerShell module and it also shows up as a PS drive. So you can use that PS drive and you can actually, I've got a list of my databases and what recovery model they're in and so on and that was fairly easy. But I don't know about you guys, but I actually hate using the providers because they're not very well documented and if you're trying to use tab completion, especially in a SQL provider, a lot of times it'll hang. So it's not one of my favorite ways to query SQL Server and I'll show you that it really doesn't matter. So we still have our SQL object that we created, so we can use it. And you'll notice that both of these commands, the output is basically the same from the SQL provider or from SMO and there's a good reason for that. So if you see what type of object we created here, we created an SMO database object and that was with the SQL management object option. Well I'm going to show you what type of object the PS provider creates. So it creates the same type of object, so all they're doing with the SQL PowerShell provider is wrapping it around SMO so that you don't have to be a developer if you don't want to and you may ask why don't these commands look the same, the output of these commands. One's got more databases than the other. They're doing some filtering with the SQL PS provider. So if I use the Force parameter and run the same command, I'll actually get all the databases. So with the SQL PS provider they had the system databases, unless you use the Force parameter. Okay, I just have a quick example of, I can also run Get-Database, which is one of the newer cmdlets. Guess what, same information. You know what type of, I bet you could guess what type of objects it's going to create. Guess what, SQL management object. So all those commands, the PS provider and the cmdlets, all they're doing is wrapping around SQL SMO. (Working) Okay, I think I've already imported these modules, but yeah I've imported those already. This is actually what you used to have to do to import the SQLPS module and keep it from complaining about the names as you would do DisableNameChecking. One question I hear from DBAs is why do this in PowerShell? Why just not do it in SQL? What's the big deal?

Using PowerShell vs. SQL

Well the big deal is I can't use the output in some other system, so since I'm the AD guy and the SQL guy and the Exchange guy and so on, if I did this in SQL, then I couldn't use the output directly anyway in another system. So what I can do, I can actually run a command. I've done a bunch of joins here and I've even done a little bit of text parsing with PowerShell. Don't be afraid to do text parsing with PowerShell. If you can write a command more efficiently using text parsing than using objects, don't be afraid to use it. I've seen people, they'll want the manager in AD and they'll actually query the user and then do another query on AD to get the manager. Well you have the manager, you have the DN for it and you can actually parse the manager out of that DN, Distinguished Name, without having to do a second query to AD. So anyway, I'll query these and guess what, all those joins and all, just imagine this is the HR database. I've got the person's name, I've got their SAM account name, their UPN, all this is in SQL, not in AD. Well guess what I can do with that? Create AD users. So let's query this OU in Active Directory and I'll show it to you as well while that's waiting, it's the AdventureWorks OU. I'll do a refresh, there's nothing in there. And we'll give this a second. It shouldn't take that long, but do we have any questions at this point? Well we got an error message. So let's do this. Let's just do a get-aduser, we'll do a get-adcomputer. We'll make sure I can talk to AD and we'll say sql01. Okay, so we're talking to AD. Let's run that command one more time. I don't see anything wrong with it. You know that's the definition of insanity is to do the same thing and expect the different results, but my definition of insanity is to do the same thing more than one time and get different results, which I just got. If you're not insane, it will drive you insane. Okay, so what we're going to do, we're actually going to encapsulate this inside Measure-Command. We're going to run the same query again, we're going to pipe it to New-ADUser, and we're going to see how long this takes. And I've shown this at some SQL Saturdays and I always challenge everybody who can type faster accurately than I can run this command. And nobody has taken me up on the challenge yet. So I run that command. It shouldn't take too long, unless we have a timeout. So it took, I can see there at the bottom it took 3.1 seconds. Well guess what, 290 Active Directory accounts. We put their name in there, we put their telephone number, we put their address, we put their job title. Now who's got temps at your company that can do that and do that accurately in 3.1 seconds? So just go ahead and fire your temps. That's the benefit of doing this in PowerShell is you're not tied to one system or one technology. So now if I query the OU, I can see there's 290 Active Directory user accounts. Okay, let's talk about the .NET Framework. I actually wrote a SQL data reader using the .NET Framework. It's a function. It's got all of the stuff you should have. It's got comment-based help to tell you how to use it, it's got parameter validation, it's got a Begin block that sets up the connection one time, it's got a Process block that'll iterate through all the commands on the same connection, and then it's actually got an End block that'll close the command when it's done, instead of building a command for every query that you're going to run. So it's a little more efficient to do it that way. So I can actually use that command to query a database. So what I did I actually got backup information from my MSDB database with that command. At this point, I want to jump over to another VM. I've got a Windows 8.1 VM here and I want to show you the SQL tools are not installed on this machine. The only SQL tools on this machine are my SQL tools. And I don't mean my SQL, I mean MrSQL. So I can take and run this data reader using the .NET Framework and receive the same results without needing, without having any dependencies whatsoever, so that's the benefit of using the .NET Framework. What I can do is and this here I actually have a command that we ran earlier, so I'm actually going to create a very simple function to get a list of the databases and remember SQL03 doesn't have PowerShell. So from a machine that doesn't have the SMO DLLs on it, I can use the .NET Framework and query a machine that doesn't have PowerShell on it. So I haven't found a reason, if you're able to write your own SQL cmdlets in the .NET Framework or use somebody else's, I haven't found a reason not to use the .NET Framework. Generally, it's because it's much simpler just to use SMO because it's there. You don't have to reinvent the wheel. That would be the downfall of the .NET Framework, but when you can steal somebody else's code on the internet, then you don't really have that downfall. Okay, so this is the code we ran and I put it in this file as well because this is the only file that's on GitHub. So now we're back to, notice now I'm going to use Invoke-Sqlcmd2, which is the file I was telling you about. I'll get the same results. But it's not a data reader. You can use it in place of where you've used Invoke-Sqlcmd, you can actually update and delete and everything. Okay, so let's do something that's a little more exciting.

Audit Transaction Logs

Let's search the transaction logs for, I'm sure at your company that something came up missing in a database and you say who deleted that and all you hear is crickets. Well somebody deleted something or some process deleted something and that's one of the reasons you want to make sure you're using service accounts for everything because guess what, if you use your account and you set up some process and something comes up missing and you're actually innocent, you're going to be guilty based on the auditing. So this is actually the command I wrote. It's got all the same things I write in every command I ever use. It actually iterates through all the transaction log backups and the active transaction log. And I'm going to show you here real quick, this is some dynamic SQL that I'm writing and I'm writing it based on PowerShell. So what happens is when you query the transactions logs, you can break the transaction log up across 64 files. Well guess what, when you want to query it, there's a position for all 64 files and if there's only 1 file, you have to have the 1 file listed, you have to have default listed 63 times and if you have 5 files, then you've got to have default listed the number of times that it's necessary, but you can dynamically generate that SQL based on information on another table in SQL Server. So you'll generate your Transact SQL code that somebody would normally have to write manually. I want to show you a simple example and then we're going to see what code this generates in addition to the delete operation that occurred. So let's find out what happened to our database. So you can see there that a delete operation occurred on 3/28 at 3:19 PM and it also gives us the log sequence number we need to do the restore with to restore it just before the delete operation occurred. The reason we knew this, we had a report from somebody that a specific record in the database was missing. And you'll notice sure enough when we try to query that record, it's not in the database. So writing dynamic SQL code. So I have a function here, a static function. It's code that I showed earlier. It's very simple. So I'll get all the users with the last name of Browning. If I wanted to make, and this will show you in a very simple way what I'm doing. So all I've done is, we do this in PowerShell all the time, all I've done is parameterized the function, but I'm using the variable inside my SQL query because it's going to get translated before it's sent to the SQL Server. So if I want the last name of Browning, I can get that, but if I want the last name of Smith, I can get that as well. So that's the idea. And the other example is very complicated and kind of convoluted, so I wanted to make sure you knew what I was doing. So what we're going to do, I actually tweaked this code and this tweak doesn't exist on GitHub. There's a tweak in here that's it's actually going to do Write-Verbose and it's going to output the query itself in the verbose output. Make sure I get a clean Output window. So what we can do we can actually take this query here, yeah write that manually. Guess what, I could take this, it's not formatted very well, but it's not formatted that bad when it works. I just want to get it to a point where you can see all these defaults because it'll run like this. So that's the code we generated. So when I execute that, guess what, same information. And then, I'll show you an example. There's no deletes in this one, but it'll still generate the code. This one's a little bit messier because I've actually got four or five, I think five backup log files. Thank you. That tells me somebody's paying attention. (Working) See normally when I go to SQL Saturdays I have stuff to give away and I had stuff to give away, you would get something. I just don't want this to be formatted so bad. I want to be able to see any problems that I have. So that looks good. So you'll notice I've got more transaction log files and we actually have one more problem here. So when I execute that, it should execute without error, without return records and it does. So it dynamically creates this information.

Restoring Database

So if you spread your transaction log backups across 64 files for some insane reason, it'll work. Okay, now for something really cool. We've only got a few more minutes. Okay, so what this is doing, it's getting me the chain of backups and what I would do is never trust the information that SQL Server gives you. You'll say sure, your boss will say do you have backups of the database, sure. It says so right there. Guess what, that's pulling it out of the database. Something else could have cleaned up those files and deleted them. Just because it says you have a backup, doesn't mean you have a backup. So that's what we have test path for. So now we're going to restore the database. We're going to restore it in an alternate database because you know March 28, that's been a while, so we would lose all our records since then if we restored over the top of it. So I actually have a command here called a MrSqlDbRestoreFileList. It gets the logical and physical filenames so that you can do a move on those to a different database name when you're performing a restore. That's the other thing you want to do when you're writing your commands is have it so you can pipe your commands together so that the output of one can be used for the input of another. So we're going to generate an error here. I read a blog article about this last week. Guess what, the log sequence number that we get from the transaction logs, can't use it in a restore because it gives you a three-part hex decimal number that's separated by colons. You have to split that, you have to convert that to numeric and the first part you just put the 2 numbers in there, the second part has to be 10 digits, so you pad it with 0s, and the third part has to be 5 digits, so you have to pad it with 0s, and then when you're done, you have to join it all back together and then you have the number you can do a restore with. So guess what, I wrote a tool to do that, I wrote a blog article about it and now I can delete that from my memory. So that's this Convert-MrSqlLogSequenceNumber, so that's the converted number. You can also pipe the, when you're searching for a delete, you can pipe it directly to the command. So now we're going to do a restore and we're going to stop at that LSN, the Log Sequence Number. Now if we do a Find-MrSqlDatabaseChange, it doesn't find anything, but this is no longer a valid command because this command searches the transaction log backups. I just created a new database. The new database doesn't have any transaction log backups. So the real way to find out if it worked is to query for the information that was deleted and there it is. So as I told you the other day, every time I write a command or write another command to undo everything, so that's cleaning up my demo. We'll jump back to the slide deck. So I'd like to thank everybody from the PowerShell team that's been around for the last 10 years. We wouldn't be where we are today if it wasn't for those guys. We'd still be the click, next, admin.

Wrap Up

This is a resource slide. There's some free stuff. PowerShell.org, the PowerShell Virtual Chapter of SQL PASS. They do like lunch time meetings. They had one today, matter of fact. SQL Saturday Technology Events. I'm speaking at SQL Saturday Atlanta on May 21 on toolmaking and I'm speaking at SQL Saturday Pensacola the first weekend in June on Desired State Configuration. And I've spoke at tons of SQL Saturdays. I speak in Baton Rouge every year. I've spoke at Birmingham, Alabama, Mobile, Alabama. I've spoke at Atlanta before and so on. I speak at some SharePoint Saturdays as well. I spoke last year at SharePoint Saturday in Nashville. The Microsoft Virtual Academy. Now SAPIEN Technologies, the reason I have them under the free section is they have some really good blog articles that June Blender is writing and also their forums. They actually have some decent forums and it's not just related to their products. They have some on just writing PowerShell code because as a SAPIEN Technologies MVP, I've been answering some of those questions. PowerShell Magazine. I've written numerous articles there. User groups. I run the Mississippi PowerShell user group as I mentioned earlier. That's a virtual user group. If you don't have a local user group and we record our meetings as well on our YouTube channel. You can, an alias for that user group website is mspowershell.com. Twitter is another resource. GitHub, blogs. If you want to find a list of good blogs, go to my blog site and see my blog role. If the people that I have in my blog role are not writing good code or not consistently blogging, then they get deleted off the list and somebody else gets added. There's a PowerShell Best Practices and Style Guide and this slide deck is also on GitHub under my presentations repository. All these are links to these sites, but that's the reading material I referenced at the beginning if your code is not passing script analyzer. Okay, there's a couple of paid product. I was actually the technical author for this Pro PowerShell for Database Developers. It's a good book. And then also Pluralsight. We've got one more slide. So that's my contact info. There's the book I co-authored and also the one I wrote a chapter in. I'm a big believer in personal branding, so you'll find me at mikefrobbins pretty much anywhere. Sign up for services, even if I don't use them. If you want my email address, I have a challenge for you. Go out to my blog site, see the about page, my email address, my real email address is encoded, so decode it and send me an email. And then there's the user group. Thank you guys for attending.

What DevOps Really Looks Like

Introduction - What DevOps Is Not

So low sodium, right, cloud, fat loss, right, there's all these marketing terms that get thrown around at us all the time and it becomes really easy to become immune to them. I mean, we're constantly having marketing stuff tossed at us. When cloud first kind of became a word in the IT industry right when we ripped it off from the telecoms, it had a real concrete useful technical meaning for about 10 minutes and then marketing people got a hold of it and ran away with it and suddenly everything was a cloud. You can get your own personal cloud, no that's a hard drive. It's not, it's hooked up to the internet sure, but that doesn't make it a cloud. So everybody kind of ran away with it and the same thing happened with DevOps. DevOps as a term has actually been around for quite a while. It didn't really start to catch on until a couple of really, really good books about it like the Phoenix project was kind of the definitive book that really got people's attention on it in a big way, I guess. A lot of .coms, startups in the current era of startups started using this type of an approach, so DevOps actually meant something and then the marketing people got a hold of it. And all of a sudden, it's we're DevOps, here's DevOps in a box and we being who we are and what we do for a living immediately started ignoring the word, justifiably, I think. And so I wanted to do this talk to kind of just it's a real thing. DevOps is not a marketing word. I want people to pay attention to this because it is a real thing; however, it isn't something you can buy in a box. It's not shrink wrapped, you can't just wake up one morning and go we should do DevOps, let's just do that. It's a massive cultural change. It's a massive philosophical change and how you decide to do IT. You can apply DevOps principles to one tiny project within your organization if it's got some certain boundaries around it that allow that. You can run your entire IT organization according to these principles. But you have to understand what it's supposed to achieve and you have to understand the tradeoffs that you have to agree to make if you're going to go down this route. And so, I've done a keynote a couple of times about really at a high level what DevOps is supposed to mean, what it's supposed to be, but I don't often talk about what it actually looks like in terms of a concrete what is this, if I'm doing this, how will I know, what's it look like, what buttons do I press to make DevOps turn on. And so that's what I wanted to do today. Talk about DevOps.

DevOps - It's Absolutely Not Marketing

It's not marketing. It really is about dev, developers, and ops, operations, working together for a common goal and that goal is to smooth out the path between the coder and whoever's consuming that code whether that's users or it's a service or something else. That is a really, really, really, really important statement that you almost can't go any further until you embrace that. Let's look at what's involved in getting an application or a service from a coder out to its user. For one, you have to start by coding it, right, and about the only way to code is to push your fingers against buttons. That is a creative process. It's an artistic process. We can put a lot of tools in place to help make developers more efficient, but at the end of the day, code comes from the human mind. And so you can only do so much to smooth that part of it out. So now the coder has finished, the coder has hit compile, a miracle has occurred and now we have code. That's when ops traditionally takes over to get the code wherever it's going to be. We're deploying it to a server, we're deploying it out to client computers wherever it is. We are almost nothing but a hurdle, right, any effort that is greater than 0 means we are slowing things down. In an ideal world, the coder would just be able to close his eyes and squint really hard and then the code would appear wherever it was supposed to be, right, that's ideal. That's impossible because developers don't have the psychic deployment powers like we do. So anytime we get involved, we are automatically creating a barrier, so our goal is to work with the devs to lower that barrier as much as possible given some reasonable things like we don't want the universe to blow up every time this happens. So, that's kindof what that is.

DevOps - It's Definitely Not For Everyone

This is not for everyone. There are some of you in this room and there are people in the room every time I talk about this who are going to think to yourself, you know, this makes sense. I'd like to work in this type of organization. Our organization will just absolutely never do this. You need to recognize if that's the case so that you do not start just banging your head into a wall. I get into a lot of conversation with folks. I had a great one with a young lady who works for a company that they do not have any internal software developers, and that's fine, lots of us don't. It's a smaller company. But how do I do this DevOps stuff with my vendors? You can't. They're not going to be complicit in this and she kind of got into this loop of no how can I make it work? You can't. You can't make it work, right. That's like saying I really, really want to drive this car on the ceiling upside down, so how do I make that. No, you don't. It's not what it's for, that's not how it works. So this isn't for everybody. You can definitely, even if it's not for you, there might be some nice pieces you can take from it. And maybe for certain projects it's for you, but recognize when this is not the right tool. So speaking of that, DevOps is not a collection of tools. You will need tools to do this. But DevOps is not something that you can go buy. DevOps is an approach to managing IT. It is an approach to software delivery. You guys know where this kind of came from, this whole DevOps movement? It came from Agile. Software developers started adopting these project management methodologies like Agile where they could really iterate very quickly and produce new builds of software every day, every week, whatever their cycle was and then they would hand it off to ops and we're like maintenance window not scheduled until next Tuesday when the moon is full, and so they would have this huge pipeline of software that they were ready to deploy and we're off over here implementing ITIL which is essentially about stopping all movement in all molecules ever, right. Who works in an ITIL shop? Yep, right. And everybody goes. Yes, we went to the DMV and said how should we run our shop. It's essentially what we did. So DevOps came as kind of a reaction to that, which is, we need to make ops more agile and you'll actually hear it taught. You'll hear about Agile ops or Agile IT and you'll kind of hear those terms thrown around as well. DevOps has become a little bit more popular.

DevOps Facts and Example

You are going to need tools though. You will either have to buy tools or build tools. This is where the concrete implementation starts to come in because PowerShell it turns out is a great way to build tools to help you achieve some of these goals and we'll talk about that. Some DevOps facts. Puppet labs today a state of the DevOps report in 2015 and talked to a bunch of different companies who are really, really all in with this approach, 60 times fewer failures, 60. I want you to think of all the software deployment fails that you have had over the past year and if you're thinking, yeah, it was about 60, well they just took it to 0. Sixty times fewer failures. A hundred and sixty-eight times faster recovery when there is a failure. Do that math. If you had a failure that took 168 days to recover from, they just took it down to one day. That's a hugely important fact because part of DevOps is accepting the fact that you will have failures. You are going to screw something up just like you have always screwed things up. This is just acknowledging it and getting to a point where you can recover fast enough that it's not so painful when you fail. It's called fail with style. Okay. They got 30 times more deployments in the field, 30 times. That means instead of deploying something once per year, you deployed it once every couple of weeks, which means you're now moving features, you're removing bugs, you're getting code out to where it needs to be, you've massively improved the agility of the business and the businesses ability to respond to changing conditions because you can react much faster. Two hundred times shorter lead for deploying. If it used to take you 200 days to get completed code out into the field, now it takes you 1 day. If it used to take you 200 hours, now it takes you 1 hour. These are a big, big deal. I mean, this is a massive change and there's a lot of benefits, but you have to acknowledge a few things, so let's run through a quick example. How many of you are familiar with Amazon Web Services Elastic Beanstalk? Okay, just a few folks. Go read up on it. Now if your, oh my company will never, I don't care. I'm not telling you to buy it. I'm telling you to go read up on it because this is the ideal text book definition of what DevOps actually should look like. Amazon themselves has taken on the role of ops, they are the IT operations department, and they have managed to remove almost all human interaction from the software deployment process while still making it safe, fast, reliable, and predictable. Here's how it works. I'm a dev, okay. I'm writing my little web app that's going to get run in Amazon. Let's say it's in a PHP or whatever goofy crap kids use these days. As part of that, in a little folder, I have a text file and the text file describes the type of environment that I want my app to run in. Whatever packages I need, whatever add ins, whatever DLLs, or blah, blah, blah, blah, blah, right, I need this much RAM, I need this type of processor, I need this type of OS, I need these things in it. One of the biggest places where we fall down in deploying software, is that the damn developers never tell us what's supposed to be running when we put it in production, right. They sit in their little ivory tower world and are like oh wow my code didn't compile, oh I was missing this package, I'll just install it. Look, it compiled and they don't ever clue us in the fact that package has to be there for the code to work, right. And that's when we run into failure. We push that sucker out live and it blows up into a million pieces because nobody actually wrote down what it was we were supposed to deploying and when they do write it down, what's that deployment document look like? You get like a Word doc, right, yeah it needs this and this and this or maybe it's a ticket. It's not something we can consume easy. It's too easy for us to screw that up. So in this model, the devs put a text document as part of their project and it's a text document I could read and it's a text document that the computer can read. In Microsoft terms, we might call this a MOF, right. So what happens is, somebody says okay, let's deploy the app maybe to a test environment. Maybe that's what we're going to do first. Amazon sucks in all the project code including that file, it sets up the environment, it compiles the application, it injects the code into the virtual machine, and then it starts running whatever automated tests you've set up. This is key because the code only runs in an environment that was set up by the automation engine. So if the developer forgot to specify something, his tests are going to fail and he can't just run in here and oh yeah right, I forgot such and such a package. He's got to go back, he's got to modify his configuration document to indicate what package was missing, he's got to push the button again, the deployment engine will spin down the old environment and spin up a new one that meets the new criteria, so by the time he gets it working, we know we've got this nailed. So what Amazon literally does is you can ping their REST API or you can go into their dashboard and you can say update app. It spins up however many virtual machines, sucks the source code into them, configures those virtual machines, sets their memory, sets the processor class, installs whatever packages you need, sets environment variables, registers those with the load balancer, deregisters the old ones, and then destroys the old ones. Everything that we're used to doing manually, they just got as a bunch of Python scripts. We might do that with a MOF. Maybe we just tell the developer look I'm going to teach you how to write DSC configuration scripts and you're going to draw up a DSC configuration script in as part of your project. And we go to deploy your project, I'm going to take that and I'm going to use it to create a MOF and that's how I'm going to configure the environment.

Set up Environment, Compile and Inject Code, Run Automated Tests

So let's dig into that a little bit more. Setting up that environment. Tools like DSC, Chef, Puppet, Ansible, Salt, SCVMM, this is designed to read the application's environment needs from that document and then set up the environment appropriately, including dependencies like packages or DLLs or whatever other dependencies might exist. We're now documenting those things in a way that can be automated. Now it sounds hard when you just talk about it, but really think about it, how many of you think you could probably rig this up at least in like a trial situation using something like maybe DSC or Chef or Puppet, yeah. The technologies all exist at this point. We can actually do this. We don't have like a complete pipeline product that we can just go buy that will magically do all this, but all the pieces exist and we'll talk about some of those pieces. So setting up the environment is straightforward. How many of you think you could automate the creation of a VM if I pushed you to it? I already have. Yeah, and a lot of you already have. If you've got something like DSC or Chef or Puppet or whatever, how many of you think you could automate the provisioning of that environment once the VM is up. Of course you could. Absolutely. How many of you think you could have a developer give you a PowerShell configuration, a DSC configuration script, you can run that, produce a MOF, but then you could also have your own operational concerns configuration script, which you could run and produce a MOF. So now you've got the stuff the developer needs, but you've also got the stuff you need, the security, the firewall, the things the developer might not necessarily care about, getting your management agents installed or whatever else and you just hand both of those to the LCM. Okay, I'll do that. I'll configure it. I'll set it up boss. Right. We have all this. We have the pieces. So once the environment is up, standard compilers just get called to compile your code or you can take it out of a checked in source control repository, right, when someone says go, you just get the latest check in from source control that's passed whatever validation it has. That code gets injected into the new VM. Tools like TeamCity can help you do this. These exist, they're out there. And then if it's a test environment, once it's up and running, you run automated tests against it. Now the exact tools you use to create those tests are going to depend on your software dev environment. Right, if you've got Microsoft developers who are doing .NET, then you're going to use the tools that are part of Visual Studio to create these automated tests. If you're pushing PowerShell code, you're going to need something like Pester. If you're pushing Python code, they've got automated test engines as well. So you're going to use the matching testing framework. You run the test against the code. If the tests fail, you bounce it back to the developer, you decommission the environment, you destroy it, they've got all the evidence they need to go try again. This isn't any effort on your part. It's all automated. It happens when the developer sits in their Visual Studio and they say go. A miracle occurs. We create the miracle and then we just watch it lovingly from afar. If the tests pass, then you repeat the entire process in the production side of the environment because the tests passed. Alright, where's the weak spot here? Just because the tests pass, am I guaranteed that this is going to work perfectly in production? No. No, I'm not. This is where you have to be willing to accept failure. You get to production, Oh God, we missed something. Fine, roll back, do the whole process again with the last known good version, which is still in source control, which is why we use source control, kids. Go get the last known version, repeat the whole process. How long do you think it would take for most applications? From the time you go oh crap, it's not working, push the abort button, how long do you think it would take to go get the old version of the source code, spin up the environment, and get the old version running in production again? Seconds to minutes. Quick, quick like a bunny. Quick, quick, quick, quick, quick, quick. Because we've automated it all and we know it's going to work because the last time we did it with that version, it worked fine. There's no reason it shouldn't work fine this time. So now that you know it failed in production, you know you forgot to test something in your automated tests. Now production is back up and running, everybody can breathe easy, go fix your tests. Right, okay well that didn't work out so well, so let's make sure that we're going to test for that situation again next time and we'll catch it. So you failed, but you recovered quickly and the learning process, right, because we're supposed to learn from failure. Most places are supposed to have the attitude that it's okay to fail so long as we learn and we don't do it again. Your learning process is going and updating those tests. That's code. The next time you run those tests, the computer is going to do it exactly the same way every single time, and so you have learned and you have improved your automated environment's ability to resist error the next time. And you were only down for a few minutes. Small price to pay because what we do now is we ship it out after spending a ton of time wasted trying to ensure we will have no failure, we have failures anyway and our learning process is everyone standing in a room screaming at each other hoping to God the developers remember to test it the next time, so it doesn't happen again. Well this is a much more codified form of failure. It's a form of failure that leads to improvement. You can point to it. You can chart the number of failures you have and you can chart the number of things you're testing. That's why people like this. Managers can get behind this because they can see the gradual improvement over time. Make sense? So we run our automated tests, and like I said, Elastic Beanstalk is the perfect example of this. You're shaking your head. In disbelief or? No, I don't know anything (inaudible). Oh, Elastic Beanstalk is just that. It's an Amazon service. You point it at your Git repo and your project needs to have a configuration file, not unlike a MOF, different because they have their own format, and you say okay deploy it, go. They spin up some VMs, they read your configuration file, they install packages, they do whatever, they suck your source code over, they put it where it's supposed to go, they register the VM with a load balancer, and they make it available to the public. It takes about 10 seconds. It's magic. And it's the perfect implementation of DevOps and a lot of people will look at that and go well that can't be DevOps because there's no ops, it's just the dev deploying stuff. Yeah, now it's awesome. There is ops. Amazon is the ops. They've just automated human beings out of that sequence because we don't scale well and we make mistakes. And this way, we've taken the one piece of the pie we cannot automate coding and we've made that the highlight feature and everything else is automagic after that. Yeah, but does that mean developers can deploy stuff straight to production? It means that depending on how you set up your pipeline, developers can deploy directly to a test environment, and if all the tests pass, does that mean it automatically goes to production? Well no. It's how you set it up. Maybe if the tests pass, it logs a help desk ticket and says we're ready for deployment and then during your next maintenance window or your next release cycle, after you've had a chance to let your users know this is coming, whatever your process is, you push the button and it all happens automatically from there. So you can still have business process cutouts and for some projects you're going to want that and for other projects you're not going to want that. You're just going to want it to sail automatically through. It's going to depend on the situation and your business needs. So ops still exists, right. People make this mistake that DevOps is no ops and that's not the case. We're still there. We've just automated all of the finicky bits and we've become decision makers because that's what people do well. Right, we're really good at processes and making decisions about those processes. Where we suck is when things get boring. Alright, and we're deploying the app again. And we're just typing, throw that package, and that, oh crap I missed two things and that exploded. Alright, we're not good at consistently doing repetitive things over and over and over and over, but we're really good at those decisions, and so we just take on a human role. Now I've got a couple of companies that I talk to that do this and there's a lot of companies that do it, a couple I talked to that do it really well, and what they found is they've basically removed most of their second tier of IT. They've got a help desk to unlock people's passwords, right, because you do, and then they've got engineers high level, tier three folks who write all this. They write all the code necessary to automate all this and that's about it. That's all they have.

Failure Is An Option

What about failure? This is the Twitter fail whale. You're going to fail. You've always failed. Like I said, this is just a way of codifying that failure and making sure that what you learn from that failure can't happen again because you've got a process in place that's coded. So one more thing. It's fun to talk about all this, but a lot of folks will get a little bit stuck on okay let's say I don't have software developers, that means I don't really need to do this, right? Well, yes and no. You guys have all heard the term infrastructure is code, right. I hate that. It's syntactically wrong. Infrastructure from code is fine. Infrastructure from code. How many of you have a vision, even if you're not doing it, have a vision of having all of your servers managed via something like DSC or Chef or Puppet? Well boys and girls, that's code. That DSC document, that configuration script, and the MOF that comes from it, that's code. This exact same pipeline applies to you. Here is my DSC configuration script. Push the button, compile, here's the MOF, let's spin up a virtual machine, let's inject the MOF into the VM, let's let the LCM do its dance, and then when it's done, we're going to run a bunch of automated tests to make sure the environment is configured the way we wanted it to be, and if it is, then that code, that DSC configuration script is approved for use in production and we can push the other button and start spinning up new servers with that thing. Okay, times change. You know what, we're switching AV vendors now, we're going to use different antimalware on our servers, so I'm going to go change my DSC configuration script to remove the old stuff, make sure it's not there, make sure the new stuff is there, make sure it's configured correctly, deploy it to the test environment, spin up the VMs, run the tests against it, looks good, push it to production, all my servers reconfigure themselves consistently every single time. Infrastructure from code means that we are developers, sorry, probably didn't think that was going to happen when you came out this week, but you're a dev now. So infrastructure from code means we can use this same pipeline to manage our infrastructure and this is way better than well how did it use to be, right. Let's say you changed management agents and now you've got to go around and uninstall the old one and install the new one on how many servers do you have, 500, 1000. You have two ways of doing that. You could do it yourself manually by running around and doing it, right, actually a couple of way, more than two. You could do it manually. You sitting in this room probably would have written a script to do it, but scripts in that kind of situation can be a little delicate, right. Anybody ever script something like that in the past? Did it work perfectly on every single machine? No because you've always got those little specials ones out there that are broken. And then you're third option is that new thing, the intern net where you go to a college and you get a couple of kids, pay them, and they run around and do it manually. But wouldn't it be better if you could have a policy-driven infrastructure where you just say look this is what you need to be today, and if that ever changes, I'll let you know, and you can run all that through this DevOps pipeline where we are both dev and ops and we automate the ops piece of it and we focus on that creative human part, the dev piece, our code, our configuration. Could you see yourself running things this way? See DSC, Chef, Puppet, all of them, they're scary as Hell if you're not doing this. If you're not pushing things through test and testing and building it as a pipeline where it goes from a successful test out into production. Just whacking out MOF files, that's scary. But this automates all the ugly bits, the testing and everything else. How many of you have played with DSC to provision a machine? Okay. So you wrote your configuration script, yeah, you ran it so you get a MOF, right. When you were just starting to do this, did you just push it out to the node first time? Yeah, yeah -Verbose -Wait right just to see what happened and when it ran, the first time you got it to run without errors, remember that? Remember how good that felt? What did you do next? It ran without errors. What's the very next thing you did? Source control. You did not put it into source control next. You RDPd into the server, didn't you, to see if it worked. Yeah, of course you did because you don't believe it. All we're doing is talking about automating that piece. Instead of you RDP-ing into the server and looking around with your human eyeballs, we want the machine to do it. Have the machine go check. Did you do this? Did you do this? Did you do this? Did you do this? Is this true? Is this true? You write a little script for it. We've got testing frameworks, but if you just wanted a little script, you could even do that. And once the machines could because the machine is going to be able to check all those things a lot faster than you and it's going to do it every time. It's not going to forget. Make sure it's in the domain. That's important. Make sure it's got an SSL certificate. Make sure it's got an HTTPS listener for remoting. Make sure the HTTP listener is turned off, right. You're going to forget at least one of those things at some point. The computer will never forget. So you build those checks. You don't RDP into the server please for the love of all that's holy stop RDP-ing into your servers. Alright. Not an option with Nano, so we're going to have to figure out how to automate this anyway. So you automate it, let it check it. And if it says thumbs up, you've got to believe it. And it might be wrong, you might have forgotten to test something and that's fine, you'll figure it out, but then go back and add it to the test. Create that confidence in your code and that's really what DevOps is. Failure is always an option here. It was always an option. We always failed. You know, one of the reasons, so Puppet labs if you dig into that state of DevOps, I think that number 200 times shorter lead time to deployment. You know why? Because they eliminated all this human QA effort where it was, okay here's the code, now we're going to have some monkeys sit here and just pound on it. They're going to run through scenarios, they're going to try and break stuff, it's going to be a little random, they're not really going to collect data, so the developers are going to get frustrated and we're going to go back and forth and fight with each other and the devs are going to say it's fixed and then we're going to do the whole cycle again and they're going to find something different and they're going to send it back and they don't have enough data. That's why it took so long because that QA process was too random, too not automated, too inconsistent. All this does is it flights all that a lot faster. It's consistent. You're still going to fail. How many of you have ever seen a big code deployment go out with 0 bugs? No. That's not true. They all go out with 0 bugs. It's not until the users touch them and infect them, right. You know that's where bugs come from, they're infected. Users infect code. Well it happens all the time. It's always happening. It's always going to happen, but at least this way, we can get a less screwed up version a lot quicker. Here's why a lot of developers like this. Rather than deploying one giant thing every year that has 800 moving parts because when something breaks, not if, when something breaks, you've got 800 moving parts to look at. They would, most of them, much prefer to ship out something every week than only change one or two things because if they break it, they're pretty sure what they broke. Right, the other problem you get with these long release cycles, these let's work on it for six months or a year or a year is by the time you get to the end and you ship the code. The guy who started the code doesn't work for your company anymore. Right, either his brain has changed because he's lived through 6 or 12 months of life or he's quit and gone on because he can't believe you can't deploy software any faster than that. But you do these things quickly and constantly and small, tiny, tiny bits, then when something does break, yeah, pretty sure I know what that was because we only changed that one line that said Y2K don't touch, we touched it. We should go put that back. So it's really easy to recover a lot faster. And so us letting them ship more stuff because who are your best beta testers, who's really going to find all your bugs? Users. Your users because they're infecting the code. So you might as well shorten up the path for getting the code out to your users because that's where all the testing happens anyway and if you can do it in small bits. And again, this is the same with infrastructure, infrastructure from code. Don't save up all the changes you want to do to your servers for a once every five-year nightmare cluster. Alright, change one little thing at a time. Okay, we're just going to push this out. Oh, that went well. Great. Okay let's do the next thing. Let's push that out and we can do it constantly and iteratively and evolve things rather than having massive revolution all the time. This is one of the reasons Microsoft is trying to shift their own model over to this so that they can just ship small little bits and gradually evolve the operating system, rather than having to come up with a brand new user interface and rearrange the entire control panel every five years, right, which is all Windows has ever done, right, new UI, new control panel.

Mapping Philosophy

So the model you're describing so far my understanding you can bring node oriented and during Dekens talk about VMware, it would be in that, Oh he had to instantiate a system to use its LCM to manage the rest of the environment, so when you compare it to _____ software, the whole infrastructure had this scripted thing talking about it wasn't just one node, it was all sorts of things. How does DSC have it mapped to---? So yeah, so this does not really talk about just one node. Take Elastic Beanstalk, for example, if you work for a large company and you've got an enormous website, that push the button might be spinning up 300 VMs, some of those might be a database, some of them might be front-end servers, it's the whole application. Same thing here. Same thing here. You usually try to segment your environment into functional chunks, which we kind of do that anyway, right. I mean if you've got a three-tier application, you kind of have its back-end and its middle tier and we try at least not to cross things, right, we want one server to basically do kind of one role in the environment. Yeah, you might have machines that are AD and DNS and DHCP, but we call those infrastructure servers and that's a role and we try not to put anything else on those, right. And so, you can have multiple things happening as part of an operation. You just try to put some role boundaries around it so that you can manage those things as a unit and I use the virtual machine a lot. Might not be a VM, might be a container, might be client machines. How many of you have ever screwed up a Group Policy? Because you're live editing the stupid things. There's not a save button. I wonder what happens if I click, what's the screaming all about? So I mean we could absolutely apply these types of things to the existing infrastructure the way we already do things. Remember, this is an approach, this is sort of a guidance on how to go about this. It's not a fixed set of tools, it's not a fixed set of scenarios. We can use examples using concrete things like DSC or Elastic Beanstalk, but those are just examples of the philosophy and that's really what this is. You can apply the philosophy to anything.

Before DevOps vs. After DevOps

So comparison, before DevOps, your recovery and response took a long time. The reason most companies are so fearful of failure is because when something does fail, it fails for a long time. Right. Most of us can handle a stubbed toe because it heals in five minutes or so. It's the broken leg that's a real pain because then you're in the chair for 10 weeks. After DevOps, the recovery and response can be really fast. Failure becomes less serious because it's less impactful. Before DevOps, everything was mission critical. How many of you have been told that email is mission critical? That's bull. I know it's inconvenient when email isn't up, but no one died. Okay. There are places where nearly every application is mission critical and those places are called hospitals because people die when things break. If no one's going to die, then you can handle a failure for a few minutes or an hour or whatever. I know it's inconvenient, but if you move over to a DevOps model, you can start to realize that yeah not everything is so mission critical. We can afford to be down for a little while. We can stomach a little inconvenience. What we couldn't stomach was yeah email is down, it's going to be down for several hours and when it comes back up, everything's gone, right. That's why everyone said oh it's mission critical. They didn't actually mean it is critical to our mission. They meant I know when this fails, it's going to be biblical and we can't handle that. Before DevOps, we know that change breaks things, so let's just not change, ever. Let's implement a management framework that has an entire committee who sits down once a month to discuss how they're not going to approve anything and we call that ITIL. After DevOps though, change is fine. You can be more agile. We have to change. We know we have to change. How many of you have organizations who from a business perspective are just pressing IT to do it faster? Yeah. Maybe don't outsource your VM management to a company that has a six-month spin up SLA, just as a suggestion. But they're pressing us because we need to be more agile and we can because change becomes less scary because we have a recovery plan that's fast and that'll work.

DevOps Isn't One Thing

DevOps is not just one thing. It's going to look different in every single organization. It's going to look different in nearly every single project, but let me just give you kind of a quick example of how this probably will change your IT organization. How many of you have separate dev and ops and maybe even like a network infrastructure team like fairly siloed IT? That goes away. It has to. And this is where we have to look at whether our organization is willing to do this. What it needs to be is not, how many users have walked into the office and said man, looking forward to getting that IP address today? Never happens. We even fool ourselves. Remember we went down this whole virtual desktop infrastructure? How many users come in and say oh I love my desktop. Love it! They want their apps. They want their services. And so, we have to reorganize IT. If we're going to do DevOps, we've got to be organized around apps and services. So what it can look like, for example, is here's the corporate application team. It's got 10 developers and 2 ops guys and a network infrastructure guy and they work together on that. And incidentally, those same two ops guys are also on this other project team because their lending their ops expertise to that team and telling the developers look in order for us to really smooth this out and we're all on this boat together, so let us help each other make it happen. If you could put these two things into your code, we're going to be able to make it much quicker and easier for everybody else, so let's do that. And they have access to that in real time and those guys float to whatever projects they're assigned to. And then all the ops guys and all the infrastructure guys and all the developers have a guild and that doesn't have anything to do with the project. That's just where the network infrastructure guys all get together for lunch every month or so and say so how's it going on all the projects? And one of them go yeah, you know what we really found out that this one brand of switch that we're trying out, rocks. It's a little easier to manage, so as you guys start moving into other projects, you might look at that. That was really successful. And somebody else can say yeah, you know what, we figured out that router over here blows up every Tuesday and all of our problems have been stemming from this one place, and so they share their domain knowledge with each other, but they're not organized that way. You don't just have a bunch of people who are disconnected from reality managing IP addresses in an Excel spreadsheet. You organize around what is delivering to the business. And if you're going to do DevOps, even for a small project, the team has to look like that. Everybody who's contributing to it needs to be in the boat. It's the only way you can make it work because otherwise what do you end up with? You get these walls, you get these communication walls, you get these direct report-to hierarchy org chart walls. It's not healthy. It's not conducive to getting the job done. One thing you'll probably start noticing too is that the projects that fail is run by the same --- project. Can we start cutting the fat? Projects that fail all the time that have a common human resource element, you can start to look at that. (Audience comment) You do have to change how you think about this. So where do you begin? Small. Do not go home and say hey you know that one giant project that actually makes the entire company run every single day, let's DevOps that sucker. Go find something little. Find something low stakes because there's going to be a learning process. You're going to have to build some tooling. You're going to have to experiment, see what works for your organization. Experiment somewhere lower impact, less mission critical. Start small and then grow from there. And if you're, yeah I know, I work for a bank, we can't do, actually most of the companies that are really leading DevOps right now are financial services companies because they're under such pressure to push out websites and mobile apps and all this other stuff and this is the way they're actually getting it done, so you can do it. How many of you are thinking yeah but my company is a little different? No, you're not. All the companies are different. All of them. You're all equally weird in different ways and that's why you're probably going to be building a lot of your own tools. You might start with building components that somebody else that you buy, but you're going to glue everything together on your own. You might go get a box of Legos, but you're going to put them together in your own way and that's how you accommodate for your differences. That's why DevOps is not something that you can just go buy at, do we have software stores anymore? Amazon, you can't buy DevOps at Amazon unless you're buying it from Amazon Web Services who does it quite well.

Wrap Up

Okay, a little bit of reading list here. There's a book called Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation. The Visible Ops Handbook and then The Phoenix Project, which is actually a really good read. Mark, you have a? No, I don't, but done it. Are there cases where somebody's actually written a 5, 10, 15-page web services? Here's a real example. These are the examples, these are the products, these are the scripts. Like you said, start with something small. Right. Yeah, so is there an example of somebody who's written something concise and small about this? Spotify. Go punch in Spotify DevOps and they have reorganized their entire company along these lines. So thanks guys. I'll see you around.